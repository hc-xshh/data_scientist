
================================================================================
第 1 页
================================================================================

【文本内容】
IEEE/ASME TRANSACTIONS ON MECHATRONICS
1
Aligning Cyber Space with Physical World: A
Comprehensive Survey on Embodied AI
Yang Liu, Member, IEEE, Weixing Chen, Yongjie Bai, Xiaodan Liang, Senior Member, IEEE, Guanbin Li,
Member, IEEE, Wen Gao, Fellow, IEEE, Liang Lin, Fellow, IEEE
Abstract—Embodied Artificial Intelligence (Embodied AI) is
crucial for achieving Artificial General Intelligence (AGI) and
serves as a foundation for various applications (e.g., intelli-
gent mechatronics systems, smart manufacturing) that bridge
cyberspace and the physical world. Recently, the emergence of
Multi-modal Large Models (MLMs) and World Models (WMs)
have attracted significant attention due to their remarkable
perception, interaction, and reasoning capabilities, making them
a promising architecture for embodied agents. In this survey, we
give a comprehensive exploration of the latest advancements in
Embodied AI. Our analysis firstly navigates through the forefront
of representative works of embodied robots and simulators, to
fully understand the research focuses and their limitations. Then,
we analyze four main research targets: 1) embodied percep-
tion, 2) embodied interaction, 3) embodied agent, and 4) sim-
to-real adaptation, covering state-of-the-art methods, essential
paradigms, and comprehensive datasets. Additionally, we explore
the complexities of MLMs in virtual and real embodied agents,
highlighting their significance in facilitating interactions in digital
and physical environments. Finally, we summarize the challenges
and limitations of embodied AI and discuss potential future direc-
tions. We hope this survey will serve as a foundational reference
for the research community. The associated project can be found
at https://github.com/HCPLab-SYSU/Embodied AI Paper List.
Index Terms—Embodied AI, Cyber Space, Physical World,
Multi-modal Large Models, Agents, Mechatronic Intelligence
I. INTRODUCTION
E
MBODIED AI was initially proposed from the Embodied
Turing Test by Alan Turing in 1950 [1] and has wide
This work is supported in part by the National Key R&D Program of China
under Grant 2021ZD0111601; in part by the open research fund of Pengcheng
Laboratory under Grant 2025KF1B0050; in part by the National Natural Sci-
ence Foundation of China under Grant 62436009, Grant 62322608, and Grant
62301532; and in part by the Guangdong Basic and Applied Basic Research
Foundation under Grant 2025A1515011874 and Grant 2023A1515011530.
(Corresponding author: Liang Lin.)
Yang Liu, Weixing Chen, Yongjie Bai are with the School of Computer
Science and Engineering, Sun Yat-sen University, China, and Guangdong
Key
Laboratory
of
Big
Data
Analysis
and
Processing,
Guangzhou,
China.
(E-mail:liuy856@mail.sysu.edu.cn,
chen867820261@gmail.com,
baiyu8581@gmail.com)
Xiaodan Liang is with the Shenzhen Campus of Sun Yat-sen University,
China, Guangdong Key Laboratory of Big Data Analysis and Processing,
Guangzhou, China, and Peng Cheng Laboratory, Shenzhen, China. (E-mail:
xdliang328@gmail.com)
Guanbin Li and Liang Lin are with the School of Computer Science
and Engineering, Sun Yat-sen University, China, Guangdong Key Labora-
tory of Big Data Analysis and Processing, Guangzhou, China, and Peng
Cheng Laboratory, Shenzhen, China. (E-mail: liguanbin@mail.sysu.edu.cn,
linliang@ieee.org)
Wen Gao is with the Peng Cheng Laboratory, Shenzhen, China, and also
with the Institute of Digital Media, Peking University, Beijing, China. (E-mail:
wgao@pku.edu.cn)
Physical Environments
Multi-modal Large Models
Sim2Real
Configurator
Perception
Actor
Critic
Intrinsic 
Cost
Memory
World 
Model
Cost
Execute
Feedback
Virtual Environment (Cyber Space)
Robots
Embodiments
Multi-modal 
Active Perception
Multi-modal  Elements
Human-Robot
Interaction
Task 
Planning 
Interaction
Embodied 
World Model
A Model
B Model
C Model
Fig. 1. The framework of the embodied agent based on MLMs and WMs,
incorporates the ABC model, which stands for AI brain, Body, and Cross-
modal sensors. The embodied agent is equipped with an embodied world
model as the A model, enabling it to understand the virtual-physical envi-
ronment. Through the C model, it actively perceives multi-modal elements,
enhancing its situational awareness. Meanwhile, the B model endows the agent
execute actions, and interact with humans while utilizing tools effectively.
applications including robotics, healthcare, and smart man-
ufacturing. Embodied AI is designed to determine whether
agents can display intelligence that is not just limited to
solving abstract problems in a virtual environment (cyber
space1), but that is also capable of navigating the complexity
and unpredictability of the physical world. For example,
embodied AI enhances mechatronic systems by integrating
with physical components for adaptive, real-world interactions,
which enables systems to learn, perceive, and execute au-
tonomously, improving efficiency and functionality, as shown
in Fig. 1. The agents in the cyber space are generally referred
to as disembodied AI, while those in the physical space are
embodied AI (Table I). Recent advances in Multi-modal Large
Models (MLMs) have injected strong perception, interaction
and planning capabilities to embodied models, to develop
general-purpose embodied agents and robots that actively
interact with virtual and physical environments. Therefore, the
embodied agents are widely considered as the best carriers
for MLMs. The recent representative embodied models are
RT-2 [2] and RT-H [3]. Nevertheless, the capabilities of
long-term memory, understanding complex intentions, and the
decomposition of complex tasks are limited for current MLMs.
To achieve Artificial General Intelligence (AGI), the de-
velopment of embodied AI stands as a fundamental avenue.
Different from conversational agents like ChatGPT [4], em-
bodied AI believes that the true AGI can be achieved by
controlling physical embodiments and interacting with both
1The agents are the foundation of both disembodied and embodied AI. The
agents can exist in both cyber and physical spaces, integrated with various
entities. The entities include not only robots but also other devices.
arXiv:2407.06886v8  [cs.CV]  25 Aug 2025


【图像 1】(158x158)
1. **图像类型**：  
   该图像是一个**徽标（Logo）**，属于图形类标识。

2. **图像中的所有文字内容**：  
   - 中文文字：**“鹏城实验室”**（位于徽标上方）  
   - 英文文字：**“PENGCHENG LABORATORY”**（环绕在徽标下方）

3. **图像中的关键视觉元素和数据信息**：  
   - 整体为圆形徽标，以蓝色为主色调，象征科技与创新。  
   - 中央图案由抽象的波浪形结构和点状元素组成，可能代表数据流、人工智能或网络连接，体现“数字”与“智能”的概念。  
   - 图案中包含一个类似地球或球体的形态，暗示全球化或系统集成。  
   - 点状元素呈环形分布，可能象征数据节点或计算单元，呼应“AI”与“智能系统”主题。  
   - 外圈为白色背景，文字清晰可辨。

4. **图像传达的主要信息**：  
   该徽标代表“鹏城实验室”（Pengcheng Laboratory），是一家专注于前沿科技研究的机构，尤其在人工智能、智能系统等领域具有研究实力。结合上下文（IEEE/ASME期刊论文），该徽标表明作者所属单位或合作机构，强调其科研背景与技术导向。

【图像 2】(636x483)
1. **图像类型**：  
   这是一幅黑白线条风格的**手绘式图形插图**，描绘了人类大脑的侧视图。

2. **图像中的文字内容**：  
   图像中**没有文字内容**。

3. **图像中的关键视觉元素和数据信息**：  
   - 图像展示了一个**人类大脑的侧面轮廓**，包括大脑皮层（cerebral cortex）的复杂沟回结构（沟和回），表现出典型的脑表面褶皱。
   - 可见**小脑（cerebellum）**位于大脑下方，呈扇形、密集条纹状结构，与大脑主体有明显区分。
   - 采用**黑白线条勾勒**，通过线条疏密变化表现立体感和纹理细节，无颜色或阴影填充。
   - 整体构图简洁，突出大脑的解剖特征，但未标注具体脑区名称或功能分区。

4. **图像传达的主要信息**：  
   该图像象征性地代表**人工智能与生物智能的关联**，尤其是在“具身人工智能”（Embodied AI）背景下，强调智能系统需模拟人类大脑的感知-行动闭环。结合论文标题“Aligning Cyber Space with Physical World”，此图可能意在体现**将数字智能（Cyber Space）与物理世界中的生物智能（如大脑）相融合**的理念，暗示AI应具备类脑的感知、决策与执行能力。

【图像 3】(495x372)
1. **图像类型**：  
   该图是一张三维建筑空间的计算机生成图形（3D渲染图），呈现为一个俯视视角的室内布局示意图，具有虚拟仿真或数字孪生的特征。

2. **图像中的所有文字内容**：  
   图像中**没有可见的文字内容**。所有信息均通过视觉元素传达。

3. **图像中的关键视觉元素和数据信息**：  
   - **整体结构**：展示了一个多层、开放式的室内建筑空间，中心为一个大型中庭（atrium），四周环绕着不同功能区域。
   - **楼层与通道**：包含多个楼层，通过楼梯、自动扶梯和平台连接。部分区域设有栏杆和玻璃围挡。
   - **人群分布**：图中散布着大量白色人形轮廓，表示行人或用户，分布在各楼层和通道中，暗示人流活动路径。
   - **功能区划分**：可见不同区域设有柜台、展台、座椅等设施，可能代表商店、服务台、休息区或展览区。
   - **光照与材质**：使用灯光效果突出重点区域，地面材质多样（如地毯、瓷砖），增强空间层次感。
   - **视角**：采用斜上方俯视角度，便于观察整体布局和空间关系。

4. **图像传达的主要信息**：  
   该图像用于展示一个复杂物理环境的数字化建模，强调空间结构、人流分布与交互场景。结合论文主题“具身人工智能”（Embodied AI），此图很可能用于说明AI系统如何在真实或模拟环境中感知、导航和与物理世界互动，体现“将网络空间与物理世界对齐”的核心理念，是具身AI在智能机电系统或智慧城市应用中的典型场景示例。

【图像 4】(218x195)
1. **图像类型**：  
   这是一张**照片**，展示了一个真实场景的室内环境。

2. **图像中的所有文字内容**：  
   图像中**没有可见的文字内容**。背景中的设备或屏幕未显示可读文本。

3. **图像中的关键视觉元素和数据信息**：  
   - 一位坐在轮椅上的老年男性，身穿格子衬衫和绿色裤子，面向一张小圆桌。  
   - 圆桌上放置一台平板电脑或小型笔记本电脑，屏幕上显示蓝色界面（具体内容不可辨识）。  
   - 周围有几把木制椅子和一张沙发，布置类似于家庭客厅或养老院休息区。  
   - 环境整洁、明亮，地面为浅色瓷砖，墙面为白色，整体氛围安静、舒适。  
   - 背景中可见另一位站立的人影（部分遮挡），可能在观察或协助。

4. **图像传达的主要信息**：  
   该图像展示了**具身人工智能（Embodied AI）在实际生活场景中的应用**，特别是智能辅助系统如何与老年人互动。轮椅使用者通过设备与数字系统交互，体现了“将虚拟空间与物理世界对齐”的理念，呼应论文主题——具身AI在智能机电系统和智慧养老等领域的潜力。

【图像 5】(221x190)
1. **图像类型**：  
   这是一张**照片**，展示了一个工业机器人机械臂的特写镜头，具有一定的科技感和专业性，可能用于说明“具身人工智能”（Embodied AI）在物理世界中的应用。

2. **图像中的所有文字内容**：  
   图像中没有直接可见的文字内容。仅在机械臂末端夹持的黄色标签上有一个模糊的黑色图案或标识，但无法辨识具体文字或符号。

3. **图像中的关键视觉元素和数据信息**：  
   - **机械臂结构**：图像主体是一个银白色的多关节工业机器人机械臂，结构精密，具有多个旋转关节，表明其具备高自由度运动能力。
   - **末端执行器**：机械臂末端装有夹持装置，正在抓取一个带有黄色标签的小型物体，该物体可能是传感器、计算模块或某种智能设备。
   - **背景环境**：背景为模糊的实验室或工业场景，蓝色调处理增强了科技氛围，突出前景的机械臂。
   - **连接线缆**：从机械臂延伸出黑色线缆，暗示其与控制系统或电源相连，体现其作为智能系统的组成部分。

4. **图像传达的主要信息**：  
   该图像通过展示一个具备感知与操作能力的机器人系统，直观地体现了“具身人工智能”（Embodied AI）的核心理念——将人工智能嵌入物理实体中，使其能够在真实环境中感知、决策和执行任务。结合论文标题“Aligning Cyber Space with Physical World”，此图强调了虚拟智能与物理世界的融合，突出了智能机电系统（如智能机器人）在实现通用人工智能（AGI）过程中的关键作用。

【图像 6】(234x133)
1. **图像类型**：  
   这是一张**照片**，展示了一个实际的室内仓储环境，其中包含自动化物流设备。

2. **图像中的所有文字内容**：  
   图像中可见的文字包括：
   - “03”（出现在左侧黄色机器人顶部）
   - “02”（出现在中间黄色机器人顶部）
   - “01”（出现在右侧黄色机器人顶部）

3. **图像中的关键视觉元素和数据信息**：
   - **多台黄色自动导引车（AGV）或移动机器人**：共三台，编号分别为01、02、03，整齐排列在仓库通道中。
   - **货架系统**：高大的金属货架，上面堆满了纸箱，表明这是一个存储密集型仓库。
   - **地面标记**：蓝色地面带有黄色引导线，用于指引机器人行驶路径。
   - **环境布局**：典型的现代化智能仓储场景，具有良好的照明和空间规划。
   - **机器人结构**：每台机器人都有平台用于承载货物，顶部装有标识牌和传感器装置。

4. **图像传达的主要信息**：  
   该图像展示了**具身人工智能（Embodied AI）在智能物流与仓储系统中的实际应用**。通过自主移动机器人（AGV）在物理环境中执行搬运任务，体现了“将数字空间与物理世界对齐”的理念，符合论文主题中关于具身AI在智能机电系统中的应用背景。图像强调了AI与物理实体的结合，是实现人工通用智能（AGI）的重要组成部分。

【图像 7】(237x138)
1. **图像类型**：  
   该图是一张**三维渲染照片**（或仿真截图），展示了一个机器人操作场景，属于计算机视觉或机器人学领域的实验环境模拟。

2. **图像中的所有文字内容**：  
   图像中**无可见文字内容**。背景和物体上均未出现可读的文字或标签。

3. **图像中的关键视觉元素和数据信息**：  
   - **机械臂**：一个白色的六自由度机械臂位于画面右侧，末端装有夹爪，正抓取或操作一个绿色瓶状物体。
   - **桌面**：浅灰色的水平台面，作为操作平台，表面平整。
   - **物体**：
     - 三个绿色瓶状物体（可能是饮料瓶或仿真物品），其中两个直立，一个被机械臂抓取。
     - 一个透明玻璃杯，位于瓶子附近。
     - 一个圆形浅色托盘，位于画面右下角。
   - **背景**：浅木纹墙面与浅色地板，构成简洁的室内环境，类似实验室或厨房场景。
   - **光照**：均匀柔和，无强烈阴影，表明为虚拟仿真环境。

4. **图像传达的主要信息**：  
   该图像展示了**具身人工智能（Embodied AI）** 的典型应用场景——机器人在物理环境中执行物体抓取与操作任务。结合上下文论文主题“Aligning Cyber Space with Physical World”，此图旨在说明AI系统如何通过物理交互（如机械臂操作）实现对现实世界的感知与控制，体现“数字空间与物理世界对齐”的核心理念。它可能用于说明机器人感知、动作规划或人机协作等研究方向。

【图像 8】(158x157)
1. **图像类型**：  
   该图像为一张**机器人照片**（或高保真渲染图），展示了一个类人形的机器人，属于实物或仿真模型的视觉呈现。

2. **图像中的所有文字内容**：  
   图像中**无文字内容**。图像本身不包含任何文本、标签或注释。

3. **图像中的关键视觉元素和数据信息**：  
   - **主体**：一个类人形机器人（Humanoid Robot），具有明显的头部、躯干、双臂和肩部结构。
   - **外观特征**：
     - 头部呈流线型，表面光滑，前额中央有一个圆形传感器或摄像头。
     - 躯干部分由金属或复合材料构成，具有分段式设计，可能包含关节和驱动装置。
     - 双臂结构复杂，肩部和上臂可见多个旋转关节，暗示具备高度自由度。
     - 手部未完全显示，但手臂末端有类似机械手的结构。
     - 整体设计偏向工业或科研用途，非消费级产品。
   - **颜色与材质**：以黑色、银灰色为主，表面具有金属光泽，可能为铝合金或碳纤维材料。
   - **姿态**：机器人呈直立姿势，双臂略微外展，似乎处于待命或演示状态。

4. **图像传达的主要信息**：  
   该图像用于说明“具身人工智能”（Embodied AI）的研究对象——即能够与物理世界交互的智能机器人系统。结合上下文（论文标题《Aligning Cyber Space with Physical World: A Comprehensive Survey on Embodied AI》），此图强调了**智能体在物理空间中的具身性**（embodiment），突出机器人作为连接虚拟（Cyber）与现实（Physical）世界的桥梁作用。它象征着具身AI的核心理念：通过身体感知与行动实现智能决策，是迈向通用人工智能（AGI）的重要路径。

【图像 9】(139x138)
1. **图像类型**：  
   该图是一张**三维机械臂的示意图**，属于**技术插图或工程图形**，用于展示机械结构。

2. **图像中的所有文字内容**：  
   图像中包含以下标注文字：  
   - “30.4”（标注在机械臂上端关节处）  
   - “15.2”（标注在中间关节处）  
   - “26.9”（标注在底座连接处）  
   - “75°”（标注在机械臂末端与基座之间的角度）

3. **图像中的关键视觉元素和数据信息**：  
   - 一个**多关节机械臂**，具有三个主要旋转关节，整体呈银灰色金属质感。  
   - 机械臂从底部基座向上延伸，分为三段连杆，每段之间通过球形或旋转关节连接。  
   - 标注了多个**尺寸参数**：  
     - “30.4”、“15.2”、“26.9” 可能表示各节臂的长度或直径（单位未标明，推测为毫米）。  
     - “75°” 表示末端执行器与基座之间的夹角，可能代表运动范围或当前姿态。  
   - 整体设计简洁、对称，符合工业机器人或仿人机械臂的典型结构。

4. **图像传达的主要信息**：  
   该图展示了一个典型的**拟人化或多自由度机械臂结构**，用于说明**具身人工智能（Embodied AI）中物理实体的形态与运动能力**。结合上下文“Aligning Cyber Space with Physical World”，此图意在强调**智能系统如何通过物理载体（如机械臂）与现实世界交互**，是实现通用人工智能（AGI）的重要组成部分。图中尺寸和角度标注表明其具备精确控制和空间定位能力，突出了**机械结构与智能控制的融合**。

【图像 10】(127x178)
1. **图像类型**：  
   该图是一张**三维渲染的图形**，展示了一只机械手（机器人手）的结构示意图。

2. **图像中的所有文字内容**：  
   图像中**没有文字内容**。

3. **图像中的关键视觉元素和数据信息**：  
   - 图像主体为一只**仿生机械手**，具有五个手指，每个手指由多个关节构成，模拟人类手指的运动结构。  
   - 手掌部分为黑色，指节部分为白色与黑色相间，关节处有金属质感的连接件。  
   - 手腕部分为银白色圆柱形结构，连接手掌，暗示其可旋转或活动。  
   - 整体设计强调**灵活性与仿生性**，可能用于人机交互、智能机械系统或机器人抓取任务。  
   - 图像风格为**高精度3D建模渲染**，背景为纯白，突出机械手本身。

4. **图像传达的主要信息**：  
   该图像展示了**具身人工智能（Embodied AI）中的典型硬件载体——仿生机械手**，强调其在物理世界中的交互能力。结合上下文（关于“具身AI”的综述），此图旨在说明具身智能如何通过物理实体（如机械臂、机器人手）与环境互动，实现感知、操作与学习的闭环，是迈向通用人工智能（AGI）的重要组成部分。

【图像 11】(355x211)
1. **图像类型**：  
   这是一张**照片**，展示了一个实验室或工业环境中的机器人系统。

2. **图像中的所有文字内容**：  
   图像中**没有可见的文字内容**。

3. **图像中的关键视觉元素和数据信息**：  
   - 两台黑色的机械臂（机器人手臂），带有蓝色电缆连接，正在协同操作。  
   - 机械臂末端装有夹持器或执行器，正对准透明塑料试管或容器。  
   - 工作台上放置了多个透明的实验托盘和试管架，表明这是一个自动化操作平台。  
   - 背景为实验室环境，包含金属框架、工作台和部分设备，整体整洁有序。  
   - 机械臂动作精准，可能正在进行液体转移、样品分拣或装配任务。

4. **图像传达的主要信息**：  
   该图像展示了**具身人工智能（Embodied AI）在物理世界中的实际应用**，具体表现为机器人系统通过感知与执行实现自动化任务，体现了“将网络空间与物理世界对齐”的核心理念。它支持论文主题——具身AI作为实现通用人工智能（AGI）的基础，并广泛应用于智能机电系统等场景。

【图像 12】(355x212)
1. **图像类型**：  
   这是一张**照片**，展示了一个实际的物理场景，具体为一个实验室或智能厨房环境中的机器人系统。

2. **图像中的所有文字内容**：  
   图像中**没有可见的文字内容**。图像本身不包含任何文本标注、标签或说明性文字。

3. **图像中的关键视觉元素和数据信息**：  
   - **机械臂系统**：图像中心是两个并排的多关节机械臂，具有类似人类手臂的结构，由多个旋转关节和黑色与银色部件组成，末端装有夹持器。
   - **操作环境**：背景是一个现代化厨房环境，包括白色橱柜、黑色灶台、不锈钢锅具（如带盖的锅和汤锅）、以及一些厨具（如漏勺、油瓶）。
   - **任务执行**：左侧机械臂正抓取一个金属托盘，右侧机械臂似乎在操作锅具，表明其正在进行烹饪相关的任务。
   - **传感器与连接线**：机械臂上可见若干电缆和连接件，暗示其具备传感反馈和控制系统。
   - **整体布局**：设备布置整齐，体现了高度集成的自动化系统设计，可能用于研究或演示“具身人工智能”（Embodied AI）在现实世界中的应用。

4. **图像传达的主要信息**：  
   该图像展示了**具身人工智能（Embodied AI）在物理世界中的实际应用实例**——即机器人通过与真实环境互动（如厨房操作）来完成复杂任务。它强调了AI系统不仅需要智能算法，还需具备感知、运动控制和物理交互能力，以实现与真实世界的深度融合，呼应论文主题“将虚拟空间与物理世界对齐”。


================================================================================
第 2 页
================================================================================

【文本内容】
IEEE/ASME TRANSACTIONS ON MECHATRONICS
2
TABLE I
COMPARISON BETWEEN DISEMBODIED AI AND EMBODIED AI.
Type
Environment
Physical Entities
Description
Representative Agents
Disembodied AI
Cyber Space
No
Cognition and physical entities are disentangled
ChatGPT [4], RoboGPT [8]
Embodied AI
Physical Space
Robots, Cars, Other devices
Cognition is integrated into physical entities
RT-1 [9], RT-2 [10], RT-H [3]
simulated and physical environments [5], [6]. As we stand
at the forefront of AGI-driven innovation, it is crucial to
delve deeper into the realm of embodied AI, unraveling their
complexities, evaluating their current developmental stage,
and contemplating the potential trajectories they may follow
in the future. Nowadays, embodied AI contains various key
techniques across Computer Vision (CV), Natural Language
Processing (NLP), and robotics, with the most representative
being embodied perception, embodied interaction, embodied
agents, and sim-to-real robotic control [7]. Therefore, it is
imperative to capture the evolving landscape of embodied AI
in the pursuit of AGI through a comprehensive survey.
Embodied agent is the most prominent basis of embodied
AI. For an embodied task, the embodied agent must fully un-
derstand the human intention in language instructions, actively
explore the surrounding environments, comprehensively per-
ceive the multi-modal elements from both virtual and physical
environments, and execute appropriate actions for complex
tasks [11], [12], as shown in Fig. 1. The rapid progress in
multi-modal models exhibits superior versatility, dexterity, and
generalizability in complex environments compared to tra-
ditional deep reinforcement learning approaches. Pre-trained
visual representations from state-of-the-art vision encoders
[13], [14] provide precise estimations of object class, pose,
and geometry, which makes the embodied models thoroughly
perceive complex and dynamic environments. Powerful Large
Language Models (LLMs) make robots better understand the
linguistic instructions from humans. Promising MLMs give
a feasible approach for aligning the visual and linguistic
representations of embodied robots. The world models [15],
[16] exhibit remarkable simulation capabilities and promis-
ing comprehension of physical laws, which makes embodied
models comprehensively understand both the physical and real
environments. These innovations empower embodied agents to
comprehensively perceive complex environment, interact with
humans naturally, and execute tasks reliably.
Despite the intensive interest in harvesting the powerful
perception and reasoning ability from MLMs, the research
community is short of a comprehensive survey that can help
sort out existing embodied AI studies, the challenges faced,
as well as future research directions. In the era of MLMs, we
aim to fill up this gap by performing a systematic survey of
embodied AI across cyber space to physical world. We conduct
the survey from different perspectives including embodied
robots, simulators, four representative embodied tasks (visual
active perception, embodied interaction, multi-modal agents
and sim-to-real adaptation), and future research directions. We
believe that this survey will provide a clear big picture of what
we have achieved, and we could further achieve along this
emerging yet very prospective research direction.
Differences from previous works: Although there have
been several survey papers [5], [6], [17], [18] for embodied AI,
most of them are outdated as they were published before the
era of MLMs, which started around 2023. To the best of our
knowledge, there are only two survey papers [6], [18] after
2023, which focused on vision-language-action models and
embodied AI system for smart manufacturing, respectively.
Embodied AI, with AI brain, Body and Cross-modal sensors,
is first proposed in [18], which is also the first work to
propose technical architecture of embodied AI system for
future smart manufacturing in the era of foundation model.
However, the MLMs, WMs and embodied agents are not fully
considered in previous surveys. Additionally, recent develop-
ments in embodied robots and simulators are also overlooked.
To address the scarcity of comprehensive survey papers in
this rapidly developing field, we propose this comprehensive
survey that covers representative embodied robots, simulators,
and four main research tasks: embodied perception, embodied
interaction, embodied agents, and sim-to-real adaptation. In
summary, the main contributions of this work are threefold:
• To the best of our knowledge, this is the first comprehen-
sive survey of embodied AI from the perspective of the
alignment of cyber and physical spaces based on MLMs
and WMs, offering novel insights about methodologies,
benchmarks, challenges, and applications.
• We categorize and summarize embodied AI into several
essential parts including robots, simulators, and four main
research tasks: embodied perception, embodied interac-
tion, embodied agents and sim-to-real adaptation, which
serve as a detailed taxonomy of embodied AI.
• To facilitate the development of robust, general-purpose
embodied agents, we propose a new dataset standard
ARIO (All Robots In One) and a unified large-scale
ARIO dataset, encompassing approximately 3 million
episodes collected from 258 series and 321,064 tasks.
The rest of this survey is organized as follows. Section
2 introduces embodied robots. Section 3 describes general
and real-scene embodied simulators. Section 4 introduces
embodied perception, including active visual perception and
visual language navigation. Section 5 introduces embodied
interaction. Section 6 introduces embodied agents including
the embodied multi-modal foundation model and embodied
task planning. Section 7 introduces sim-to-real adaptation
including embodied world model, data collection and training.
In Section 8, we discuss promising research directions.
II. EMBODIED ROBOTS
Embodied agents interact with the physical environment,
including robots, smart appliances, and autonomous vehicles,
etc. Fixed-base robots, shown in Fig. 2 (a), are used in
laboratory automation and industry due to their precision, e.g.,
Franka Emika panda [19], [20], Kuka iiwa [21], [22], and
Sawyer [23], [24]. Wheeled robots, depicted in Fig. 2 (b),
are efficient in logistics and warehousing due to their simple
structure and low cost, like Kiva and Jackal robots [25]. They
face challenges on uneven terrain. Tracked robots, shown in



================================================================================
第 3 页
================================================================================

【文本内容】
IEEE/ASME TRANSACTIONS ON MECHATRONICS
3
TABLE II
GENERAL SIMULATOR. HFPS: HIGH-FIDELITY PHYSICAL SIMULATION; HQGR: HIGH-QUALITY GRAPHICS RENDERING; RRL: RICH ROBOT LIBRARY;
DLS: DEEP LEARNING SUPPORT; LSPC: LARGE-SCALE PARALLEL COMPUTING; ROS: TIGHT INTEGRATION WITH ROS; MSS: MULTIPLE SENSOR
SIMULATION; CP: CROSS-PLATFORM NAV: ROBOT NAVIGATION AD: AUTO DRIVING; RL: REINFORCEMENT LEARNING LSPS: LARGE-SCALE PARALLEL
SIM MR: MULTI-ROBOT SYSTEMS RS: ROBOT SIMULATION. ◦INDICATES THAT THE SIMULATOR EXCELS AT THIS ASPECT.
Simulator
Year
HFPS
HQGR
RRL
DLS
LSPC
ROS
MSS
CP
Physics Engine
Main Applications
Genesis [35]
2024 ◦
◦
◦
◦
◦
◦
◦
Custom
RL, LSPS, RS
Isaac Sim [36]
2023 ◦
◦
◦
◦
◦
◦
◦
◦
PhysX
Nav, AD
Isaac Gym [37]
2019
◦
◦
◦
PhysX
RL,LSPS
Gazebo [38]
2004
◦
◦
◦
◦
◦
ODE, Bullet, Simbody, DART Nav,MR
PyBullet [39]
2017
◦
◦
Bullet
RL,RS
Webots [40]
1996
◦
◦
◦
◦
ODE
RS
MuJoCo [41]
2012 ◦
◦
◦
Custom
RL, RS
Unity ML-Agents [42]
2017
◦
◦
◦
Custom
RL, RS
AirSim [43]
2017
◦
◦
Custom
Drone sim, AD, RL
MORSE [44]
2015
◦
◦
Bullet
Nav, MR
V-REP (CoppeliaSim) [45] 2013
◦
◦
◦
◦
Bullet, ODE, Vortex, Newton MR, RS
(a)
(d)
Quadruped Robots
(Boston Dynamics Spot)
(e) Humanoid Robots
(Tesla Optimus)
(b)
Wheeled Robots
(Jackal robot)
(c) Tracked Robots
(iRobot PackBot)
(f) Biomimetic Robots
Fixed-base Robots
(Franka Emika Panda)
Fig. 2. The Embodied Robots include Fixed-base Robots, Quadruped Robots,
Humanoid Robots, Wheeled Robots, Tracked Robots, and Biomimetic Robots.
Fig. 2 (c), are ideal for off-road tasks such as agriculture
and disaster recovery. Their track systems provide stability
on soft terrains [26] Quadruped robots, illustrated in Fig. 2
(d), excel in complex terrain exploration and rescue missions.
Examples include Unitree Robotics’ A1 and Go1, and Boston
Dynamics Spot. Humanoid robots, shown in Fig. 2 (e), mimic
human movements and behaviors to provide personalized ser-
vices. Their dexterous hands enable them to perform intricate
tasks [27], [28]. With LLM, these robots are anticipated to
enhance efficiency and safety in manufacturing, healthcare,
and services [29]. Biomimetic robots, depicted in Fig. 2 (f),
replicate the movements and functions of natural organisms.
This simulation aids in operating within complex environ-
ments and boosts energy efficiency by emulating biological
mechanisms [30], [31]. Examples include fish-like [32], insect-
like [33], and soft-bodied robots [34].
III. EMBODIED SIMULATORS
Embodied simulators are crucial for embodied AI due
to their cost-effectiveness, safety features, scalability, rapid
prototyping, and accessibility for research. They allow for
controlled experimentation, data generation for training and
evaluation, and standardized benchmarks. To facilitate inter-
action with the environment, it’s essential to build realistic
simulations by considering physical characteristics, object
properties, and their interactions. This section will introduce
the commonly used simulation platforms in two parts: the
general simulator based on underlying simulation and the
simulator based on real scenes.
Isaac Sim
Pybullet
Gazebo
Webots
MuJoCo
Unity ML-Agents
AirSim
MORSE
V-REP (CoppeliaSim)
Genesis
Fig. 3. Examples of General Simulators. The MuJoCo’s figure is from [46].
A. General Simulator
The physical interactions and dynamic changes present
in real environments are irreplaceable. However, deploying
embodied models in the physical world often incurs high costs
and faces numerous challenges. General-purpose simulators
provide a virtual environment that closely mimics the physical
world, allowing for algorithm development and model training,
which offers significant cost, time, and safety advantages.
Isaac Sim [36] is an advanced simulation platform for
robotics and AI research. It has high-fidelity physical sim-
ulation, real-time ray tracing, an extensive library of robotic
models, and deep learning support. Its application scenarios in-
clude autonomous driving, industrial automation, and human-
robot interaction. Gazebo [47] is an open-source simulator
for robotics research. It has extensive robot libraries, and
tight integration with ROS. It supports the simulation of
various sensors and offers numerous pre-built robot models
and environments. It is mainly used for robot navigation
and control and multi-robot systems. PyBullet [39] is the
python interface for the Bullet physics engine. It is easy to
be used and has diverse sensor simulation and deep learning
integration. PyBullet supports real-time physical simulation,
including rigid body dynamics, collision detection, and con-
straint solving. Moreover, the newly launched Genesis [35]
has differentiable physics engine and impressive generative
capabilities. Table. II presents the key features and primary
application scenarios of 11 general-purpose simulators. Fig. 3
shows the visualization effects of the general simulators.
B. Real-Scene Based Simulators
Achieving universal embodied agents in household activities
is a primary focus in embodied AI. These embodied agents
need to deeply understand human daily life and perform
complex embodied tasks such as navigation and interaction in
indoor environments. To meet the demands of these complex
tasks, the simulated environments need to be close to the


【图像 1】(715x375)
1. **图像类型**：  
   这是一张**机器人设备的渲染图**（3D图形/示意图），展示了一个具有机械臂和履带式底盘的机器人系统。图像风格为高精度技术渲染，背景为纯白色，突出主体结构。

2. **图像中的所有文字内容**：  
   图像中**没有可见的文字内容**。所有文本信息来自页面上下文，与图像本身无关。

3. **图像中的关键视觉元素和数据信息**：  
   - **履带式底盘**：顶部为黑色履带结构，带有两个轮子和一个中央控制模块，表面有显示屏或传感器区域。
   - **机械臂**：从底盘延伸出一条多关节机械臂，由多个金属节段组成，具有灵活的运动能力。
   - **末端执行器**：机械臂末端装有一个夹持装置（类似钳子），用于抓取物体。
   - **摄像头模块**：在机械臂底部安装有一个小型摄像头或传感器单元。
   - **天线**：从底盘后部伸出两根细长的黑色天线，可能用于无线通信。
   - **整体设计**：结构紧凑，具备移动性（履带）和操作性（机械臂），适用于复杂环境下的任务执行。

4. **图像传达的主要信息**：  
   该图像展示了一款**多功能移动机器人平台**，结合了自主移动能力（履带）、精密操作能力（机械臂）和感知能力（摄像头、传感器）。其设计表明适用于科研、工业自动化或机器人仿真测试场景，尤其可能用于**机器人导航（NAV）、自动驾驶（AD）或强化学习（RL）实验**，与表格中提到的“RRL”（Rich Robot Library）、“MSS”（Multiple Sensor Simulation）等特性相呼应。

【图像 2】(406x188)
1. **图像类型**：  
   该图像为一张**三维渲染图**（3D rendering），展示了一个软体机器人鱼的结构模型，属于**图形/示意图**类型。

2. **图像中的所有文字内容**：  
   - 图像下方标注文字：“**Soft Robotic Fish**”  
   - 图像右侧部分可见“**D**”（可能为表格或图表标题的一部分，但不完整）

3. **图像中的关键视觉元素和数据信息**：  
   - 主体是一个**仿生软体机器人鱼**，外形类似鱼类，具有流线型身体。
   - 鱼身由多个模块组成：
     - 外壳为半透明材质，可看到内部结构。
     - 尾鳍为红色，呈扇形，用于推进。
     - 背鳍也为红色，位于背部中部。
     - 内部包含电子元件（如电路板、电机等），显示其机械与电子集成设计。
     - 鱼头部分有传感器或控制单元的结构。
     - 整体采用绿色、灰色、蓝色等颜色区分不同组件。
   - 图像呈现的是一个**剖面视图**，展示了机器人鱼的内部构造与外部形态结合。

4. **图像传达的主要信息**：  
   该图像展示了一种**软体机器人鱼的设计与结构**，强调其仿生外形与内部机电一体化特征。结合上下文（IEEE/ASME TRANSACTIONS ON MECHATRONICS）及表格内容，此图像可能是用于说明某类机器人仿真平台中支持的机器人类型（如“RS: ROBOT SIMULATION”），特别是用于水下或生物启发式机器人研究的软体机器人系统。

【图像 3】(370x289)
1. **图像类型**：  
这是一张**实物照片**，展示了一个具有生物形态特征的机器人或仿生装置。

2. **图像中的所有文字内容**：  
图像中**没有可见的文字内容**。

3. **图像中的关键视觉元素和数据信息**：  
- 一个透明、半透明结构的机械装置，外形类似章鱼（Octopus），拥有多个柔软、弯曲的“触手”。
- “触手”呈浅蓝绿色，具有柔性和灵活性，可能由柔性材料制成，用于抓取或操作。
- 装置中央有一个类似大脑或控制单元的球状透明部件，内部有微小的发光点（可能是传感器或电路）。
- 该装置连接到一个金属或塑料框架上，框架包含一些电子元件和接口，可能用于供电或数据传输。
- 一根黑色线缆从装置底部延伸出去，表明其与外部系统相连。
- 整体设计结合了生物学形态与工程技术，强调仿生学与软体机器人技术。

4. **图像传达的主要信息**：  
该图像展示了一种**仿生软体机器人**，可能用于探索复杂环境或执行精细操作任务。其设计灵感来自章鱼，突出**柔韧性、适应性与多功能性**，体现了先进机器人技术在仿生结构与智能控制方面的进展。结合上下文（如“机器人仿真”、“多机器人系统”等），此图像可能用于说明新型机器人平台的设计理念或实验原型。

【图像 4】(302x826)
1. **图像类型**：  
   该图是一张**3D渲染图形**，展示了一个机器人模型的俯视图（从上方视角），具有高度写实的金属质感和光影效果。

2. **图像中的所有文字内容**：  
   图像中**无可见文字内容**。仅在页面上下文（表格II）中提及了与图像相关的术语，但这些不在图像本身内。

3. **图像中的关键视觉元素和数据信息**：  
   - **机器人结构**：图像展示了一个双足机器人，具有两条腿、两个机械臂和一个躯干部分。整体设计对称，呈现人形轮廓。
   - **腿部**：两条腿向上伸展，脚部呈尖状，可能用于行走或平衡。
   - **手臂**：双臂向两侧展开，末端为夹爪式机械手，具备抓取功能。
   - **材质与光泽**：表面为银灰色金属材质，带有高光反射，表现出精密工程设计。
   - **视角**：从正上方俯视，强调机器人的对称性和结构布局。
   - **背景**：纯黑色背景，突出机器人主体，增强科技感和未来感。

4. **图像传达的主要信息**：  
   该图像旨在展示一个**高度拟真的人形机器人模型**，可能用于说明机器人仿真平台（如文中提到的“HIGH-FIDELITY PHYSICAL SIMULATION”）的能力。结合上下文中的“ROBOT SIMULATION”、“MULTI-ROBOT SYSTEMS”等关键词，此图可能用于体现仿真系统中对复杂机器人结构的建模能力，尤其是其物理细节、运动学结构以及多自由度机械臂的设计。它强调了仿真环境在支持高保真度机器人行为模拟方面的潜力。

【图像 5】(599x599)
1. **图像类型**：  
   该图是一张**三维渲染图形**（3D rendering），展示了一种仿生飞行机器人在空中的动态姿态。

2. **图像中的所有文字内容**：  
   图像中**无可见文字内容**。

3. **图像中的关键视觉元素和数据信息**：  
   - 主体是一个**仿昆虫结构的微型飞行机器人**，具有四片类似昆虫翅膀的薄翼，呈对称分布。
   - 机身主体为黑色框架，内部包含黄色电路板和红色电子元件，显示其为电子驱动系统。
   - 翅膀处于运动状态，带有模糊效果，表明正在高速拍打，模拟真实飞行。
   - 在机器人下方有一个小的蓝色矩形物体，可能是传感器、目标物或测试设备的一部分。
   - 整体背景为纯白色，突出机器人主体，强调其结构与动态特性。

4. **图像传达的主要信息**：  
   该图像展示了一种**高仿生度的微型飞行机器人**，可能用于研究仿生飞行机制、机器人导航或自主控制。结合上下文中的“ROBOT NAVIGATION”、“AUTO DRIVING”、“REINFORCEMENT LEARNING”等关键词，此机器人可能被用于**机器人导航、强化学习训练或物理仿真环境中的实验平台**，体现了先进机器人技术与仿生设计的结合。

【图像 6】(1751x1536)
1. **图像类型**：  
   该图是一张**3D渲染的机器人机械臂产品图**，属于**图形/示意图**类型，用于展示机械臂的外观和结构。

2. **图像中的所有文字内容**：  
   图像中可见的文字为：  
   - “**SHARKY ROBOT**” —— 印刷在右侧机械臂的黑色部分上。

3. **图像中的关键视觉元素和数据信息**：  
   - 图中展示了两个相同的**白色机械臂**，呈对称布局，从顶部固定基座延伸出，具有多个关节，形态流畅。  
   - 每个机械臂由多个旋转关节组成，末端装有**夹持器（gripper）**，用于抓取物体。  
   - 机械臂的底座部分带有蓝色指示灯或接口，可能表示传感器或通信端口。  
   - 机械臂整体设计现代、紧凑，表面光滑，具备工业级外观。  
   - 右侧机械臂的中部有一个黑色部件，上面印有“SHARKY ROBOT”，表明品牌或型号信息。  
   - 无坐标轴、数值或标签等量化信息，但通过结构可推断其为**多自由度协作机械臂**。

4. **图像传达的主要信息**：  
   该图像展示了**Sharky Robot品牌的协作型机械臂**，强调其**灵活的运动能力、模块化设计和高精度控制潜力**。结合上下文（如表格中提到的“RICH ROBOT LIBRARY”、“ROBOT SIMULATION”等），此图很可能用于说明**机器人仿真平台中支持的真实机器人模型**，或作为**高保真物理模拟（HFPS）的对象**，体现其在机器人仿真研究中的应用价值。

【图像 7】(400x400)
1. **图像类型**：  
   该图像为一张**产品渲染图**（或3D模型图），展示了一款机器人平台的结构设计，属于技术设备类图形。

2. **图像中的所有文字内容**：  
   图像中可见的文字为：  
   - “**e-Puck**”（位于机器人车体侧面，品牌或型号标识）

3. **图像中的关键视觉元素和数据信息**：  
   - **主体结构**：一台小型地面移动机器人，具有三个轮子（两前一后）的差速驱动结构，适用于室内环境。  
   - **颜色与外观**：车身主要为黑色，底部有黄色支撑板，整体设计紧凑。  
   - **传感器与模块**：  
     - 前方装有一个摄像头模组（可能是RGB-D或普通相机），用于视觉感知。  
     - 下方安装有类似激光雷达（LiDAR）或超声波传感器的装置，用于环境探测。  
     - 一根细长的天线从右侧伸出，可能用于无线通信（如Wi-Fi、蓝牙）。  
   - **机械结构**：机器人底部通过金属支架支撑传感器，具备一定的悬架或调节能力，以适应不同地形。  

4. **图像传达的主要信息**：  
   该图展示了一种**集成多种传感器的移动机器人平台**，强调其在导航、感知和自主控制方面的硬件配置。结合上下文（IEEE/ASME Transactions on Mechatronics 及表格中提到的“NAV: ROBOT NAVIGATION”、“MSS: MULTIPLE SENSOR SIMULATION”等），此图像可能用于说明**机器人仿真系统中所模拟的物理机器人原型**，尤其是支持多传感器融合与导航任务的高保真度物理仿真对象。  

综上，这是一张用于说明机器人硬件结构的示意图，重点突出其传感器集成能力和移动平台设计，服务于机器人仿真与自动化研究场景。

【图像 8】(439x439)
1. **图像类型**：  
   这是一张产品实物照片，展示了一款机器人或机械装置的外观。

2. **图像中的所有文字内容**：  
   图像中可见的文字为“**d-robotics**”，位于设备主体黄色部分的侧面，是品牌或公司标识。

3. **图像中的关键视觉元素和数据信息**：  
   - 设备主体为黄色与黑色相间，结构紧凑，具有多个机械臂状结构（共四条黑色臂），呈对称分布，末端带有连接件或传感器接口。  
   - 中央部分有一个控制模块，包含按钮和显示屏区域，可能用于操作或状态显示。  
   - 顶部有一块透明的弧形罩板，可能是防护罩或传感器窗口。  
   - 底部为白色平台，可能用于安装或固定。  
   - 整体设计具有工业感和科技感，暗示其为高精度机器人系统的一部分。

4. **图像传达的主要信息**：  
   该图展示了一款由 d-robotics 公司开发的机器人硬件设备，可能用于机器人仿真、多机器人系统或物理交互场景。结合上下文（如“HIGH-FIDELITY PHYSICAL SIMULATION”、“MULTI-ROBOT SYSTEMS”等术语），此设备很可能是用于高保真物理模拟或机器人实验的硬件平台，强调其在机器人技术领域的应用价值。

【图像 9】(495x265)
1. **图像类型**：  
   该图是一张**三维仿真环境的截图**，展示了一个机器人仿真平台中的虚拟场景。

2. **图像中的所有文字内容**：  
   图像中**无可见文字内容**。所有文本信息来自页面上下文（如表格标题和注释），但未直接出现在图像本身。

3. **图像中的关键视觉元素和数据信息**：  
   - **场景结构**：一个工业风格的室内环境，包含砖墙、金属地板、天花板和通道，具有工厂或仓库的布局特征。
   - **机器人设备**：左侧有一个蓝色的多层机械结构，带有红色托盘，疑似自动化仓储系统或搬运机器人；右侧有一条橙色的机械臂，可能用于抓取或装配任务。
   - **传送带系统**：中央区域有多个平行排列的传送带，表明这是一个物流或生产流程模拟场景。
   - **建筑细节**：背景中有类似货架和控制台的结构，增强了工业仿真的真实感。
   - **光照与视角**：采用第一人称视角，顶部有光源照明，整体光照均匀，显示为高保真图形渲染（HQGR）效果。

4. **图像传达的主要信息**：  
   该图像展示了一个**高保真度的机器人仿真环境**，支持复杂工业场景下的机器人行为测试，例如自动化物流、机械臂操作和多机器人协作。结合上下文中的“HFPS”（高保真物理仿真）、“HQGR”（高质量图形渲染）和“RRL”（丰富机器人库）等术语，可推断此图用于说明某款机器人仿真器在**物理精度、图形表现和功能多样性**方面的优势，适用于机器人导航（NAV）、自动驾驶（AD）和强化学习（RL）等研究领域。

【图像 10】(290x293)
1. **图像类型**：  
   该图是一张**3D图形渲染截图**，属于**虚拟仿真环境中的机器人操作场景**，可能来自一个机器人仿真平台（如Gazebo、PyBullet或类似工具）。

2. **图像中的所有文字内容**：  
   图像中**没有可见的文字内容**。所有文本信息均来自页面上下文（如表格标题和注释），但未直接出现在图像本身中。

3. **图像中的关键视觉元素和数据信息**：  
   - **机器人机械臂**：画面右侧有一个白色与橙色相间的机械臂，末端装有夹爪，正伸向桌面上的物体。
   - **桌面与物体**：桌面上有两个白色托盘，分别装有橙色水果（可能是橙子）和绿色水果（可能是苹果或青柠）。机械臂似乎正在抓取或操作这些物品。
   - **厨房环境模拟**：背景为仿真的厨房环境，包括红色橱柜、木质台面、黑色椅子等，整体风格偏向生活化场景。
   - **高保真图形渲染**：物体具有明显的光影效果和材质质感，显示其具备**高质量图形渲染（HQGR）** 特性。
   - **传感器与执行器布局**：机械臂结构复杂，包含多个关节，暗示支持**多自由度运动控制**，适合用于**机器人导航（NAV）** 或**自动驾驶（AD）** 等任务的仿真测试。

4. **图像传达的主要信息**：  
   该图像展示了一个**高保真度的机器人仿真环境**，重点突出机器人在复杂生活场景中的**物体交互能力**，特别是**抓取与操作任务**。结合上下文中的“HFPS”（高保真物理仿真）、“HQGR”（高质量图形渲染）和“MSS”（多传感器仿真）等术语，可推断此图用于说明某款机器人仿真器在**物理真实感、视觉表现力和任务复杂性**方面的优势，适用于**强化学习（RL）** 和**多机器人系统（MR）** 的研究与训练。

【图像 11】(425x334)
1. **图像类型**：  
   该图是一张**3D计算机图形渲染截图**，展示了一个机器人仿真环境中的场景，属于**机器人仿真软件的可视化输出**。

2. **图像中的所有文字内容**：  
   图像中**没有可见的文字内容**。所有文字信息均来自页面上下文（如表格标题和注释），但未在图像本身中显示。

3. **图像中的关键视觉元素和数据信息**：  
   - **机器人模型**：画面中央是一个具有多个关节和轮式结构的机器人，主体为浅蓝色和黄色，带有机械臂和传感器部件，可能用于模拟移动与操作任务。
   - **机械臂与抓取动作**：左侧有一只机械臂正抓取一个红色的小型物体（可能是传感器或工具），表明仿真支持**精细操作任务**。
   - **立方体障碍物**：左上方有一个白色立方体，可能代表环境中的静态障碍物或目标对象。
   - **背景环境**：地面为黑白棋盘格图案，常见于虚拟仿真环境以辅助空间感知；背景较暗，突出前景物体。
   - **多机器人系统暗示**：右上方有一个半透明的机器人轮廓，可能表示**多机器人协同**或**时间步态叠加**效果，暗示支持**多机器人系统（MR）仿真**。
   - **高保真图形渲染**：整体光照、阴影和材质表现较为细腻，符合“高保真图形渲染（HQGR）”特征。

4. **图像传达的主要信息**：  
   该图像展示了**高保真机器人仿真环境**中的典型场景，强调了**机器人操作、多机器人协作、复杂环境交互**等能力。结合上下文中的表格内容（如HQGR、RRL、MSS等），可推断此图用于说明某款机器人仿真器具备**高质量图形渲染、丰富机器人库、多传感器支持**等功能，适用于**机器人导航、自动驾驶、强化学习训练**等应用。

【图像 12】(338x272)
1. **图像类型**：  
   该图像为**3D图形渲染截图**，展示了一个机器人或机械结构的虚拟模型，属于计算机仿真环境中的可视化内容。

2. **图像中的所有文字内容**：  
   图像中**无可见文字内容**。背景和模型上均未包含可读文本。

3. **图像中的关键视觉元素和数据信息**：  
   - **主体对象**：一个由多个部件组成的复杂机械结构，具有类机器人特征，包括：
     - 一个带有卡通狗头造型的头部（棕色毛发、白色面部）；
     - 蓝色和白色的关节式机械臂与躯干；
     - 多个蓝色圆柱形连接件，构成类似“蛇形”或“柔性臂”的结构；
     - 一个橙色圆形底座，可能代表轮子或传感器模块；
     - 中央躯干部分有一个三角形标志，内含符号（可能是品牌或系统标识）。
   - **颜色与材质**：整体采用明亮色彩（蓝、白、棕、橙），表面光滑，具有典型的3D建模风格。
   - **空间布局**：模型呈俯视视角，位于灰色网格背景之上，表明其处于虚拟仿真环境（如物理引擎或机器人仿真平台）中。
   - **附加元素**：模型上方有一个绿色立方体，可能表示目标物体或交互点；紫色环状物可能表示轨迹、力场或传感器范围。

4. **图像传达的主要信息**：  
   该图展示了**一个高保真度的机器人仿真模型**，用于演示机器人在虚拟环境中的结构设计与行为模拟。结合上下文中的“HFPS: HIGH-FIDELITY PHYSICAL SIMULATION”和“HQGR: HIGH-QUALITY GRAPHICS RENDERING”，此图像很可能来自一个支持高质量图形渲染和物理仿真的机器人仿真平台，用于研究机器人导航、多机器人系统或多传感器集成等任务。

【图像 13】(479x272)
1. **图像类型**：  
   该图是一张**计算机生成的仿真环境截图**，展示了一个自动驾驶或机器人导航场景。图像包含主视图和多个嵌入式子图（小窗口），用于显示不同传感器或处理模块的输出。

2. **图像中的所有文字内容**：  
   图像中未包含明显可读的文字内容。仅在右上角有一个红色圆形图标（可能是界面按钮），其余区域无文本信息。

3. **图像中的关键视觉元素和数据信息**：  
   - **主视图**：从车辆前挡风玻璃视角拍摄的三维渲染道路场景，包括道路、树木、天空、电线杆和远处建筑物，具有较高的图形真实感。
   - **左上角小图**：灰度图像，疑似**激光雷达（LiDAR）点云投影**，显示了前方障碍物和地面的轮廓，顶部有白色条状结构可能代表传感器安装位置。
   - **中上部小图**：彩色语义分割图，使用不同颜色标注环境类别（如绿色为草地、紫色为道路、粉色为建筑等），表明系统具备**语义理解能力**。
   - **右上角小图**：带有“Depth Map”标签的深度图（尽管标签不清晰，但根据上下文推断），显示场景中物体的深度信息，近处较亮、远处较暗，用于空间感知。
   - **底部绿色文本框**：左侧有少量绿色字符，可能是调试日志或状态信息，但无法辨识具体内容。

4. **图像传达的主要信息**：  
   该图展示了**一个高保真度机器人仿真平台**（如用于自动驾驶）的多模态感知能力，整合了：
   - 视觉（RGB图像）
   - 深度感知
   - LiDAR点云
   - 语义分割

   结合上下文中的“HQGR: HIGH-QUALITY GRAPHICS RENDERING”、“MSS: MULTIPLE SENSOR SIMULATION”、“AD: AUTO DRIVING”，此图旨在说明该仿真器支持**多传感器融合与复杂环境建模**，适用于自动驾驶算法测试与训练。

【图像 14】(400x300)
1. **图像类型**：  
   该图是一张**三维计算机图形渲染截图**，属于机器人仿真环境的可视化展示，可能来自一个机器人仿真软件（如Gazebo、Webots等）。

2. **图像中的所有文字内容**：  
   图像中**无可见文字内容**。所有文本信息均来自页面上下文（如表格标题和注释），但未直接出现在图像本身。

3. **图像中的关键视觉元素和数据信息**：  
   - **场景布局**：图像展示了一个室内三维仿真环境，视角为俯视略偏斜。
   - **机器人模型**：画面右侧有一个复杂的机械结构，形似多自由度机械臂或移动机器人平台，带有多个关节和传感器部件。
   - **家具与设施**：左侧有一把金属框架椅子，下方有小型电子设备或控制台；地面中央有一个蓝色发光屏幕，可能是虚拟显示器或交互界面。
   - **环境细节**：地面为灰色网格状纹理，墙壁为浅色，整体呈现实验室或测试室风格。
   - **物体分布**：空间内布置了多种设备，包括货架、工具箱、传感器模块等，模拟真实工作环境。
   - **光照与阴影**：存在明显光照效果，投射出物体阴影，增强立体感。

4. **图像传达的主要信息**：  
   该图像展示了**高保真物理仿真环境**（HFPS）中的机器人系统部署场景，强调仿真器对复杂机器人、多传感器、真实环境交互的支持能力。结合上下文“GENERAL SIMULATOR”表格，此图用于说明某款仿真器具备**高质量图形渲染（HQGR）** 和**丰富机器人库（RRL）** 的特性，适用于机器人导航（NAV）、自动驾驶（AD）、强化学习（RL）等任务的开发与测试。

【图像 15】(538x295)
1. **图像类型**：  
   该图是一张**三维图形渲染截图**，属于机器人仿真环境的可视化展示，可能来自一个机器人模拟器（如Gazebo、PyBullet等）。

2. **图像中的所有文字内容**：  
   图像中**没有可见的文字内容**。所有文本信息均来自页面上下文，而非图像本身。

3. **图像中的关键视觉元素和数据信息**：  
   - **机器人机械臂**：画面中央是一个具有多个关节的蓝色与灰色相间的机械臂，末端夹持着一个棕色长方体物体，显示其正在执行抓取或搬运任务。
   - **工作台**：左侧有一个白色工作台，上面放置了多种工具，包括锤子、扳手、螺丝刀、香蕉等，表明这是一个用于操作或测试的场景。
   - **箱子与障碍物**：机械臂右侧有一个棕色立方体箱子，可能是目标对象或障碍物；旁边还有一个带开口的白色结构，可能代表容器或传送装置。
   - **地面与环境**：地面为棋盘格图案，常见于仿真环境中用于空间定位参考；整体环境简洁，具有典型的虚拟仿真风格。
   - **视角**：俯视角度，便于观察机械臂动作与周围环境布局。

4. **图像传达的主要信息**：  
   该图像展示了**机器人在仿真环境中执行抓取与操作任务的场景**，重点体现机械臂与物体之间的交互能力。结合上下文中的“RRL”（Rich Robot Library）、“MSS”（Multiple Sensor Simulation）和“ROS”集成等关键词，可推断此图为某机器人仿真平台的功能演示，强调其在复杂任务建模、物理交互和多传感器支持方面的应用能力。

【图像 16】(529x292)
1. **图像类型**：  
   该图是一张**三维图形渲染截图**，展示了一个机器人仿真环境中的多个机械臂模型，属于计算机生成的可视化图像，可能来自一个机器人仿真平台（如Gazebo、PyBullet等）。

2. **图像中的所有文字内容**：  
   图像左下角有一段代码或日志输出文本，内容如下：  
   ```
   [info] add: ur5e_0
   [info] add: ur5e_1
   [info] add: ur5e_2
   [info] add: ur5e_3
   [info] add: ur5e_4
   [info] add: ur5e_5
   [info] add: ur5e_6
   [info] add: ur5e_7
   [info] add: ur5e_8
   [info] add: ur5e_9
   [info] add: ur5e_10
   [info] add: ur5e_11
   [info] add: ur5e_12
   [info] add: ur5e_13
   [info] add: ur5e_14
   [info] add: ur5e_15
   [info] add: ur5e_16
   [info] add: ur5e_17
   [info] add: ur5e_18
   [info] add: ur5e_19
   [info] add: ur5e_20
   [info] add: ur5e_21
   [info] add: ur5e_22
   [info] add: ur5e_23
   [info] add: ur5e_24
   [info] add: ur5e_25
   [info] add: ur5e_26
   [info] add: ur5e_27
   [info] add: ur5e_28
   [info] add: ur5e_29
   [info] add: ur5e_30
   [info] add: ur5e_31
   [info] add: ur5e_32
   [info] add: ur5e_33
   [info] add: ur5e_34
   [info] add: ur5e_35
   [info] add: ur5e_36
   [info] add: ur5e_37
   [info] add: ur5e_38
   [info] add: ur5e_39
   [info] add: ur5e_40
   [info] add: ur5e_41
   [info] add: ur5e_42
   [info] add: ur5e_43
   [info] add: ur5e_44
   [info] add: ur5e_45
   [info] add: ur5e_46
   [info] add: ur5e_47
   [info] add: ur5e_48
   [info] add: ur5e_49
   [info] add: ur5e_50
   [info] add: ur5e_51
   [info] add: ur5e_52
   [info] add: ur5e_53
   [info] add: ur5e_54
   [info] add: ur5e_55
   [info] add: ur5e_56
   [info] add: ur5e_57
   [info] add: ur5e_58
   [info] add: ur5e_59
   [info] add: ur5e_60
   [info] add: ur5e_61
   [info] add: ur5e_62
   [info] add: ur5e_63
   [info] add: ur5e_64
   [info] add: ur5e_65
   [info] add: ur5e_66
   [info] add: ur5e_67
   [info] add: ur5e_68
   [info] add: ur5e_69
   [info] add: ur5e_70
   [info] add: ur5e_71
   [info] add: ur5e_72
   [info] add: ur5e_73
   [info] add: ur5e_74
   [info] add: ur5e_75
   [info] add: ur5e_76
   [info] add: ur5e_77
   [info] add: ur5e_78
   [info] add: ur5e_79
   [info] add: ur5e_80
   [info] add: ur5e_81
   [info] add: ur5e_82
   [info] add: ur5e_83
   [info] add: ur5e_84
   [info] add: ur5e_85
   [info] add: ur5e_86
   [info] add: ur5e_87
   [info] add: ur5e_88
   [info] add: ur5e_89
   [info] add: ur5e_90
   [info] add: ur5e_91
   [info] add: ur5e_92
   [info] add: ur5e_93
   [info] add: ur5e_94
   [info] add: ur5e_95
   [info] add: ur5e_96
   [info] add: ur5e_97
   [info] add: ur5e_98
   [info] add: ur5e_99
   [info] add: ur5e_100
   ```

3. **图像中的关键视觉元素和数据信息**：  
   - **多个UR5e机械臂**：图像中显示了大量相同的白色机械臂（UR5e），排列成网格状，分布在深蓝色棋盘格背景上。这些机械臂姿态各异，部分处于运动状态。
   - **三维空间布局**：机械臂以阵列形式分布，表明这是一个大规模并行仿真场景。
   - **颜色与光影**：机械臂为白色，关节处有黑色细节；地面为蓝黑相间的棋盘格，增强深度感。
   - **箭头指示**：部分机械臂末端有红色或绿色箭头，可能表示力方向、运动轨迹或传感器反馈。
   - **文本日志**：左下角的日志显示系统正在依次添加从`ur5e_0`到`ur5e_100`共101个机器人实例，表明这是在构建一个大型机器人系统仿真。

4. **图像传达的主要信息**：  
   该图像展示了**一个大规模并行机器人仿真实验环境**，其中包含超过100个UR5e机械臂，用于验证高并发、多机器人系统的仿真能力。结合上下文表格中提到的“LSPS: LARGE-SCALE PARALLEL SIMULATION”（大规模并行仿真）和“MR: MULTI-ROBOT SYSTEMS”（多机器人系统），可推断此图旨在说明该仿真器支持**大规模、多机器人协同仿真**，适用于强化学习、自动驾驶、机器人导航等复杂任务的研究与训练。

【图像 17】(514x289)
1. **图像类型**：  
   该图是一张**3D渲染的计算机图形截图**，展示了一个机器人仿真环境。图像具有高保真度的物理模拟和高质量图形渲染特征，属于典型的机器人仿真平台可视化场景。

2. **图像中的所有文字内容**：  
   - 图像左下角有“**AIDIA**”字样（倒置显示）。
   - “AIDIA”下方有一个绿色螺旋状标志（非文字）。
   - 其他部分无明显可读文字。

3. **图像中的关键视觉元素和数据信息**：  
   - 多个不同类型的机械臂（如黄色、绿色、白色等）在封闭空间内协同工作。
   - 顶部为木质结构平台，其上安装了传感器或小型设备。
   - 机械臂围绕中央区域操作，可能在执行装配、抓取等任务。
   - 环境为金属围栏包围的实验室或测试舱，体现工业或科研场景。
   - 图像中可见多个机器人关节、末端执行器（如夹爪），以及连接线缆与控制单元。
   - 颜色编码区分不同机器人系统（如黄、绿、白），有助于识别个体。
   - 背景中可见蓝色灯光，增强视觉层次感，暗示高科技环境。

4. **图像传达的主要信息**：  
   该图像展示了**一个复杂多机器人系统的高保真仿真环境**，强调了大规模并行计算、多机器人协作、高精度物理模拟和高质量图形渲染能力。结合上下文表格（TABLE II）中提到的“HFPS”、“HQGR”、“MR”（多机器人系统）、“LSPC”等术语，此图很可能用于说明某款机器人仿真器（如AIDIA）在支持多机器人协同、复杂任务调度及真实感渲染方面的功能优势。

【图像 18】(429x240)
根据您提供的信息，以下是对该图像的详细分析：

---

**1. 图像类型**  
该图像为 **空白图像（或缺失图像）**。从视觉上看，图像区域完全为空白（纯白色背景），未显示任何内容，可能为图像加载失败、缺失或被移除。

---

**2. 图像中的所有文字内容**  
图像中 **无可见文字内容**。图像区域为空，未包含任何文本、标签或注释。

---

**3. 图像中的关键视觉元素和数据信息**  
- **无视觉元素**：图像中没有图形、图表、照片、图标、线条、颜色或其他可识别的视觉组件。
- **无数据信息**：无法提取任何数据、数值、趋势、结构或关系。

---

**4. 图像传达的主要信息**  
由于图像内容缺失或为空白，**无法传达任何具体信息**。结合上下文（PDF第3页中的第18张图像，位于“TABLE II”之后），推测该图像原本应为与机器人仿真器对比相关的图表或表格，但当前未正确显示。

---

**补充说明**：  
根据页面上下文，“TABLE II”列出了多种机器人仿真器的功能特性（如HFPS、HQGR、ROS集成等），因此第18张图像可能本应是这些仿真的比较图、架构图或性能可视化图表。但由于图像缺失，其原意无法确认。

---

**结论**：  
当前图像为空白，**无有效内容可供分析**。建议检查原始PDF文件是否完整，或确认图像是否在其他位置重复出现。

【图像 19】(429x240)
1. **图像类型**：  
   该图是一张**三维图形渲染截图**，属于机器人仿真环境的可视化界面，可能来自一个机器人模拟器（如Gazebo、Webots等）。

2. **图像中的所有文字内容**：  
   图像中**无可见文字内容**。所有文字信息均来自页面上下文（如表格II），但未直接出现在该图像中。

3. **图像中的关键视觉元素和数据信息**：  
   - **机械臂结构**：画面中央为一个蓝橙相间的机械臂，具有多个关节，末端装有夹持器或工具。
   - **工具操作**：机械臂正抓取一个带有蓝色和绿色部件的小型物体，可能是某种传感器或工具模块。
   - **背景环境**：地面为蓝白棋盘格图案，常见于虚拟仿真场景中用于空间定位参考。
   - **右侧设备**：右后方有一个黄色平台，上面排列着多根红色和蓝色的杆状物，可能代表传感器阵列或测试装置。
   - **上方结构**：左上角有一黑色矩形结构，类似机器柜或控制台，内部可见部分组件。
   - **视角**：采用第一人称或俯视视角，突出机械臂的操作过程。

4. **图像传达的主要信息**：  
   该图像展示了一个**高保真度机器人仿真环境中的机械臂操作场景**，强调了机器人在复杂环境中执行精细任务的能力。结合上下文（如“HFPS: HIGH-FIDELITY PHYSICAL SIMULATION”、“HQGR: HIGH-QUALITY GRAPHICS RENDERING”），此图旨在体现仿真器具备高质量图形渲染与物理模拟能力，适用于机器人导航、自动化操作等研究应用。


================================================================================
第 4 页
================================================================================

【文本内容】
IEEE/ASME TRANSACTIONS ON MECHATRONICS
4
AI2-THOR
Virtualhome
Matterport 3D
Habitat
iGibson
TDW
SAPIEN
Infinite-World
Fig. 4. Examples of Real-Scene Based Simulators.
real world, which places high demands on the complexity
and realism of the simulators. These simulators mostly collect
data from the real world, create photorealistic 3D assets, and
build scenes using 3D game engines like UE5 and Unity. The
rich and realistic scenes make simulators based on real world
environments the top choice in household activities.
SAPIEN [48] stands out for its design, specifically tailored
to simulate interactions with joint objects like doors, cabinets,
and drawers. VirtualHome [49] is notable for its unique envi-
ronment graph, which facilitates high-level embodied planning
based on natural language descriptions of environments. While
AI2-ThOR [50] offers a wealth of interactive scenes, these
interactions, similar to those in VirtualHome, are script-based
and lack real physical interactions. This design suffices for em-
bodied tasks not requiring fine-grained interactions. Both iGib-
son [51] and TDW [52] provide fine-grained embodied control
and highly simulated physical interactions. iGibson excels in
offering abundant and realistic large-scale scenes, making it
suitable for complex and long-term mobile operations, whereas
TDW allows greater user freedom in scene expansion and
features unique audio and flexible fluid simulations, making it
indispensable for related simulation scenarios. Matterport3D
[53], a foundational 2D-3D visual dataset, is widely used
and extended in embodied AI benchmarks. Although the
embodied agent in Habitat lacks interaction capabilities, its
extensive indoor scenes, user-friendly interfaces, and open
framework make it highly regarded in embodied navigation.
InfiniteWorld [54] focuses on unified and scalable simulation
framework and implemented various improvements and the
latest implicit asset reconstruction, as well as natural language-
driven scene generation and editing. It provides strong support
for complex robotic interactions through distributed collabo-
ration, AI assistance, and Human-in-the-Loop.
Besides, automated simulation scene construction is greatly
beneficial for obtaining high-quality embodied data. RoboGen
[55] customizes tasks from randomly sampled 3D assets
through LLMs, thereby creating scenes and automatically
training agents; HOLODECK [56] can automatically cus-
tomize corresponding high-quality simulation scenes in AI2-
THOR based on human instructions; PhyScene [57] gen-
erates interactive and physically consistent high-quality 3D
scenes based on conditional diffusion. The Allen Institute
for Artificial Intelligence expanded AI2-THOR and proposed
ProcTHOR [58], which can automatically generate simulated
scenes with sufficient interactivity, diversity, and rationality.
IV. EMBODIED PERCEPTION
The “north stars” of the future of visual perception is
embodied-centric visual reasoning and social intelligence [59].
3D Scene Undestanding
Visual SLAM
Improving the Accuracy of 
Localization and Mapping
Improving Semantic 
Analysis Capabilities
Passive Visual Perception
Activate Exploration
Active Visual Perception
Improving Observational Abilities
Obersvation
Action
Improving Environmental State Certainty
Fig. 5. The schematic diagram of active visual perception. Visual SLAM and
3D Scene Understanding provide the foundation for passive visual perception,
while active exploration provides activeness to the passive perception system.
These elements works collaboratively for the active visual perception system.
Unlike merely recognizing objects in images, agent with em-
bodied perception must move in the physical world and inter-
act with the environment. This requires a deeper understanding
of 3D space and dynamic environments. Embodied perception
requires visual perception and reasoning, understanding the
3D relations within a scene, and predicting and performing
complex tasks based on visual information.
A. Active Visual Perception
Active visual perception systems require fundamental ca-
pabilities such as state estimation, scene perception, and
environment exploration. As shown in Fig. 5, these capabilities
have been extensively studied within the domains of Visual
Simultaneous Localization and Mapping (vSLAM) [94], [95],
3D Scene Understanding [96], and Active Exploration [11].
These research areas contribute to developing robust active
visual perception systems, facilitating improved environmental
interaction and navigation in complex, dynamic settings. We
briefly introduce these three components and summarize the
methods mentioned in each part in Table III.
1) Visual Simultaneous Localization and Mapping: Simul-
taneous Localization and Mapping (SLAM) aims to determine
a robot’s position within an unknown environment while
simultaneously constructing a map of the environment [97].
Range-based SLAM [98], [99] relies on rangefinders, such
as laser scanners, radar, and sonar, to generate point cloud
representations. However, this approach is costly and provides
limited environmental information. In contrast, Visual SLAM
(vSLAM) [94], [95] employs on-board cameras to capture
image frames and build environmental representations. Its ad-
vantages include low hardware costs, high accuracy in small-
scale scenarios, and the ability to capture rich environmental
details. Classical vSLAM can be broadly categorized into
Traditional vSLAM and Semantic vSLAM [95]. Traditional
vSLAM uses image data and multi-view geometry to estimate
a robot’s pose and construct low-level maps (e.g., sparse, semi-
dense, or dense point clouds) through methods like filter-based
approaches (e.g., MonoSLAM [60]), keyframe-based methods
(e.g., ORB-SLAM [61]), and direct tracking techniques (e.g.,
LSD-SLAM [62]). However, low-level maps do not directly
correspond to objects, making them challenging for robots to
interpret and utilize. Semantic vSLAM addresses this limita-
tion by integrating semantic information, enhancing robots’
ability to perceive and navigate unexplored environments.


【图像 1】(672x630)
1. **图像类型**：  
   这是一张**3D场景的多视角拼接图**（或称“全景拼接图”），属于**虚拟现实/仿真环境的可视化图形**，展示了一个基于真实世界扫描构建的室内三维空间模型。

2. **图像中的所有文字内容**：  
   图像中**没有可见的文字内容**。所有文本信息来自页面上下文，而非图像本身。

3. **图像中的关键视觉元素和数据信息**：  
   - 图像由多个**矩形平面**组成，每个平面代表一个房间或区域的**全景视图**（如天花板、墙壁、地面等），这些平面以非连续方式排列，形成一个立体结构。
   - 每个平面展示了不同房间的内部细节，包括：
     - 厨房（有灶台、抽油烟机、橱柜）
     - 卫生间（有洗手池、马桶、淋浴区）
     - 客厅/卧室（有家具、窗户、装饰画）
     - 门廊与走廊
   - 视觉风格为**高保真度渲染**，具有真实光照、纹理和物体布局，表现出**高度还原的真实感**。
   - 整体构图类似于**3D建模软件中的“展开视图”**，用于展示整个建筑空间的各个部分。

4. **图像传达的主要信息**：  
   该图像展示了**基于真实场景构建的虚拟仿真环境**（Real-Scene Based Simulators）的典型示例，强调其通过采集真实世界数据并重建为高保真3D模型的能力。这种技术常用于机器人导航、AI训练、虚拟现实等领域，体现了当前仿真平台（如Matterport 3D、Habitat等）在**环境真实性与复杂性方面的高水平**。

【图像 2】(640x360)
1. **图像类型**：  
   该图是一张**3D渲染截图**，展示了一个基于真实场景的虚拟室内环境（厨房与餐厅区域），属于计算机仿真或虚拟现实模拟器中的可视化示例。

2. **图像中的所有文字内容**：  
   图像中**没有可见的文字内容**。图像本身为纯视觉渲染，未包含任何文本标签、说明或字幕。

3. **图像中的关键视觉元素和数据信息**：  
   - **空间布局**：展示了一个小型厨房与用餐区结合的室内场景，包括厨房操作台、橱柜、烤箱、微波炉等设施。
   - **家具与装饰**：红色餐桌搭配白色餐椅，桌上摆放有餐具、饮料瓶和食物模型；墙上挂有多幅艺术画作，颜色鲜明。
   - **照明与材质**：采用逼真的光照效果，墙面、地板和家具表面具有清晰的纹理和阴影，体现高保真度建模。
   - **视角**：从斜上方俯视角度拍摄，突出空间结构与物体布置。
   - **风格特征**：整体设计现代且色彩对比强烈（红白为主），符合典型虚拟仿真环境中对真实感与可识别性的追求。

4. **图像传达的主要信息**：  
   该图像用于展示**基于真实世界场景构建的高保真虚拟仿真环境**，强调其在细节、光照、材质和空间布局上的真实性。它代表了如AI2-THOR、Matterport 3D、Habitat等真实场景驱动型仿真器的能力，旨在支持机器人导航、人机交互等研究任务，体现“真实世界数据驱动”的仿真技术优势。

【图像 3】(818x461)
1. **图像类型**：  
   该图是一张**3D渲染截图**，属于虚拟仿真环境中的场景展示，用于演示机器人在真实场景基础上构建的模拟器中的操作行为。

2. **图像中的所有文字内容**：  
   图像中未包含明显的文本内容。但在右侧抽屉边缘隐约可见极小的文字“openai”，可能是模型或数据来源标识，但非主要信息。

3. **图像中的关键视觉元素和数据信息**：  
   - 一个白色的机械臂（机器人手臂），具有多关节结构，外观类似现实中的工业或服务机器人（如UR系列）。  
   - 机械臂正在与一个木制柜子互动，其末端执行器正接触并打开柜子的一个抽屉。  
   - 柜子为深棕色木质纹理，有两个抽屉，每个抽屉带有金属把手。  
   - 抽屉已部分打开，内部为浅色材质，显示了逼真的物理交互效果。  
   - 整体光照柔和，阴影清晰，表明使用了高质量的3D渲染引擎（如Unity或Unreal Engine）进行建模。

4. **图像传达的主要信息**：  
   该图像展示了基于真实场景构建的机器人仿真环境（Real-Scene Based Simulator）的能力，强调其在**高保真度、物理真实感和复杂交互**方面的表现。结合上下文，此图旨在说明这类模拟器（如AI2-THOR、Habitat等）如何通过采集真实世界数据，利用3D游戏引擎创建接近现实的环境，以支持机器人任务训练与测试，例如物体抓取、开门等日常操作。

【图像 4】(708x450)
1. **图像类型**：  
   该图是一张**3D渲染截图**，属于虚拟场景模拟器生成的视觉化图像，用于展示基于真实场景的仿真环境。

2. **图像中的所有文字内容**：  
   图像中**无可见文字内容**。图像本身为纯视觉渲染，未包含任何文本标注或标签。

3. **图像中的关键视觉元素和数据信息**：  
   - **室内环境**：呈现一个现代风格的室内空间，具有斜顶木质天花板、浅色墙壁和灰色窗帘。  
   - **家具与陈设**：  
     - 墙上安装有大型白色玻璃门储物柜，内部摆放书籍、盒子等物品，体现生活化细节。  
     - 柜子上方悬挂黑色金属支架，下方挂有吊灯和装饰性灯具。  
     - 左侧墙角有一台电视机，旁边放置几何造型的装饰物。  
     - 天花板上倒挂着一把椅子和一盏吊灯，暗示可能为倒置视角或特殊构图。  
     - 右侧墙上挂有一把吉他，增强生活气息。  
   - **光影效果**：阳光透过窗户在天花板和墙壁上投下清晰的阴影，表现出高精度光照模拟。  
   - **材质与纹理**：木材、玻璃、金属、布料等材质均表现出较高的真实感，符合“photorealistic”（照片级真实）标准。

4. **图像传达的主要信息**：  
   该图展示了**基于真实世界场景构建的高保真虚拟仿真环境**，强调其在细节、光照、材质和布局上的真实性。结合上下文，此图像用于说明如AI2-THOR、Matterport 3D等仿真平台如何利用真实世界数据创建逼真的3D场景，以支持机器人、AI等领域的训练与测试。图像突出了此类仿真器在复杂性和真实感方面的优势。

【图像 5】(646x432)
1. **图像类型**：  
   该图是一张**三维计算机生成的虚拟场景截图**，属于基于真实场景的仿真环境（Real-Scene Based Simulator）的可视化展示。图像风格为3D渲染，具有游戏引擎（如Unity或UE5）典型的视觉特征。

2. **图像中的所有文字内容**：  
   图像中**没有可见的文字内容**。所有文本信息来自页面上下文（如图注“Fig. 4. Examples of Real-Scene Based Simulators.”），但图像本身不含文字。

3. **图像中的关键视觉元素和数据信息**：  
   - **空间布局**：呈现一个室内房间的三维视图，视角略偏上方，可能模拟机器人或虚拟代理的观察角度。  
   - **家具与物体**：包括桌子、椅子、柜子、窗户、灯具、电视等常见家居物品，部分物体以倒置或悬浮方式显示，暗示正在进行物理或空间建模测试。  
   - **坐标轴标记**：画面中央有蓝色、绿色、红色的坐标轴线（X/Y/Z轴），表明这是一个用于仿真的3D环境，可能用于机器人导航或交互任务。  
   - **光照与阴影**：光线从窗户进入，地面有圆形光斑，模拟自然光照效果，增强真实感。  
   - **地板与天花板**：地板为木质拼花图案，天花板上有网格线，可能是用于定位或测量的参考网格。  
   - **整体风格**：高度逼真，细节丰富，符合“photorealistic 3D assets”描述。

4. **图像传达的主要信息**：  
   该图像展示了**基于真实世界场景构建的高保真度仿真环境**，用于机器人或AI系统在复杂室内环境中的训练与测试。其核心特点是**高度还原现实世界的细节与物理特性**，支持多任务交互（如导航、抓取、感知）。这类仿真器常用于自动驾驶、家庭服务机器人等领域，强调真实感与复杂性。  

> 注：结合上下文，此图是“Real-Scene Based Simulators”类别下的示例之一，旨在说明此类仿真器如何利用真实数据构建逼真3D场景。

【图像 6】(639x376)
1. **图像类型**：  
   该图是一张**计算机生成的3D仿真场景截图**，属于虚拟环境模拟器的可视化展示，具体为基于真实场景构建的室内三维环境。

2. **图像中的所有文字内容**：  
   图像中**无直接可见的文字内容**。相关文字信息来自页面上下文（如“Fig. 4. Examples of Real-Scene Based Simulators.”），但图内本身未包含文本。

3. **图像中的关键视觉元素和数据信息**：  
   - **多视角布局**：图像由三个部分组成：
     - 左上角：俯视视角显示一个房间，一个人形角色平躺在地板上，旁边有一架钢琴、椅子和桌子。
     - 右上角：另一个俯视视角，显示一个蓝色机器人或虚拟人物模型平躺在地板上，周围有家具（如沙发）。
     - 下方中央：一个二维平面图（floor plan），展示了整个房屋的布局，包括厨房、客厅、卧室等区域，标注了家具位置。
   - **环境细节**：房间具有真实的家居布置（如沙发、餐桌、床、书桌），地面材质为浅色瓷砖或木地板，整体风格偏向现代住宅。
   - **人物与模型**：左上角为人类形态的角色，右上角为蓝色机器人形态角色，表明该仿真支持多种代理（agent）类型。
   - **光照与纹理**：具有较高的视觉真实感，光影处理细致，符合使用3D游戏引擎（如Unity或UE5）渲染的特点。

4. **图像传达的主要信息**：  
   该图像展示了**基于真实世界场景构建的虚拟仿真环境**，强调其高保真度和复杂性。通过多个视角（俯视图和平面图）呈现了逼真的室内空间结构与交互对象，用于支持机器人、AI代理在真实环境中进行行为训练与测试。这体现了“Real-Scene Based Simulators”（基于真实场景的模拟器）的核心优势：高度还原现实世界的物理与视觉特征，适用于智能体训练、导航、人机交互等任务。

【图像 7】(661x372)
1. **图像类型**：  
   这是一张**三维仿真场景的截图**，属于虚拟现实或机器人仿真环境中的渲染图像，可能来自基于真实场景构建的仿真平台（如AI2-THOR、Habitat等）。

2. **图像中的所有文字内容**：  
   图像中**没有明显可读的文字内容**。虽然背景白板上有涂鸦和草图，但无法辨识具体文字或符号。

3. **图像中的关键视觉元素和数据信息**：  
   - 一个白色的**机械臂**位于画面中央，正在操作桌面上的物体，其姿态表明正在进行抓取或放置动作。
   - 桌面为木质，上面摆放了多种日常物品：苹果、杯子、纸巾盒、盘子、瓶子等，模拟真实生活场景。
   - 背景包括一张黑色皮质沙发、一把椅子、地面散落的衣物和杂物，增强了场景的真实感。
   - 墙上有一块白板，上面画有手绘草图，可能是技术设计或实验记录。
   - 地面为浅色瓷砖，右下角有一个蓝色边框的贴纸或标签，具体内容不可读。
   - 整体光照自然，阴影清晰，具有较高的**视觉真实度**，符合“photorealistic”（照片级真实感）的描述。

4. **图像传达的主要信息**：  
   该图像展示了一个**基于真实世界场景构建的机器人仿真环境**，用于测试机器人在复杂、杂乱家庭环境中的感知与操作能力。它强调了仿真器对真实世界细节的还原能力，包括物体布局、光照、材质和交互行为，体现了当前先进仿真平台（如AI2-THOR、Habitat等）在提升机器人训练真实性的努力。

【图像 8】(615x462)
1. **图像类型**：  
   该图是一张**3D计算机生成的模拟场景截图**，属于虚拟现实或机器人仿真环境中的渲染图像，用于展示基于真实场景的机器人仿真平台。

2. **图像中的所有文字内容**：  
   图像中**无可见文字内容**。图像本身为纯视觉渲染，未包含文本标签、说明或标注。

3. **图像中的关键视觉元素和数据信息**：  
   - **机器人机械臂**：画面中央是一个白色带蓝色条纹的多关节机械臂，末端装有摄像头或传感器，正从下方伸向桌面。
   - **桌面与物品**：上方为木质桌面，上面放置了以下物品：
     - 一个透明碗
     - 一个托盘，内含一个绿色苹果和一个红色苹果
     - 一个遥控器（灰色，带按钮）
     - 一张蓝色卡片（可能为门禁卡或标签）
   - **室内环境**：背景为客厅场景，包括：
     - 一张蓝色沙发，带有紫色靠垫
     - 墙上挂有三幅画作（抽象图案）
     - 地面为浅色地板
   - **视角**：图像采用**俯视角度**，强调机械臂与桌面物体之间的交互关系。
   - **光照与纹理**：具有较高的光影真实感和材质细节（如木纹、布料、塑料），体现高保真度3D建模。

4. **图像传达的主要信息**：  
   该图像展示了**基于真实世界场景构建的机器人仿真环境**，强调其在**视觉真实性和物理交互性**方面的特点。结合上下文“Real-Scene Based Simulators”，此图旨在说明此类仿真平台如何通过采集真实世界数据并使用3D游戏引擎（如UE5、Unity）重建复杂、逼真的家居环境，以支持机器人任务训练（如抓取、导航等）。该场景突出了仿真系统的**高保真度、复杂布局和真实物体交互能力**，是现代机器人研究中“真实场景驱动仿真”的典型代表。


================================================================================
第 5 页
================================================================================

【文本内容】
IEEE/ASME TRANSACTIONS ON MECHATRONICS
5
TABLE III
THE COMPARISON OF THE ACTIVE VISUAL PERCEPTION METHODS.
Function
Type
Methods
vSLAM
Traditional vSLAM
MonoSLAM [60], ORB-SLAM [61], LSD-SLAM [62]
Semantic vSLAM
SLAM++ [63], QuadricSLAM [64], So-SLAM [65],
SG-SLAM [66], OVD-SLAM [67], GS-SLAM [68]
3D Scene Understanding
Projection-based
MV3D [69], PointPillars [70], MVCNN [71]
Voxel-based
VoxNet [72], SSCNet [73]), MinkowskiNet [74], SSCNs [75], Embodiedscan [76]
Point-based
PointNet [77], PointNet++ [78], PointMLP [79]), PointTransformer [80], Swin3d [81], PT2 [82],
3D-VisTA [83], LEO [84], PQ3D [85], PointMamba [86], Mamba3D [87]
Active Exploration
Interacting with the environment
Pinto et al. [88], Tatiya et al. [89]
Changing the viewing direction
Jayaraman et al. [90], NeU-NBV [91], Hu et al. [92], Fan et al. [93]
2) 3D Scene Understanding: 3D scene understanding [100]
aims to distinguish objects’ semantics, identify their locations,
and infer the geometric attributes from 3D scene data [101],
which is fundamental in autonomous driving [102], robot
navigation [103], and human-computer interaction [104] etc.
A scene may be recorded as 3D point clouds using 3D
scanning tools like LiDAR or RGB-D sensors. Unlike images,
point clouds are sparse, disordered, and irregular. Recent
advances in deep learning for 3D scene understanding can
be categorized into projection-based, voxel-based, and point-
based methods. Concretely, projection-based methods (e.g.,
MV3D [69], PointPillars [70], MVCNN [71] ) project 3D
points onto various image planes and employ 2D CNN-
based backbones for feature extraction. Voxel-based methods
convert point clouds into regular voxel grids to facilitate 3D
convolution operations (e.g., VoxNet [72], SSCNet [73]), and
some works improve their efficiency through sparse convo-
lution (e.g., MinkowskiNet [74], SSCNs [75], Embodiedscan
[76]). In contrast, point-based methods process point clouds
directly (e.g., PointNet [77], PointNet++ [78], PointMLP
[79]). Recently, to achieve model scalability, Transformers-
based (e.g., PointTransformer [80], Swin3d [81], PT2 [82],
3D-VisTA [83], LEO [84], PQ3D [85] ) and Mamba-based
(e.g., PointMamba [86], Mamba3D [87]) architectures have
emerged. Notably, PQ3D [85] enhances scene understanding
by seamlessly integrating features from point clouds, multi-
view images, and voxels.
3) Active Exploration: The 3D scene understanding meth-
ods allow robots to passively perceive the environment, with
static information acquisition and decision-making regardless
of scene changes. Thus, while passive perception is essential, it
must be complemented by active exploration, enabling robots
to dynamically interact with and perceive their surroundings.
The relationship between them is shown in Fig. 5. Current
methods addressing active perception focus on interacting
with the environment [88], [89] or by changing the viewing
direction to obtain more visual information [90]–[93].
For example, Pinto et al. [88] proposed a curious robot that
learns visual representations through physical interaction with
the environment rather than relying solely on dataset category
labels. To address the challenge of interactive object perception
across robots with varying morphologies, Tatiya et al. [89]
proposed a multi-stage projection framework that transfers
implicit knowledge through learned exploratory interactions.
Recognizing the challenge of autonomously capturing infor-
mative observations, Jayaraman et al. [90] proposed a rein-
forcement learning method where an agent learns to actively
acquire informative visual observations by reducing its uncer-
tainty about unobserved parts of its environment. NeU-NBV
step-by-step instruction
Walk along the hallway past the 
shoe cabinet, turn left at the 
corner, and then turn left again 
when passing the decorative 
painting to enter the bedroom. 
Keep walking straight until you 
reach the vacuum cleaner.
Bed room
Bath room
Hallway
Living room
Dining room
Kitchen
Study room
Bed room
Bath room
Hallway
Living room
Dining room
Kitchen
Study room
Bed room
Bath room
Hallway
Living room
Dining room
Kitchen
Study room
described goal navigation
navigation with interaction
Find the pink laptop on the 
desk in the study room at the 
right side of the end of the 
hallway.
Go to the kitchen and take an 
apple from the refrigerator, 
wash it in the sink, and put it on 
the plate on the table of dining 
room.
(a)
(b)
Natural Language
Agent
Oracle
 Interactive
Noninteractive
Planning
 
Dialog
Environment
Fig. 6.
(a) Overview of VLN. The embodied agent communicates with
humans through natural language. Humans issue instructions to the embodied
agent, who completes tasks such as planning and dialog. Subsequently,
through collaborative cooperation or the embodied agent’s independent ac-
tions, actions are made in interactive or non-interactive environments based
on visual observations and instructions, (b) Different tasks of VLN.
[91] introduced a mapless planning framework that iteratively
positions an RGB camera to capture the most informative
images of an unknown scene. Hu et al. [92] developed a
robot exploration algorithm that predicts the value of future
states using a state value function. To address the issue of
accidental input in open-world environments, Fan et al. [93]
treated active recognition as a sequential evidence-gathering
process, providing step-by-step uncertainty quantification and
reliable prediction under evidence combination theory.
B. Visual Language Navigation
Visual Language Navigation (VLN) is an essential task,
aiming at navigating in unseen environments following linguis-
tic instructions. VLN requires robots to understand complex
and diverse visual observations and meanwhile interpret in-
structions at different granularities. The input typically consists
of two parts: visual information and natural language instruc-
tions. The visual information can either be a video of past
trajectories or a set of historical-current observation images.
The natural language instructions include the target that the
agent needs to reach or the task that the agent is expected to
complete. The agent must use the above information to select
one or a series of actions from a list of candidates to fulfill the
requirements of the natural language instructions. This process
could be represented as Action = M(O, H, I), where Action
is the chosen action or a list of action candidates, O is the


【图像 1】(199x275)
1. **图像类型**：  
   该图像为一个**图标式图形**（iconographic illustration），属于简化风格的机器人示意图，非真实照片、数据图表或截图。

2. **图像中的所有文字内容**：  
   图像中**无文字内容**。

3. **图像中的关键视觉元素和数据信息**：  
   - 图像描绘了一个**机器人头部的正面视图**，具有以下特征：
     - 顶部有多个矩形模块，可能代表传感器阵列或摄像头。
     - 中央有一个较大的矩形区域，内部包含一个斜向条纹图案和两个小圆点，可能象征显示屏或视觉感知系统。
     - 两侧各有一个“C”形结构，类似机械臂或夹爪。
     - 下部是一个较小的矩形结构，带有两个圆形孔洞，类似眼睛或传感器，下方有两个短竖线，可能表示支撑结构或连接部件。
   - 整体采用扁平化设计，颜色以深灰和浅绿为主，突出科技感和功能性。

4. **图像传达的主要信息**：  
   该图像用于**视觉上象征机器人或智能感知系统**，与上下文中的“主动视觉感知方法”（active visual perception methods）相呼应。它可能代表具备视觉SLAM、语义理解或3D场景分析能力的机器人平台，强调其在环境感知和自主导航方面的功能。尽管无具体数据，但作为插图，它辅助说明论文中讨论的技术应用场景。

【图像 2】(120x140)
根据您提供的信息，当前图像内容为一个表情符号（emoji），具体是一个卡通风格的猴子头像，与PDF文档中“TABLE III”的上下文无关。以下是对该图像的详细分析：

---

### 1. 图像类型  
**表情符号（Emoji）**  
- 类型：图形化图标，属于数字表情符号类别  
- 风格：扁平化、卡通化设计  
- 来源：常见于社交媒体或聊天应用中的表情符号库  

---

### 2. 图像中的所有文字内容  
**无文字内容**  
- 图像中不包含任何可读文本或字符  

---

### 3. 图像中的关键视觉元素和数据信息  
- **主体形象**：一只猴子的头部，具有明显的卡通特征  
- **面部特征**：  
  - 圆形脸庞，肤色较浅  
  - 黑色眼睛，瞳孔为白色小点  
  - 简单的微笑嘴巴  
  - 褐色毛发，覆盖头顶和脸颊两侧  
  - 顶部有类似帽子或头饰的橙色部分（可能象征王冠或装饰）  
- **色彩**：主要使用棕色、橙色、肤色和黑色，对比鲜明  
- **对称性**：左右对称，符合典型表情符号的设计规范  

---

### 4. 图像传达的主要信息  
- **情感表达**：通常用于表示幽默、调皮、好奇或轻松的情绪  
- **文化含义**：猴子在不同文化中有多种象征意义（如机智、顽皮），但在现代网络语境中多用于娱乐或调侃  
- **功能用途**：作为非语言沟通工具，在聊天、评论等场景中辅助表达情绪  

---

### 补充说明  
该图像**并非来自PDF第5页的第2张图像**，而是用户上传的一个独立表情符号。实际PDF中的图像应为表格（TABLE III）或相关图表，而此图与原文本内容无直接关联。

如果您希望分析PDF中的真实图像，请提供正确的图像文件或描述。

【图像 3】(199x275)
1. **图像类型**：  
   该图是一幅**图标式图形（iconic illustration）**，属于简化风格的机器人示意图，用于视觉表达而非数据展示。

2. **图像中的所有文字内容**：  
   图像中**无文字内容**。

3. **图像中的关键视觉元素和数据信息**：  
   - 图像描绘了一个**机器人外形的简化图标**，整体呈对称结构。  
   - 上部为机器人头部区域，包含两个类似“C”形的机械臂或传感器结构，中间有一个矩形屏幕，屏幕上显示一个斜向的绿色条状图案（可能表示摄像头或感知模块）。  
   - 中部为身体部分，由多个矩形块组成，呈现模块化设计。  
   - 下部为底盘或基座，带有两个圆形轮子和两侧的突出结构，暗示移动能力。  
   - 整体颜色以深灰为主，辅以浅绿点缀，风格简洁、现代，具有科技感。

4. **图像传达的主要信息**：  
   该图标代表一个**具备视觉感知与自主移动能力的智能机器人**，结合上下文（如vSLAM、语义SLAM、3D场景理解等），推测其象征**主动视觉感知系统在机器人中的应用**，强调机器人通过视觉进行环境建模与理解的能力。该图可能用于说明机器人在智能感知系统中的角色或功能。

【图像 4】(211x247)
根据您提供的信息，这张图像（PDF第5页中的第4张图像）实际上**并非来自您描述的学术论文上下文**，而是显示了一个**卡通风格的人物头像插图**，与表格内容无直接关联。

以下是基于该图像的详细分析：

---

### 1. 图像类型  
**图形 / 卡通插图**  
这是一幅简洁的矢量风格卡通人物头像，具有扁平化设计特征，可能用于装饰或作为标识。

---

### 2. 图像中的所有文字内容  
**无文字内容**  
图像中不包含任何可读的文字、标签或符号。

---

### 3. 图像中的关键视觉元素和数据信息  
- **人物面部特征**：  
  - 圆形脸型，肤色为浅色。  
  - 两个黑色圆形眼睛，对称分布。  
  - 微笑的弧形嘴巴，呈上扬状态。  
  - 两侧有圆形腮红，颜色为浅橙色。  
  - 耳朵轮廓清晰可见。  

- **头发与胡须**：  
  - 深棕色长发，从头顶垂下并覆盖耳朵两侧。  
  - 下巴处有浓密的深棕色络腮胡，呈倒三角形状。  

- **帽子**：  
  - 头顶佩戴一顶橙色帽子，仅显示帽檐部分，形状为半圆形。  

- **整体风格**：  
  - 极简主义、扁平化设计，使用纯色块构成，无阴影或渐变。  
  - 风格类似网络表情包或品牌吉祥物。

---

### 4. 图像传达的主要信息  
该图像**不传达技术或学术信息**，其主要功能可能是：  
- 作为文档中的装饰性元素或图标。  
- 表示某个人物角色（如作者、虚拟助手或品牌代表）。  
- 用于增强页面视觉吸引力，但与“vSLAM”、“3D场景理解”等技术主题无关。

---

### 总结  
此图像为**非技术性卡通插图**，不含文字或数据，与上下文中的学术表格内容无直接联系。它可能属于文档排版中的装饰元素，而非研究图表或实验结果展示。

【图像 5】(497x374)
1. **图像类型**：  
   该图像为一个**图形符号**，具体是一个**白色十字形图案**，背景为黑色。属于**简单几何图形**，非图表、照片或截图。

2. **图像中的所有文字内容**：  
   图像中**无任何文字内容**。

3. **图像中的关键视觉元素和数据信息**：  
   - 图像由一个**白色十字形**构成，形状类似于国际通用的“加号”（+）或医疗急救标志。
   - 十字的四个臂长度相等，宽度一致，整体对称。
   - 背景为纯黑色，与白色十字形成高对比度。
   - 无其他图形、标注或数据点。

4. **图像传达的主要信息**：  
   该图像可能作为**占位符、分隔符或装饰性符号**使用，用于文档排版或视觉强调。结合上下文（学术论文中的表格后），它可能表示**章节分隔、页面标记或出版物中的设计元素**，但**不承载具体技术或数据信息**。其主要功能是**视觉提示或格式化用途**，而非信息传递。

【图像 6】(497x374)
1. **图像类型**：  
   该图是一张**三维场景的俯视示意图**，属于**计算机生成的图形（3D渲染图）**，用于展示室内环境布局。图像由多个视角拼接而成，呈现不同房间的俯视结构。

2. **图像中的所有文字内容**：  
   图像中**无可见文字内容**。所有标注和说明均来自上下文表格或页面文本。

3. **图像中的关键视觉元素和数据信息**：  
   - 图像由**三个独立的俯视图**组成，分别展示了不同的室内空间：
     - **左图**：一个小型房间，包含桌子、椅子、冰箱等家具，地面为浅色，可能为厨房或客厅。
     - **中图**：一个较大的房间，包含床（蓝色床垫）、衣柜、书桌、地毯（绿色椭圆），地面为深色瓷砖，可能是卧室。
     - **右图**：一个带有红色L型沙发、电视柜、门和窗户的房间，墙壁为黄色与黑色搭配，可能是客厅或起居室。
   - 所有房间均以**俯视角度**绘制，清晰显示了家具布置、地板材质、墙壁颜色等细节。
   - 图像风格为**简化3D建模渲染**，用于可视化空间结构，常见于机器人导航、SLAM（同步定位与地图构建）或语义理解研究中。

4. **图像传达的主要信息**：  
   该图像用于**展示复杂室内环境的布局结构**，旨在辅助说明主动视觉感知方法（如vSLAM、3D场景理解）在真实或模拟环境中的应用。结合上下文（如TABLE III中提到的SLAM++、MV3D、PointNet等方法），此图很可能用于**演示这些算法在多房间、多物体场景下的感知能力**，强调对空间结构、语义对象（如床、沙发、桌子）的识别与建模。


================================================================================
第 6 页
================================================================================

【文本内容】
IEEE/ASME TRANSACTIONS ON MECHATRONICS
6
TABLE IV
COMPARISON OF DIFFERENT VLN DATASETS. M3D: MATTERPORT3D,
AT: AI2-THOR, OG: OMNIGIBSON, I: INDOOR, D: DISCRETE, O:
OUTDOOR, C: CONTINUOUS, SBS: STEP-BY-STEP INSTRUCTIONS, DGN:
DESCRIBED GOAL NAVIGATION, DDN: DEMAND-DRIVEN NAVIGATION,
NWI: NAVIGATION WITH INTERACTION, LSNWI: LONG-SPAN
NAVIGATION WITH INTERACTION, D&O: DIALOG AND ORACLE
Dataset
Year
Simulator
Environment
Feature
Size
R2R [105]
2018
M3D
I, D
SbS
21,567
R4R [106]
2019
M3D
I, D
SbS
200,000+
VLN-CE [107]
2020
Habitat
I, C
SbS
-
TOUCHDOWN [108]
2019
-
O, D
SbS
9,326
REVERIE [109]
2020
M3D
I, D
DGN
21,702
SOON [110]
2021
M3D
I, D
DGN
3,848
DDN [111]
2023
AT
I, C
DDN
30,000+
ALFRED [112]
2020
AT
I, C
NwI
25,743
OVMM [113]
2023
Habitat
I, C
NwI
7,892
BEHAVIOR-1K [114]
2023
OG
I, C
LSNwI
1,000
CVDN [115]
2020
M3D
I, D
D&O
2,050
DialFRED [116]
2022
AT
I, C
D&O
53,000
current observation, H is the historical information, and I is
the natural language instruction.
1) Datasets: In VLN, natural language instructions can be
a series of detailed action descriptions, a fully described goal,
or just a roughly described task, even only the demands of
human. The tasks that embodied agents need to complete
maybe just a single navigation, or navigation with interaction,
or multiple navigation tasks that need to be completed in
sequence. These differences bring different challenges to VLN,
and many different datasets have been built. Based on these
differences, we introduce some important VLN datasets.
Room to Room (R2R) [105] is a VLN dataset based on
Matterport3D. In R2R, embodied agents navigate according to
step-by-step instructions, choosing the next adjacent naviga-
tion graph node to advance based on visual observations until
they reach the target location. Room-for-Room [106] extends
the paths in R2R to longer trajectories, which requires stronger
long-distance instruction and history alignment capabilities of
embodied agents. VLN-CE [107] extends R2R and R4R to
continuous environments, embodied agents can move freely in
the scene. Different from the above datasets based on indoor
scenes, the TOUCHDOWN dataset [108] is created based
on Google Street View. In TOUCHDOWN, embodied agents
follow instructions to navigate in the street view rendering
simulation of New York City to find the specified object.
Similar to R2R, the REVERIE dataset [109] is also built based
on the Matterport3D simulator. REVERIE requires embodied
agents to accurately locate the distant invisible target object
specified by concise, human-annotated high-level natural lan-
guage instructions. In SOON [110], agents receive a long and
complex instruction from coarse to fine to find the target object
in the 3D environment. During navigation, agents first search
a larger area, and then gradually narrow the search range
according to the visual scene and instructions. DDN [111]
moves a step further beyond these datasets, only providing
human demands without specifying explicit objects. The agent
needs to navigate through the scene to find objects.
ALFRED dataset [112] is based on the AI2-THOR sim-
ulator. In ALFRED, embodied agents need to understand
environmental observations and complete household tasks in
an interactive environment according to coarse-grained and
fine-grained instructions. The task in OVMM [113] dataset
is to pick any object in any unseen environment and place it
TABLE V
COMPARISON OF VLN METHODS.
Method
Model
Year
Feature
LVERG [117]
2020
Graph Learning
CMG [118]
2020
Adversarial Learning
RCM [119]
2021
Reinforcement learning
FILM [120]
2022
Semantic Map
Memory-
LM-Nav [121]
2022
Graph Learning
Understanding
HOP [122]
2022
History Modeling
Based
NaviLLM [123]
2024
Large Model
FSTT [124]
2024
Test-Time Augmentation
DiscussNav [125]
2024
Large Model
GOAT [126]
2024
Causal Learning
VER [127]
2024
Environment Encoder
NaVid [128]
2024
Large Model
LookBY [129]
2018
Reinforcement Learning
Future-
NvEM [130]
2021
Environment Encoder
Prediction
BGBL [131]
2022
Graph Learning
Based
Mic [132]
2023
Large Model
HNR [133]
2024
Environment Encoder
ETPNav [134]
2024
Graph Learning
Others
MCR-Agent [135]
2023
Multi-Level Model
OVLM [136]
2023
Large Model
in a specified location. OVMM provides a simulation based
on Habitat and a framework for implementation in the real
world. Behavior-1K dataset [114] is based on human needs,
comprising 1,000 long-sequence, complex, skill-dependent
daily tasks. Agents need to complete long-span navigation-
interaction tasks which contain thousands of low-level action
steps based on visual information and language instructions.
These complex tasks requires strong capabilities of under-
standing and memory. CVDN [115] requires embodied agents
to navigate to the target based on dialogue history, and ask
questions for help to decide the next action when uncertain.
DialFRED [116], an extension of ALFRED, allows agents to
ask questions during the navigation and interaction process
to get help. These datasets introduce additional oracles, and
embodied agents need to obtain more information beneficial
to navigation by asking questions.
2) Method: VLN has made great strides recently with the
astonishing performance of LLMs, the direction and focus
of VLN have been profoundly influenced. Nevertheless, the
VLN methods can be divided into two directions: Memory-
Understanding Based and Future-Prediction Based.
Memory-Understanding based methods focus on the per-
ception and understanding of the environment, as well as
model design based on historical observations or trajectories,
which is a method based on past learning. Future-Prediction
based methods pay more attention to modeling, predicting, and
understanding the future state, which is a method for future
learning. Since VLN can be regarded as a partially observable
Markov decision process, where future observations depend on
the current environment and actions of the intelligent agent,
historical information has important significance for navi-
gation decisions, especially long-span navigation decisions,
hence Memory-Understanding based methods have always
been the mainstream of VLN. However, Future-Prediction
based methods still have important significance. Its essential
understanding of the environment has great value in VLN
in continuous environments, especially with the rise of the
concept of world model, Future-Prediction based methods are
receiving more and more attention from researchers.
Memory-Understanding based. Graph-based learning is
an essential part of the memory-understanding based method.



================================================================================
第 7 页
================================================================================

【文本内容】
IEEE/ASME TRANSACTIONS ON MECHATRONICS
7
It usually represents the navigation process in the form of a
graph, where the information obtained by the agent at each
time step is encoded as nodes of the graph. The agent obtains
global or partial navigation graph information as a representa-
tion of the historical trajectory. LVERG [117] encoded the
language information and visual information of each node
separately, design a new language and visual entity relation-
ship graph to model the inter-modal relationship between
text and vision, and the intra-modal relationship between
visual entities. LM-Nav [121] used a goal-conditioned distance
function to infer connections between original observation sets
and construct a navigation graph, and extracted landmarks
from the instructions through a LLM. Although HOP [122] is
not based on graph learning, it requires to model time-ordered
information at different granularities, thereby achieving a deep
understanding of historical trajectories and memories.
The navigation graph discretizes the environment, but con-
currently understanding and encoding the environment is also
important. FILM [120] used RGB-D observations and seman-
tic segmentation to gradually build a semantic map from 3D
voxels during the navigation. VER [127] quantified the phys-
ical world into structured 3D units through 2D-3D sampling,
providing fine-grained geometric details and semantics.
Different learning schemes explore how to utilize historical
trajectories and memories better. Through adversarial learning,
CMG [118] alternated between imitation learning and explo-
ration encouragement schemes, effectively strengthening the
understanding of instructions and historical trajectories, short-
ening the difference between training and inference. GOAT
[126] directly trained unbiased models through Backdoor Ad-
justment Causal Learning (BACL) and Frontdoor Adjustment
Causal Learning (FACL), conducted contrastive learning with
vision, navigation history, and their combination to instruc-
tions, enabling the agent to make fuller use of information. The
enhanced cross-modal matching method proposed by RCM
[119] used goal-oriented external rewards and instruction-
oriented internal rewards to perform cross-modal grounding
globally and locally and learns from its own historical good
decisions through self-supervised imitation learning. FSTT
[124] introduced TTA into VLN and optimizes the model in
terms of gradients and model parameters at two scales of time
steps and tasks, effectively improving model performance.
The specific application of large models in Memory-
Understanding based methods is to understand the repre-
sentation of historical memory and to understand the en-
vironment and tasks based on its extensive world knowl-
edge. NaviLLM [123] integrated the historical observation
sequence into the embedding space through the visual encoder,
inputs the multi-modal information of the fusion encoding
into the LLM and fine-tunes it, reaching the state-of-the-art
on multiple benchmarks. NaVid [128] made improvements
in the encoding of historical information, achieves different
degrees of information retention on historical observations and
current observations through different degrees of pooling. LH-
VLN [137] proposed the NavGen platform, the long-horizon
navigation benchmark, and the Multi-Granularity Dynamic
Memory (MGDM) module to enhance task evaluation and
model adaptability in dynamic environments.
Future-Prediction Based. Graph-based learning is also
widely used in Future-Prediction based methods. BGBL [131]
and ETPNav [134] used a similar method to design a waypoint
predictor that can predict movable path points in a continuous
environment based on the observation of the current navigation
graph node. They aim to migrate complex navigation in
a continuous environment to node-to-node navigation in a
discrete environment, thereby bridging the performance gap
from discrete environments to continuous environments.
Improving the understanding and perception of the future
environment through environmental encoding is also one of
the research directions for predicting and exploring the future.
NvEM [130] used a theme module and a reference module to
perform fusion encoding of neighbor views from the global
and local perspectives. This is actually an understanding and
learning of future observations. HNR [133] used a large-
scale pre-trained hierarchical neural radiation representation
model to directly predict the visual representation of the
future environment rather than pixel-level images using three-
dimensional feature space encoding, and builds a navigable
future path tree based on the representation of the future envi-
ronment. They predict the future environment from different
levels, providing effective references for navigation decisions.
Some reinforcement learning methods are also applied to
predict and explore future states. LookBY [129] employed
reinforcement prediction to enable the prediction module to
imitate the world and forecast future states and rewards. This
allows the agent to directly map “current observations” and
“predictions of future observations” to actions, achieving state-
of-the-art performance at the time. The rich world knowledge
and zero-shot performance of large models provide many
possibilities for Future-Prediction based methods. MiC [132]
required the LLM to directly predict the target and its possible
location from the instructions and provides navigation instruc-
tions through the description of scene perception. This method
requires LLMs to fully exert its ‘imagination’ and build an
imagined scene through prompts.
In addition, there are some methods that both learn from
the past and for the future. MCR-Agent [135] designed a
three-layer action strategy, which requires the model to predict
the target from the instructions, predict the pixel-level mask
for the target to be interact, and learn from the previous
navigation decision; OVLM [136] required the LLMs to
predict the corresponding operations and landmark sequences
for the instructions. During the navigation process, the visual
language map will be continuously updated and maintained,
and the operations will be linked to the waypoints on the map.
V. EMBODIED INTERACTION
Embodied interaction refer to scenarios where agents inter-
act with humans and the environment in physical or simulated
space. The typical embodied interaction tasks are Embodied
Question Answering (EQA) and embodied grasping.
A. Embodied Question Answering
For EQA task, the agent needs to explore the environment
from a first-person perspective to gather information necessary



================================================================================
第 8 页
================================================================================

【文本内容】
IEEE/ASME TRANSACTIONS ON MECHATRONICS
8
TABLE VI
COMPARISON OF DIFFERENT EQA DATASETS.
Dataset
Year
Type
Data Sources
Simulator
Query Creation
Answer
Size
EQA v1 [138]
2018
Active EQA
SUNCG
House3D
Rule-Based
open-ended
5,000+
MT-EQA [139]
2019
Active EQA
SUNCG
House3D
Rule-Based
open-ended
19,000+
MP3D-EQA [140]
2019
Active EQA
MP3D
Simulator based on MINOS
Rule-Based
open-ended
1,136
IQUAD V1 [141]
2018
Interactive EQA
-
AI2THOR
Rule-Based
multi-choice
75,000+
VideoNavQA [142]
2019
Episodic Memory EQA
SUNCG
House3D
Rule-Based
open-ended
101,000
SQA3D [143]
2022
QA only
ScanNet
-
Manual
multi-choice
33,400
K-EQA [144]
2023
Active EQA
-
AI2THOR
Rule-Based
open-ended
60,000
OpenEQA [145]
2024
Active EQA, Episodic Memory EQA
ScanNet, HM3D
Habitat
Manual
open-ended
1,600+
HM-EQA [146]
2024
Active EQA
HM3D
Habitat
VLM
multi-choice
500
S-EQA [147]
2024
Active EQA
-
VirtualHome
LLM
binary
-
EXPRESS-Bench [148]
2025
Exploration-aware EQA
HM3D
Habitat
VLM
open-ended
2,044
Single Objective
Question: What room 
is the vase located in?
Multiple Objectives
Multi-agent
Question: Is there a washing 
machine in the house?
: A1
: A2
: A3
ANSWER
Interaction
Question: Are there any 
apples in the fridge?
Knowledge-based
Question: Is there an object in the 
living room used to lower the 
temperature?
Episodic Memory
Object States
Question: Could someone be watching 
TV in the living room?
Question: Is the dinner table the 
same color as the TV cabinet?
Action: 
open the fridge
TV   
on
curtain  
withdrawn
air conditioner   
on
Answer: Living room.
Answer: No.
Answer: Yes.
Answer: No.
Answer: It is mounted on the wall 
above the cabinets, directly across from 
the stove.
Answer: Yes.
Question: Where is the clock?
Answer: Yes.
turn 
right
turn 
right
turn 
right
move 
forward
move 
forward
turn 
left
...
...
Exploring...
Episodic 
History
move 
forward
Fig. 7. The gray box displays the scenes an agent observes during exploration.
The other boxes show various types of question answering tasks. Except for
the task of answering questions based on episodic memory, the agent ceases
exploration once it has gathered sufficient information to answer the question.
to answer the given questions. An agent with autonomous
exploration and decision-making capabilities must not only
consider which actions to take to explore the environment but
also determine when to stop exploring to answer questions.
Existing works focus on different types of questions, some of
which are shown in Fig. 7. In this section, we will introduce
the existing datasets, discuss the related methods, describe the
metrics used to evaluate model performance, and address the
remaining limitations of this task.
1) Datasets: We briefly introduce several embodied ques-
tion answering datasets, which are summarized in Table VI.
EQA v1 [138] is the first dataset designed for EQA. Built
on synthetic 3D indoor scenes from the SUNCG dataset [73]
within the House3D [149] simulator, EQA v1 comprises four
types of questions: location, color, color room, and preposi-
tion. Similar to EQA v1, MT-EQA [139] is built in House3D
using SUNCG by executing functional programs consisting of
some basic operations. However, it further extends the single-
object question answering task to a multi-object setting. Six
types of questions are designed, involving the comparison of
color, distance, and size between multiple objects. MP3D-
EQA [140] is built on a simulator developed based on MINOS
[150] using the Matterport3D dataset [151], expanding the
question-answering task to a realistic 3D environment. Refer-
ring to EQA v1, MP3D-EQA utilizes three types of templates:
location, color, and color room, generating a total of 1,136
questions in 83 home environments. IQUAD V1 [141] is built
upon AI2-THOR and consists of three types of questions:
existence, counting, and spatial relationships. Unlike other
datasets, answering IQUAD V1 questions requires the agent
to have a good understanding of affordances and interact
with the dynamic environment. VideoNavQA [142] decouples
the visual reasoning from the navigation aspect of the EQA
problem. In this task, the agent accesses videos corresponding
to exploration trajectories with sufficient information to answer
questions. SQA3D [143] simplifies protocol (QA only) while
still preserving the function of benchmarking embodied scene
understanding, enabling more complex, knowledge-intensive
questions and a much larger scale of data collection.
Unlike previous datasets that explicitly specify target objects
in questions, K-EQA [144] features complex questions with
logical clauses and knowledge-related phrases, requiring prior
knowledge to answer. OpenEQA [145] is the first open-
vocabulary dataset for EQA, supporting both episodic memory
and active exploration cases. The episodic memory EQA (EM-
EQA) tasks involve an agent developing an understanding
of the environment from its episodic memory to answer
questions. In active EQA (A-EQA) tasks, the agent answers
questions by taking exploratory actions to gather necessary
information. Utilizing GPT4-V, HM-EQA [146] is constructed
in the Habitat simulator using HM3D. It includes 500 ques-
tions across 267 different scenes, which can be roughly
categorized into identification, counting, existence, status, and
location. S-EQA [147] leverages GPT-4 in VirtualHome for
data generation and employs cosine similarity calculations to
decide whether to retain the generated data, thereby enhancing
dataset diversity. In S-EQA, answering questions requires the
assessment of a collection of consensus objects and states to
reach an existential “Yes/No” answer. EXPRESS-Bench [148]
is the largest exploration-aware EQA dataset that consists of
777 exploration trajectories and 2,044 samples. It also intro-
duces novel evaluation metrics to ensure faithful assessment.
2) Methods: The embodied question answering task mainly
involves navigation and question-answering subtasks, with
implementation methods broadly categorized into two types:
neural network-based and LLMs/VLMs-based.
Neural Network Methods. In early work, researchers
mainly addressed the embodied question answering task by
building deep neural networks. They trained and fine-tuned
these models using techniques such as imitation learning and
reinforcement learning to improve performance.
The EQA task was first proposed by Das et al. [138].
In their work, the agent consists of four main modules:


【图像 1】(270x274)
该图像并非来自您提供的PDF文本上下文（即第8页中的表格内容），而是与之无关的一张室内照片。以下是对这张图像的详细分析：

---

**1. 图像类型**  
这是一张**室内环境的照片**，展示了一个客厅场景。

---

**2. 图像中的所有文字内容**  
图像中**没有可见的文字内容**。

---

**3. 图像中的关键视觉元素和数据信息**  
- **房间结构**：  
  - 房间具有倾斜的木质天花板，表明可能是阁楼或坡屋顶结构。
  - 墙面为深绿色，右侧墙面为浅色（白色或米色）。
  - 天花板上有一个方形天窗，但玻璃部分缺失或被遮挡，呈现黑色空洞状。
- **家具与设备**：  
  - 左侧有一台电视机，放置在深色木制电视柜上，电视屏幕显示模糊画面（可能为静态图像或视频）。
  - 电视柜下方有储物格，摆放着一些物品。
  - 右侧有一张棕色沙发，上面放有靠垫和一条毯子。
  - 沙发前地面上有一块地毯。
- **空调设备**：  
  - 在电视上方墙壁安装了一台壁挂式空调。
- **装饰物**：  
  - 左侧墙上挂有一幅小画框。
  - 电视两侧各有一幅装饰画。
  - 右侧墙上有另一幅较大的画框。
- **光线与氛围**：  
  - 室内光线较暗，主要光源来自天窗和可能的室内灯光。
  - 整体氛围略显陈旧，但整洁。

---

**4. 图像传达的主要信息**  
这张照片展示了一个典型的家庭客厅环境，重点突出了其空间布局、家具配置和建筑特征（如斜顶、天窗）。由于图像未包含任何标注或技术细节，它更可能用于**环境感知、室内场景理解或视觉导航任务的示例图像**，例如在机器人导航或视觉问答（EQA）研究中作为场景背景。

> 注：此图像与您提供的PDF上下文（关于EQA数据集的表格）无直接关联，可能是独立附图或误插入。

【图像 2】(270x269)
该图像为一张室内场景的照片，具体分析如下：

1. **图像类型**：  
   这是一张真实拍摄的室内环境照片（非图表、截图或图形），展示了一个家庭厨房与餐厅相连的空间。

2. **图像中的所有文字内容**：  
   图像中无可见文字内容。

3. **图像中的关键视觉元素和数据信息**：  
   - **空间布局**：画面左侧为厨房区域，设有白色橱柜、台面和部分电器；右侧为用餐区，有一张铺着桌布的餐桌和多把椅子。
   - **家具与装饰**：餐桌上摆放有黄色餐盘、植物和装饰物；墙上挂有画作或相框；天花板上有一个圆形灯具。
   - **光线与氛围**：自然光从窗户照入，整体色调偏暖，营造出居家生活感。
   - **细节特征**：厨房上方有悬挂的装饰性物品（如干花或编织物）；地板为浅色瓷砖或复合材料。

4. **图像传达的主要信息**：  
   该图像用于展示一个典型的住宅内部环境，可能作为环境问答任务（EQA）中的视觉输入示例，尤其适用于需要理解空间布局、物体位置和语义关系的任务。结合上下文表格（TABLE VI），此图可能是用于说明“Active EQA”或“Interactive EQA”类数据集中的典型场景，例如来自AI2THOR或类似仿真平台的真实风格渲染图，用以训练模型进行视觉问答。

【图像 3】(272x271)
1. **图像类型**：  
   这是一张室内环境的**照片**，具体为一个客厅场景的实景图。

2. **图像中的所有文字内容**：  
   图像中**没有可见的文字内容**。电视屏幕显示的画面模糊不清，无法辨识其中的文字或信息。

3. **图像中的关键视觉元素和数据信息**：  
   - **房间结构**：斜顶天花板，木质屋顶，墙面为深绿色与浅色拼接。
   - **家具布置**：
     - 一台电视机放置在黑色电视柜上，电视柜有多个储物格。
     - 电视柜两侧各有一个装饰画框。
     - 右侧有一张棕色沙发，沙发上放有靠垫和毯子。
     - 左侧角落可见部分家具（可能是书架或边桌）。
   - **设备**：墙上安装有一台空调，位于电视上方。
   - **窗户/天窗**：屋顶中央有一个方形开口（可能是天窗或通风口），内部较暗。
   - **地面**：浅色地板，可能为瓷砖或复合地板。
   - **整体氛围**：居家环境，略显陈旧但功能齐全。

4. **图像传达的主要信息**：  
   该图像展示了一个典型的室内居住空间，用于**环境感知或视觉问答任务中的场景示例**。结合上下文（表格VI讨论EQA数据集），此图很可能是来自某个**基于模拟器或真实环境的视觉问答数据集**（如House3D、AI2THOR等）中的样本图像，用于测试模型对室内布局、物体识别和空间关系的理解能力。其主要用途是作为**视觉输入**，配合问题（如“电视旁边有什么？”）进行推理与回答。

【图像 4】(231x231)
1. **图像类型**：  
   这是一张室内环境的**照片**，具体为一个厨房的实景图像。

2. **图像中的所有文字内容**：  
   图像中**没有可见的文字内容**。

3. **图像中的关键视觉元素和数据信息**：  
   - **场景**：一个典型的家庭厨房，包含灶台、水槽、冰箱、橱柜等常见设施。  
   - **布局**：厨房呈狭长型，左侧为烹饪区（炉灶、抽油烟机、操作台），中间有水槽，右侧为储物柜和冰箱。  
   - **颜色与材质**：墙面为浅绿色，地板为浅色木质地板，橱柜为米白色，整体色调明亮温馨。  
   - **细节**：灶台上放置了锅具，水槽旁有餐具，冰箱顶部堆放了一些物品（如红色容器），背景可见通往其他房间的门。  
   - **光照**：自然光从远处门口进入，结合室内灯光，整体照明良好。

4. **图像传达的主要信息**：  
   该图像展示了一个用于**环境问答任务（EQA）的虚拟或真实室内场景示例**，可能作为训练或测试数据集中的视觉输入。结合上下文表格（TABLE VI）中提到的“SUNCG”、“House3D”、“AI2THOR”等三维环境模拟器，此图可能是这些仿真环境中生成的逼真渲染图像，用于支持**基于场景理解的问答系统**研究，例如回答关于物体位置、空间关系或动作可行性的问题。

【图像 5】(231x226)
1. **图像类型**：  
   这是一张室内环境的**照片**，展示了一个住宅内部的场景，可能是用于视觉问答（EQA）任务的数据来源之一。

2. **图像中的所有文字内容**：  
   图像中**没有可见的文字内容**。虽然背景中有一个挂钟，但其显示的时间或刻度无法清晰辨认，且无其他可读文本。

3. **图像中的关键视觉元素和数据信息**：  
   - **房间布局**：画面展示了一条走廊，通向一个带有门的房间，地面为浅色木地板。
   - **墙壁颜色**：墙壁为浅绿色，营造出温馨的家居氛围。
   - **家具与物品**：
     - 左侧有白色储物柜和衣柜，部分柜子上摆放着装饰品或杂物。
     - 右侧有一个高大的木质隔断或柜子，上面堆放着大量书籍或杂志，排列整齐。
     - 墙上挂有一个圆形时钟，指针指向约10:10的位置（推测）。
     - 走廊尽头可见一扇木门，门框为浅色，门后似乎有光线透入。
   - **光照情况**：自然光从远处门或窗户进入，整体光线明亮均匀。

4. **图像传达的主要信息**：  
   该图像呈现了一个典型的家庭室内环境，具有丰富的空间结构和日常物品布置，可能用于**视觉问答（EQA）任务**中作为场景输入。结合上下文中的“House3D”、“SUNCG”等数据源，此图很可能来自一个基于三维模拟器生成的虚拟室内场景，用于训练或测试模型对空间关系、物体位置和语义理解的能力。它体现了**交互式或主动式视觉问答任务**所需的复杂环境细节。

【图像 6】(148x151)
该图像为一张**室内场景照片**，具体描述如下：

---

### 1. 图像类型  
**照片**（实景拍摄的室内环境）

---

### 2. 图像中的所有文字内容  
**无文字内容**。图像中未包含任何可见的文字或标签。

---

### 3. 图像中的关键视觉元素和数据信息  
- **房间布局**：一个客厅或起居室的内部场景。
- **主要家具**：
  - 一台大型平板电视，放置在黑色电视柜上。
  - 电视柜下方有多个储物格，部分关闭，部分敞开。
  - 左侧有一张棕色皮质沙发。
  - 前景中有一个木质咖啡桌，表面略显陈旧，带有纹理和磨损痕迹。
- **墙面与装饰**：
  - 墙面为深绿色，营造出较沉稳的氛围。
  - 电视上方墙上挂有一幅小型画作或相框。
- **其他细节**：
  - 电视柜右侧摆放着一些小物件，如花瓶、装饰品等。
  - 地板为浅色木地板。
  - 整体光线柔和，来自室内照明。

---

### 4. 图像传达的主要信息  
该图像展示了一个典型的现代家庭客厅环境，用于**视觉场景理解或空间感知任务**的上下文背景。结合其所在论文的上下文（IEEE/ASME TRANSACTIONS ON MECHATRONICS，涉及EQA数据集比较），此图可能作为**真实世界环境示例**，用于说明某些EQA（Environment Question Answering）数据集（如House3D、AI2THOR等）所模拟或采集的真实场景参考。它强调了复杂室内环境中的物体布局、视角和光照条件，有助于研究机器人导航、视觉问答或交互式环境理解等任务。

---

### 补充说明  
尽管该图本身无文字，但其作为**真实世界场景的视觉样本**，可能被用于训练或评估基于视觉的问答系统（如EQA任务），尤其适用于需要理解空间关系、物体位置和语义内容的模型。

【图像 7】(147x151)
1. **图像类型**：  
   这是一张室内环境的**照片**，展示了一个住宅空间的局部场景，可能用于视觉问答（EQA）任务中的数据来源。

2. **图像中的所有文字内容**：  
   图像中**没有可见的文字内容**。该图是纯视觉场景，未包含任何文本或标注。

3. **图像中的关键视觉元素和数据信息**：  
   - **空间布局**：画面显示一个带有楼梯的室内角落，左侧为深色木质楼梯扶手，右侧为客厅区域。
   - **家具**：右侧有一把米色布艺扶手椅，靠墙放置；椅子上方墙上挂着一幅小画框。
   - **窗户与自然光**：背景中有一扇大窗户，窗外可见绿色植物，阳光透过窗户照亮室内，营造明亮氛围。
   - **装饰物**：窗边有一个白色花瓶，内插有绿色植物，上方悬挂着一个白色的吊灯。
   - **墙面颜色**：右侧墙壁为蓝绿色，与浅色地板形成对比。
   - **整体风格**：现代家居风格，简洁温馨。

4. **图像传达的主要信息**：  
   该图像作为**视觉问答（EQA）任务中的场景示例**，用于模拟真实家庭环境，供模型理解空间关系、物体位置和语义信息。结合上下文表格（如SUNCG、House3D、AI2THOR等数据集），此图可能来源于AI2THOR或类似虚拟环境平台，用于训练或评估交互式视觉问答系统的能力，例如回答关于“椅子旁边有什么？”、“窗户在哪里？”等问题。

【图像 8】(125x129)
该图像为一张**室内场景的截图**，可能来自一个三维环境模拟器（如AI2THOR、House3D等），用于视觉问答（EQA）任务的研究背景。

---

### 1. 图像类型  
**截图**（具体为三维虚拟环境中的室内场景截图）

---

### 2. 图像中的所有文字内容  
**无文字内容**。图像中未包含任何可见文本或标注。

---

### 3. 图像中的关键视觉元素和数据信息  
- **房间结构**：画面展示了一个室内角落，墙面呈浅绿色，天花板为木质纹理。
- **窗户**：墙上有一个方形窗户，窗框为白色，窗外为黑色（表示外部黑暗或未渲染区域）。
- **空调设备**：窗户下方安装了一台白色的壁挂式空调。
- **视角**：采用斜角俯视视角，突出空间布局与物体位置关系。
- **光照**：室内光线均匀，无明显阴影，符合模拟环境中常见的渲染风格。

---

### 4. 图像传达的主要信息  
此图展示了用于**交互式环境问答（Interactive EQA）任务的虚拟室内场景示例**，可能是从AI2THOR等仿真平台中截取的。其目的是体现复杂室内环境中的对象布局与空间关系，用于训练模型理解环境并回答基于视觉的问题（如“空调在哪里？”、“窗户旁边有什么？”）。结合上下文表格（TABLE VI）中提到的IQUAD V1和AI2THOR，可推断此图为IQUAD数据集中的典型场景样本，强调**交互性与多选择题型的问答任务**。

--- 

> 总结：该图是**一个三维虚拟室内环境的截图**，用于说明交互式环境问答任务中所涉及的视觉场景，无文字，但具有明确的空间结构和常见家居元素（窗、空调），服务于机器感知与推理研究。

【图像 9】(254x253)
1. **图像类型**：  
   这是一张室内环境的**照片**，展示了一个真实的客厅场景。

2. **图像中的所有文字内容**：  
   图像中**没有可见的文字内容**。

3. **图像中的关键视觉元素和数据信息**：  
   - **空间结构**：房间具有斜屋顶设计，天花板为木质结构，呈浅棕色。左侧墙壁为深绿色，右侧为白色。
   - **家具布置**：
     - 左侧有一台电视机放置在黑色电视柜上，电视正在播放画面。
     - 电视柜前方有一个带有图案的木制茶几。
     - 中间区域摆放着一张棕色沙发和一把白色扶手椅，椅子上搭有毛毯。
     - 地面铺有深色地毯。
   - **设备与装饰**：
     - 深绿色墙上安装有一台空调（壁挂式）。
     - 墙上挂有画框或装饰物。
     - 房间后方可见一扇门通往其他区域，门旁有镜子。
     - 天花板上有一个方形开口，可能是天窗或通风口。
   - **光照与氛围**：自然光从右侧进入，整体光线柔和，营造出温馨的居家氛围。

4. **图像传达的主要信息**：  
   该图像展示了一个用于**环境问答任务（EQA）的典型室内场景**，可能作为模拟器（如AI2THOR、House3D等）中虚拟环境的参考或真实数据来源。结合上下文表格内容（如IQUAD V1使用AI2THOR），此图可能用于说明交互式环境问答任务中所涉及的真实世界环境复杂性，包括家具布局、视角变化和多模态感知需求。

【图像 10】(253x269)
1. **图像类型**：  
   这是一张室内场景的彩色照片，展示了一个住宅客厅的内部环境。

2. **图像中的所有文字内容**：  
   图像中**没有可见的文字内容**。电视屏幕上显示的是一个模糊的画面，但无法辨认具体文字或字幕。

3. **图像中的关键视觉元素和数据信息**：  
   - **房间结构**：斜顶天花板，木质屋顶板，墙壁为深绿色。
   - **家具布置**：
     - 左侧有一台电视机，放置在黑色电视柜上，电视柜下方有储物空间。
     - 电视前有一个复古风格的木箱（可能是装饰品或储物箱）。
     - 中央区域有一张棕色沙发，搭配靠垫。
     - 右侧有一把白色扶手椅，上面搭着毯子。
     - 地面铺有灰色地毯。
   - **其他细节**：
     - 墙上安装了一台空调。
     - 天花板上有一个方形开口（可能是通风口或采光窗）。
     - 房间右侧可见通往另一房间的门廊，背景中隐约可见床铺，表明可能连接卧室。
     - 整体光线自然，来自窗户（未直接显示），营造出温馨的家庭氛围。

4. **图像传达的主要信息**：  
   该图像呈现了一个典型的家庭客厅场景，用于**环境问答（EQA）任务的视觉上下文**。结合上下文表格（TABLE VI）中提到的“SUNCG”、“House3D”等虚拟环境数据集，此图很可能是从这些三维模拟环境中渲染出的真实感场景，用以支持**基于视觉的问答系统测试**，例如回答关于房间布局、物体位置或功能的问题。其目的是提供具有真实感和复杂性的环境样本，供模型进行视觉理解与推理训练。

【图像 11】(256x271)
1. **图像类型**：  
这是一张室内环境的**照片**，展示了一个住宅空间的内部场景，属于真实世界环境的视觉记录。

2. **图像中的所有文字内容**：  
图像中**没有可见的文字内容**。该图像为纯视觉场景，未包含任何文本或标注。

3. **图像中的关键视觉元素和数据信息**：  
- **空间布局**：图像显示一个开放式的客厅区域，连接着走廊和多个房间（如卧室、浴室等），具有典型的住宅结构。
- **家具与装饰**：包括沙发、扶手椅、茶几、挂画、植物等，营造出居家氛围。
- **材质与颜色**：地板为浅色木质材料，天花板为深色木板，墙壁为浅色调，整体风格偏向现代乡村风。
- **光照与视角**：自然光从右侧窗户进入，画面明亮，拍摄角度为室内平视视角，略带广角效果，展现了较宽的空间视野。
- **门与通道**：多扇门通向不同房间，部分门处于开启状态，显示了空间的连通性。

4. **图像传达的主要信息**：  
该图像用于展示一个**真实的室内环境**，可能作为环境问答（EQA）任务中的视觉输入示例。结合上下文（如表格中提到的SUNCG、House3D、AI2THOR等数据集），此图很可能是用于**视觉-语言理解任务**（如视觉问答、导航或交互式推理）的参考场景，体现复杂室内结构和日常家居布置，供模型学习空间关系与语义理解。

【图像 12】(255x271)
1. **图像类型**：  
   这是一张室内场景的**照片**，展示了一个住宅内部的客厅或餐厅区域。

2. **图像中的所有文字内容**：  
   图像中**没有可见的文字内容**。图像本身为纯视觉场景，未包含任何文本信息。

3. **图像中的关键视觉元素和数据信息**：  
   - **空间布局**：房间呈开放式结构，左侧为厨房区域（白色橱柜、台面），右侧为用餐区（餐桌与椅子）。
   - **家具**：包括一张铺有花纹桌布的长方形餐桌，周围摆放着几把木质餐椅；墙上挂有画作，角落有植物装饰。
   - **墙面与装饰**：左侧墙面为浅绿色竖条纹墙板，墙上挂有一幅风景画；背景墙为白色，窗户较大，带有窗帘，自然光充足。
   - **其他细节**：天花板上有一个圆形灯具，房间内有绿植点缀，整体风格偏向传统家居设计。
   - **环境氛围**：光线明亮，整洁有序，具有生活气息。

4. **图像传达的主要信息**：  
   该图像展示了一个典型的室内家居环境，可能用于**视觉问答（EQA）任务中的场景示例**，特别是在涉及空间理解、物体识别和语义推理的AI研究中。结合上下文（如表格中提到的SUNCG、House3D、AI2THOR等数据集），此图可能是某个虚拟或真实室内场景的数据样本，用于训练或评估模型在复杂环境中回答问题的能力。

【图像 13】(243x262)
1. **图像类型**：  
   这是一张室内环境的**照片**，展示了一个真实的家居场景，可能用于视觉问答（EQA）任务中的数据采集或示例。

2. **图像中的所有文字内容**：  
   图像中**没有可见的文字内容**。背景墙上的画框、家具标签等均无可读文字。

3. **图像中的关键视觉元素和数据信息**：  
   - **空间布局**：一个开放式客厅与厨房区域相连，地面为浅色瓷砖，天花板有木质横梁装饰。
   - **家具与物品**：
     - 左侧有多个白色储物柜，上方堆放杂物。
     - 中间靠墙有一个高大的浅色衣柜。
     - 右侧有一张餐桌，铺着黄色桌布，上面放有餐具和绿色植物。
     - 餐桌旁有几把椅子，其中一把为棕色皮质。
     - 窗户位于右侧墙，挂着白色窗帘，自然光透入。
     - 墙上挂有相框和装饰画。
   - **颜色与材质**：整体色调偏暖，以米白、浅绿、木色为主，营造温馨氛围。

4. **图像传达的主要信息**：  
   该图像呈现了一个典型的家庭室内环境，用于**视觉问答（EQA）任务的上下文场景**，可能作为训练或测试数据集的一部分。结合上下文表格（TABLE VI），此类图像通常用于“主动式”或“交互式”EQA任务，支持基于规则生成的问题（如“桌子上有多少盆植物？”），并要求模型理解空间关系与物体属性。此图可用于评估模型在真实复杂环境下的视觉理解能力。

【图像 14】(247x265)
1. **图像类型**：  
   这是一张室内环境的**照片**，具体为一个住宅房间的实景截图，可能用于模拟或数据集构建中的场景示例。

2. **图像中的所有文字内容**：  
   图像中**没有可见的文字内容**。背景中虽有家具和装饰物，但未出现可读的文字。

3. **图像中的关键视觉元素和数据信息**：  
   - **房间布局**：画面展示了一个带有绿色墙壁的室内空间，包含多个门洞和家具。
   - **家具与物品**：
     - 左侧有一个白色储物柜，柜顶摆放着杂物（如红色容器）。
     - 中间有一扇打开的门，通向另一个房间，门后可见一面挂钟（圆形，黑色边框）和部分家具。
     - 右侧是一个高大的浅色木制衣柜，顶部堆放着多件彩色衣物。
     - 墙上挂着一些布料或衣物。
   - **颜色与光照**：墙面为浅绿色，光线来自右侧窗户，整体明亮自然。
   - **视角**：第一人称视角，类似机器人或虚拟代理在室内导航时的视图。

4. **图像传达的主要信息**：  
   该图像展示了用于**环境问答（EQA）任务的典型室内场景**，可能来源于如AI2THOR、SUNCG或House3D等仿真平台。其目的是提供真实感强的视觉上下文，供模型理解空间结构、物体位置及语义关系，以支持基于视觉的问答任务（如“钟表在哪里？”）。结合上下文表格，此类图像常用于训练和评估主动或交互式环境问答系统。

【图像 15】(238x257)
1. **图像类型**：  
   这是一张室内环境的**照片**，具体为一个住宅厨房的实景图。

2. **图像中的所有文字内容**：  
   图像中**没有可见的文字内容**。

3. **图像中的关键视觉元素和数据信息**：  
   - **空间布局**：画面展示了一个狭长型的厨房，呈“走廊式”结构，左侧为灶台、水槽和橱柜，右侧为高柜和储物空间。  
   - **家具与设备**：包括白色炉灶、双槽水槽、抽油烟机、冰箱（位于背景中间）、吊柜和地柜。  
   - **地面材质**：浅色木质地板，具有自然纹理。  
   - **墙面与装饰**：墙壁为淡绿色，部分区域贴有瓷砖（如灶台后方）。  
   - **光照与视角**：自然光从远处门口进入，整体光线明亮；拍摄视角为平视，略带广角效果，展现空间纵深。  
   - **其他细节**：门框为白色，远处可见通往其他房间的通道；右侧柜子上放置了红色花瓶等装饰物。

4. **图像传达的主要信息**：  
   该图像用于展示一个典型的室内家居环境，可能作为**场景理解或视觉问答任务的数据来源**，尤其适用于涉及空间导航、物体识别或交互式问答的研究场景。结合上下文（表格VI中提到的EQA数据集），此图很可能属于**AI2THOR模拟器或类似平台生成的虚拟/真实室内环境数据集**，用于训练或评估机器人或视觉系统在复杂家庭环境中的感知与推理能力。

【图像 16】(250x265)
1. **图像类型**：  
   这是一张室内环境的照片，具体为一个住宅客厅的实景图。

2. **图像中的所有文字内容**：  
   图像中无可见文字内容。

3. **图像中的关键视觉元素和数据信息**：  
   - **房间结构**：斜顶天花板，木质屋顶，墙面为蓝绿色。
   - **家具布置**：
     - 左侧有一段深色木制楼梯，通向二楼。
     - 靠窗处放置一张米色单人沙发，旁边有落地灯。
     - 中央靠墙位置有一个电视柜，上面摆放一台大屏幕电视，正在播放画面（内容模糊）。
     - 电视柜前地面上有一个旧式纸箱或储物箱。
   - **窗户与采光**：左侧有一扇大窗户，自然光进入室内。
   - **空调设备**：右侧墙上安装了一台壁挂式空调。
   - **装饰细节**：墙上挂有小相框，电视柜上有一些小摆件。
   - **整体风格**：现代简约风格，略带乡村气息。

4. **图像传达的主要信息**：  
   该图像展示了一个典型的家庭室内环境，可能用于模拟或测试场景理解任务（如视觉问答、导航等）。结合上下文中的“EQA”（Environment Question Answering）数据集，此图可能是用于构建交互式或主动式视觉问答任务的参考场景之一，强调真实感和空间布局复杂性。它可能作为训练或评估AI系统在理解室内环境、回答关于物体位置或功能问题时的基准场景。

【图像 17】(254x270)
1. **图像类型**：  
   这是一张室内场景的**照片**，展示了一个家庭住宅的客厅与餐厅区域。

2. **图像中的所有文字内容**：  
   图像中**无可见文字内容**。背景中可能有装饰画或家具上的标签，但无法辨识具体文字。

3. **图像中的关键视觉元素和数据信息**：  
   - **空间布局**：图像呈现一个开放式空间，左侧为客厅区域（含绿色墙面、门、植物），右侧为餐厅区域（含餐桌、椅子、桌布）。
   - **家具与装饰**：
     - 餐桌上有黄色桌布、餐具和花瓶。
     - 椅子为深色木质，搭配黑色椅垫。
     - 墙上挂有画框。
     - 左侧角落有一盆大型绿植。
     - 门后可见厨房区域（白色橱柜、冰箱）。
   - **光线与环境**：自然光从右侧窗户照入，整体明亮温馨，地板为浅色木质或复合材料。
   - **风格**：现代家居风格，偏传统温馨设计。

4. **图像传达的主要信息**：  
   该图像用于展示一个**真实或模拟的家庭室内环境**，可能是作为环境问答（EQA）任务中的视觉输入示例。结合上下文（如表格中提到的SUNCG、House3D、AI2THOR等数据集），此图可能代表一种用于训练或测试视觉问答系统的**三维场景渲染图**或**真实场景照片**，旨在提供丰富的空间与物体细节，支持对环境的理解与推理任务（如“餐桌上有几个杯子？”、“绿色植物在哪个房间？”）。


================================================================================
第 9 页
================================================================================

【文本内容】
IEEE/ASME TRANSACTIONS ON MECHATRONICS
9
Grounding & Grasp Pose Generation
(a) Language-guided Grasping
Direct Object 
Specification
Inference 
Required
Baymax.
I am thirsty, can you 
give me something 
to drink?
Spatial 
Reasoning
Logical 
Reasoning 
Grasp the keyboard 
that is to the right of 
the brown kleenex box.
Scene
Instruction
Grasp the knife at 
its handle.
Agent with Arm
Instruction
Scene
Grasping
Observation
Action
Grounding & Grasp Pose Generation
(b) Human-Agent-Object Interaction
(c) Publication Status
0
100
200
400
300
500
600
700
2017
2018 2019
2020 2021 2022 2023
Total published papers Number of papers published per year
Fig. 8.
The overview of the embodied grasping task. (a) demonstrates examples of language-guided grasping for different types of tasks, (b) provides an
overview of human-agent-object interaction, (c) shows Google Scholar search results for topics of “Language-guided Grasping”.
vision, language, navigation, and answering. These modules
are primarily constructed using traditional neural building
blocks: Convolutional Neural Networks (CNNs) and Recurrent
Neural Networks (RNNs). Some subsequent works [152] re-
tained modules like the question answering module proposed
by Das et al. [138] and improved the model. Additionally,
Wu et al. [152] proposed integrating the navigation and QA
modules into a unified SGD training pipeline for joint training,
thereby avoiding employing deep reinforcement learning to
simultaneously train the separately trained navigation and
question answering modules. From the perspective of task
singularity, several works [153] expanded the task to include
multiple objectives and multi-agent, respectively, making it
necessary for the model to store and integrate the information
obtained by the agent’s exploration through methods such as
feature extraction and scene reconstruction. Considering the
interaction between the agent and the dynamic environment,
Gordon et al. [141] introduced the Hierarchical Interactive
Memory Network. There is also a limitation in previous works
where agents are unable to use external knowledge to answer
complex questions and lack knowledge of the explored parts
of the scene. To address this, Tan et al. [144] leveraged
the neural program synthesis method and the table converted
from the knowledge and 3D scene graphs, allowing the action
planner to access object-related information. Additionally, an
approach based on Monte Carlo Tree Search (MCTS) is used
to determine the next location for the agent.
LLMs/VLMs Methods. Majumdar et al. [145] used LLMs
and VLMs for episodic memory EQA (EM-EQA) and Active
EQA (A-EQA) tasks. For EM-EQA task, they considered
Blind LLMs, Socratic LLMs with language descriptions of
the episodic memory, Socratic LLMs with descriptions of
the constructed scene graph, and VLMs processing multiple
scene frames. The A-EQA task extended EM-EQA methods
with frontier-based exploration (FBE) [154] for problem-
independent environment exploration. Some works [146],
[155] employed frontier-based exploration method to iden-
tify areas for subsequent exploration and to build semantic
maps. They ended the exploration early utilizing conformal
prediction or image-text matching to avoid over-exploration.
Patel et al. [156] emphasized the question answering aspect
of the task. They leveraged multiple LLM-based agents to
explore the environment and enable them to independently
answer questions with “yes” or “no” answers. These individual
responses are utilized to train a Central Answer Model, to
aggregate responses and generate robust answers.
3) Limitations: a) Dataset: Constructing datasets requires
substantial manpower and resources. Additionally, there are
still few large-scale datasets, and the metrics for evaluating
model performance vary across different datasets, complicat-
ing the testing and comparison of performance, b) Model:
Despite the advancements brought by LLMs, the performance
of these models still lags significantly behind human levels.
Future work may focus more on effectively storing environ-
mental information explored by agents and guiding them to
plan actions based on environmental memory and questions,
while also enhancing the interpretability of their actions.
B. Embodied Grasping
Embodied interaction includes not only question-answering
but also performing tasks like grasping and placing objects
based on human instructions. Embodied grasping combines
traditional kinematic methods [157], [158] with large models
such as LLMs and vision-language models, enabling multi-
sensory perception and reasoning for task execution. Figure
8 (b) shows an overview of human-agent-object interactions
where embodied grasping is performed.
1) Datasets: Recently, a substantial number of grasping
datasets [159]–[163] have been generated. These datasets
typically contain annotated grasping data based on images
(RGB, depth), point clouds, or 3D scenes. With the advent of
MLMs and the application of foundational language models
to robotic grasping, there is an urgent need for datasets that
include linguistic text. Consequently, existing datasets have
been extended or reconstructed to create semantic-grasping
datasets [165]–[167]. These datasets are instrumental in study-
ing grasping models grounded in language, enabling agents to
develop a broad understanding of semantics.
Traditional grasping datasets encompass data for both single
objects [159] and cluttered scenes [164], providing stable
grasp annotations (4-DOF or 6-DOF) that conform to kine-
matics for each object. These data can be collected from real
desktop environments [159], typically including RGB, depth,
and point cloud data, or from virtual environments [162],
which include image data, point clouds, or scene models.
While these datasets are useful for grasping models, they lack
semantic information. To bridge this gap, these datasets have
been augmented or extended with semantic expressions [165],
[168], thereby linking language, vision, and grasping. By in-
corporating semantic information, agents can better understand
and execute grasping tasks. This enhancement allows for the


【图像 1】(97x97)
该图像为 **空白图像**（即无内容的纯黑或纯白背景），无法提取任何有效视觉信息。

---

### 1. 图像类型  
- **无内容图像**（可能是缺失、损坏或未加载的图像占位符）

---

### 2. 图像中的所有文字内容  
- **无文字内容**

---

### 3. 图像中的关键视觉元素和数据信息  
- **无可见元素**：无图形、图表、照片、图标、线条、颜色区域或其他视觉组件  
- **可能状态**：图像未正确渲染，或原PDF中该位置为空白

---

### 4. 图像传达的主要信息  
- **无法传达有效信息**：由于图像完全空白，无法支持对“Grounding & Grasp Pose Generation”相关内容的可视化理解。  
- **上下文推测**：根据页面文本，此图应为与“语言引导抓取”（Language-guided Grasping）或“人-智能体-物体交互”相关的示意图，但当前图像缺失。

---

### 总结  
该图像 **缺失或损坏**，未能提供任何可分析的内容。建议检查原始PDF文件是否完整，或确认图像是否在其他版本中存在。

【图像 2】(466x262)
1. **图像类型**：  
   这是一张**实物照片**，展示了一个机器人实验场景，用于研究语言引导的抓取（language-guided grasping）和人-机-物交互。

2. **图像中的所有文字内容**：  
   - 图像中可见的实物文字包括：
     - “SPAM”（罐头食品上的品牌标识）
     - “Baymax.”（白色机器人玩具上的名称，可能为背景提示或指令对象）

3. **图像中的关键视觉元素和数据信息**：  
   - **场景布置**：桌面上铺有几何图案的桌布，摆放了多种日常物品，包括：
     - 一串葡萄
     - 一个红色苹果
     - 一罐“SPAM”午餐肉
     - 一个蓝色塑料漏斗
     - 一把红色手柄的螺丝刀
     - 一个金属量杯
     - 一个木块
     - 一个透明塑料容器
     - 一个白色毛绒机器人（Baymax形象）
     - 一个带蓝色灯光的圆柱形设备（可能是机器人手臂或传感器）
   - **布局与关系**：物品分布杂乱但可识别，暗示这是一个用于测试机器人理解语言指令并执行抓取任务的实验环境。
   - **机器人/代理**：右侧的白色机器人（Baymax）可能作为交互代理或视觉参考点，用于空间推理任务。

4. **图像传达的主要信息**：  
   该图像展示了**语言引导抓取系统**的实际应用场景，强调机器人需要通过自然语言理解（如“Grasp the knife at its handle”或“Grasp the keyboard that is to the right of the brown kleenex box”）进行**物体定位（grounding）** 和 **抓取姿态生成**。图像中的多样化物体和复杂布局表明系统需具备**空间推理、逻辑推理和语义理解能力**，以完成人类指令下的精确操作。结合上下文，此图用于说明多模态交互中“从语言到动作”的完整流程。

【图像 3】(466x304)
1. **图像类型**：  
   这是一张实物照片，展示了一个桌面场景，包含多种日常物品。

2. **图像中的所有文字内容**：  
   图像本身没有直接的文字内容。但根据上下文（PDF第9页），该图是“Grounding & Grasp Pose Generation”部分的插图，与语言引导抓取任务相关。图中物体上无可见文字，但背景文本提到如“Grasp the keyboard that is to the right of the brown kleenex box”等指令。

3. **图像中的关键视觉元素和数据信息**：  
   - 桌面场景包含以下物品：
     - 一个黑色键盘（前景中心）
     - 一个白色键盘（后方）
     - 一盒蓝色纸巾（左侧）
     - 一个棕色小盒子（后方中央偏左）
     - 两个土豆（左下角）
     - 一个苹果、一个柠檬（右侧）
     - 一支绿色笔状物（在苹果旁）
     - 一袋面包或类似食品（右上角）
   - 物品排列有序，用于模拟真实环境中的多物体交互场景。
   - 所有物体均为常见家居物品，便于机器人进行目标识别与抓取任务。

4. **图像传达的主要信息**：  
   该图像用于说明“语言引导抓取”（Language-guided Grasping）任务中的**场景理解与对象定位**问题。通过自然语言指令（如“抓住棕色纸巾盒右边的键盘”），系统需实现：
   - **语义 grounding**：将语言描述与具体物体关联
   - **空间推理**：理解物体间的相对位置
   - **抓取姿态生成**：为指定物体生成合适的抓取动作

   此图作为典型场景示例，支持机器人在复杂环境中基于语言指令完成精确操作的研究。

【图像 4】(284x279)
1. **图像类型**：  
   这是一张**实物照片**，展示了一个包含多种日常物品的实验场景，用于机器人抓取任务的研究。

2. **图像中的所有文字内容**：  
   图像中可见的文字包括：
   - 牛奶盒上的中文标签：“**纯牛奶**”（部分可见）
   - 牛奶盒上方品牌标识：“**蒙牛**”（部分可见）
   - 其他物品上无明显文字

3. **图像中的关键视觉元素和数据信息**：
   - **背景环境**：绿色桌面，后方为金属网格结构（可能是机器人工作台或实验室设备）。
   - **物体列表**：
     - 一个红色杯子（位于右上角）
     - 一个白色牛奶盒（竖立在中央偏左）
     - 一个红色手柄的工具（类似螺丝刀或小撬棍，横放在牛奶盒前）
     - 一个紫色球体
     - 一个蓝色球体
     - 一个黄色球体
     - 一个黑色遥控器状物体（左下角）
     - 一个银色螺钉（位于红色杯子前方）
     - 一根细长金属杆（从上方伸入画面，可能为机械臂的一部分）
   - **空间布局**：物体分布较为分散，呈自然摆放状态，适合进行目标识别与抓取定位测试。

4. **图像传达的主要信息**：
   该图展示了机器人在真实物理环境中进行**语言引导抓取**（language-guided grasping）任务的典型场景。结合上下文“Grounding & Grasp Pose Generation”，此图用于说明机器人如何根据自然语言指令理解目标对象（如“grasp the knife at its handle”），并实现对特定物体的精确定位与抓取。图像中的多类物体和复杂布局体现了对**语义理解、空间推理与物体识别能力**的综合考验，是人机交互与智能机器人系统研究中的典型测试环境。

【图像 5】(488x264)
1. **图像类型**：  
   该图是一张**3D渲染图像**，结合了真实物体的视觉表现与计算机生成的几何标注（如彩色线条和坐标轴），用于展示机器人感知与抓取任务中的“接地”（grounding）与“抓取姿态生成”过程。

2. **图像中的所有文字内容**：  
   图像中**无直接文字内容**。但根据上下文（PDF第9页第5张图），其标题或说明来自页面文本：“Grounding & Grasp Pose Generation”，并分为两个子部分：(a) Language-guided Grasping 和 (b) Human-Agent-Object Interaction。此图为(a)部分的示例。

3. **图像中的关键视觉元素和数据信息**：
   - **场景布置**：一个桌面上摆放着多个日常物品，包括：
     - 一个白色毛绒玩具（类似Baymax）
     - 一个红色苹果
     - 一把蓝色手柄的刀具
     - 一个透明水壶
     - 一个黄色盒子（可能是纸巾盒）
     - 一个蓝色漏斗
     - 一串黑色葡萄
     - 一支黑色笔
     - 一个白色杯子
   - **点云/网格背景**：整个桌面覆盖着灰白色点云或网格状纹理，表示三维空间的深度感知或传感器数据（如RGB-D相机采集的数据）。
   - **彩色坐标轴结构**：在毛绒玩具上方有一个由红、绿、蓝三色线构成的三维坐标系，可能代表物体的位姿估计（6DoF pose）或机器人末端执行器的目标位置。
   - **虚线与箭头**：部分物体周围有虚线或箭头，暗示抓取方向或动作路径，例如指向刀具的手柄区域。

4. **图像传达的主要信息**：  
   该图展示了**语言引导下的机器人抓取任务**中，如何将自然语言指令（如“抓住刀的手柄”）与三维场景中的具体对象进行关联（即“接地”），并生成相应的抓取姿态。图像通过叠加坐标系和点云数据，直观地表现了机器人对环境的理解能力，包括物体识别、空间定位和动作规划。这体现了多模态融合（语言+视觉+空间推理）在智能机器人系统中的应用。

【图像 6】(277x262)
1. **图像类型**：  
   该图是一张**热力图（Heatmap）叠加在3D场景渲染图上的可视化图像**，属于**图形/示意图**类型，用于展示语言引导抓取任务中的目标物体识别与空间推理过程。

2. **图像中的所有文字内容**：  
   图像本身**没有包含任何文字内容**。文字信息来自页面上下文，包括：
   - “Grounding & Grasp Pose Generation”
   - “(a) Language-guided Grasping”
   - “Direct Object Specification”
   - “Inference Required”
   - “Baymax.”
   - “I am thirsty, can you give me something to drink?”
   - “Spatial Reasoning”
   - “Logical Reasoning”
   - “Grasp the keyboard that is to the right of the brown kleenex box.”
   - “Scene”
   - “Instruction”
   - “Agent with Arm”
   - “Grasping”
   - “Observation”
   - “Action”

3. **图像中的关键视觉元素和数据信息**：
   - **主体对象**：一个类似机器人或卡通人物（“Baymax”）的三维模型，其身体被覆盖一层**热力图颜色**（从蓝色到红色再到绿色），表示不同区域的关注度或置信度。
     - 热力图中**绿色和黄色区域集中在躯干中心**，表明这是系统关注的重点区域。
     - 肢体部分呈橙色至红色，可能代表次级注意力或动作执行部位。
   - **背景环境**：一个桌面场景，包含以下物品：
     - 一个苹果（位于右下方）
     - 一个键盘（左侧偏下，部分可见）
     - 一盒纸巾（可能是棕色Kleenex盒子，但未明确标注）
     - 其他模糊的物体，如线缆或工具
   - **颜色编码**：热力图使用**蓝→绿→黄→红**的颜色梯度，通常表示**激活强度或置信度**，其中绿色/黄色为高激活区，蓝色为低激活区。
   - **视角**：从后方略微俯视的角度拍摄，突出显示机器人背部及周围物体。

4. **图像传达的主要信息**：
   - 该图展示了**语言引导抓取（Language-guided Grasping）系统**如何通过自然语言指令理解场景并定位目标物体。
   - 热力图覆盖在机器人身上，表明系统正在对“Baymax”这一对象进行**语义接地（grounding）**，即识别出语言中提到的实体（如“Baymax”）在视觉场景中的对应位置。
   - 结合上下文中的指令“I am thirsty, can you give me something to drink?”，可以推断系统正尝试识别饮料类物体（如瓶装水、杯子等），而当前热力图聚焦于Baymax，说明系统可能正在进行**对象识别与推理过程**，将语言指令与视觉感知结合。
   - 整体上，图像强调了**多模态理解能力**：从语言输入出发，通过空间和逻辑推理，在复杂环境中定位目标并生成抓取姿态。

> **总结**：此图是语言驱动机器人交互系统中“语义接地”阶段的可视化结果，通过热力图显示系统对特定对象（Baymax）的关注程度，体现其在理解语言指令、进行空间推理和物体识别方面的能力。

【图像 7】(403x297)
1. **图像类型**：  
   这是一张实物照片，展示了一个桌面场景，用于演示机器人视觉与语言理解任务中的“对象接地”（grounding）和抓取姿态生成。

2. **图像中的所有文字内容**：  
   图像本身中没有直接可见的文字。但根据上下文（PDF第9页），该图属于“Grounding & Grasp Pose Generation”部分，其标题为“(a) Language-guided Grasping”，并关联以下指令示例：
   - “I am thirsty, can you give me something to drink?”
   - “Grasp the keyboard that is to the right of the brown kleenex box.”
   - “Grasp the knife at its handle.”

3. **图像中的关键视觉元素和数据信息**：  
   - **桌面场景**：白色桌面，背景为浅色柜体。
   - **物体列表**：
     - 一个黑色电脑键盘（位于前景中央）。
     - 一个蓝色包装的纸巾盒（左侧，带有“Kleenex”字样）。
     - 一个棕色木纹小盒子（后方偏左）。
     - 一个青绿色塑料键盘（后方中间）。
     - 一个透明胶带卷（右后方）。
     - 一个苹果（红色，位于中间偏右）。
     - 一个柠檬（黄色，位于苹果右侧）。
     - 两颗土豆（左侧靠近纸巾盒）。
     - 一支笔（白色带绿色帽，位于键盘上方）。
   - **空间布局**：物体分布较为分散，形成典型室内桌面环境，便于进行空间推理和对象识别。

4. **图像传达的主要信息**：  
   该图像用于说明“语言引导抓取”（language-guided grasping）任务，即机器人需根据自然语言指令，在复杂场景中定位目标对象（如“键盘”、“刀柄”等），并生成合适的抓取动作。图中物体的多样性和位置关系支持对空间推理、语义理解及对象接地能力的测试。例如，“键盘在棕色纸巾盒右侧”这一描述可被用于验证系统能否正确识别相对位置。

【图像 8】(385x384)
1. **图像类型**：  
   该图像是一个**热力图（Heatmap）**，属于可视化图形，用于表示某种数据分布或注意力强度。它通常用于显示模型在图像中关注的区域。

2. **图像中的所有文字内容**：  
   图像中**无可见文字内容**。原始文本上下文中的文字不属于该图像本身。

3. **图像中的关键视觉元素和数据信息**：  
   - 图像背景为深蓝色，代表低激活或关注度区域。  
   - 右上角有一个明亮的、以红色和黄色为主的高亮区域，呈心形或椭圆形，表示**高度关注或激活区域**。  
   - 左侧有模糊的浅蓝色轮廓，可能是一个物体的形状（如瓶子或容器），但细节不清晰，可能是被遮挡或未被重点关注。  
   - 热力图的色彩从蓝色（冷/低）到红色/黄色（热/高）过渡，表明存在一个显著的焦点区域。

4. **图像传达的主要信息**：  
   该热力图展示了在某个视觉任务（如抓取指令理解）中，系统对场景中特定区域的关注度分布。右上角的高亮区域表明模型**重点关注了某个对象（可能是饮料瓶或杯子）**，这与上下文中的语言指令“我渴了，能给我点喝的吗？”相呼应，说明模型通过语言引导实现了对目标物体的**语义定位（Grounding）**，并可能用于后续的抓取姿态生成。

【图像 9】(282x276)
1. **图像类型**：  
   该图是一张**实物照片**，展示了一个机器人操作场景的实验环境，包含多个日常物品和一个标注了抓取姿态的红色杯子。

2. **图像中的所有文字内容**：  
   图像中可见的文字为：  
   - “Aqua”（在白色管状物上）  
   - “SPF 30”（在白色管状物上）  
   - 其他文字因模糊或部分遮挡无法辨认。

3. **图像中的关键视觉元素和数据信息**：  
   - **场景布置**：绿色桌面上摆放着多种物体，包括：
     - 一支白色带蓝色花纹的“SPF 30”防晒霜管
     - 一个红色塑料杯（被绿色线条标记）
     - 一个黑色鼠标
     - 一个紫色球体
     - 一个蓝色球体
     - 一个黄色球体
     - 一根红色手柄的小工具（类似镊子或探针）
     - 一个小金属螺母或零件
     - 一个透明塑料盖
   - **视觉标注**：红色杯子上叠加了**绿色三维坐标轴**（X、Y、Z方向），表示机器人对目标物体的**抓取姿态估计**，即抓取点和方向。
   - **背景**：带有条纹的金属网格结构，可能是机器人工作台的一部分。

4. **图像传达的主要信息**：  
   该图展示了**语言引导下的机器人抓取任务**中的**物体定位与抓取姿态生成**过程。具体而言：
   - 机器人根据指令（如“Grasp the red cup”）识别目标物体（红色杯子）。
   - 通过视觉系统进行**物体接地（Grounding）**，即将语言描述映射到真实场景中的物体。
   - 利用空间推理生成**抓取姿态（Grasp Pose）**，由绿色坐标系表示，用于指导机械臂执行抓取动作。
   - 图像体现了“**Language-guided Grasping**”（语言引导抓取）的研究内容，属于多模态交互与机器人感知融合的典型应用。

> 注：此图对应论文中“Grounding & Grasp Pose Generation”部分，是实现人机交互与自然语言理解结合的关键步骤之一。

【图像 10】(246x244)
1. **图像类型**：  
   这是一张**照片**，展示了一个餐桌布置的俯视图。

2. **图像中的所有文字内容**：  
   图像中**没有文字内容**。

3. **图像中的关键视觉元素和数据信息**：  
   - 一张深色木质桌面。  
   - 一个白色的圆形餐盘位于画面中央。  
   - 餐盘左侧有两把银色叉子（一把靠近餐盘，另一把稍远）。  
   - 餐盘右侧有一把银色餐刀，刀刃朝向餐盘。  
   - 左上角部分可见另一个白色餐盘的一角。  
   - 整体布局符合标准西式餐桌礼仪（刀叉位置）。

4. **图像传达的主要信息**：  
   该图像用于**场景理解与物体定位**的上下文，可能作为“语言引导抓取”任务中的视觉输入示例。结合页面文本，此图可能是为了说明智能代理如何通过语言指令（如“抓住右边的刀”）进行**空间推理**和**物体接地（grounding）**，即识别并定位特定物体（如餐刀）在场景中的位置，以生成正确的抓取姿态。

【图像 11】(239x238)
1. **图像类型**：  
   这是一张实物照片，展示了一个餐桌场景的俯视图。

2. **图像中的所有文字内容**：  
   图像中无可见文字。

3. **图像中的关键视觉元素和数据信息**：  
   - 一张深色木质桌面。  
   - 中央放置一个白色圆形餐盘。  
   - 餐盘左侧有两把银色叉子（一把完整，一把部分可见）。  
   - 餐盘右侧有一把刀，刀柄为粉红色，刀身为银色。  
   - 左上角部分可见一个白色杯碟组合（仅边缘可见）。  
   - 所有餐具均整齐摆放，符合西式餐桌上常见的布局。

4. **图像传达的主要信息**：  
   该图像用于说明“语言引导抓取”（Language-guided Grasping）任务中的物体定位与操作场景，具体体现为：  
   - 场景中存在多个可抓取对象（如刀、叉、盘子），需通过自然语言指令进行精确识别与操作。  
   - 例如，指令“Grasp the knife at its handle”要求智能体理解并执行对特定物体（刀）及其部位（手柄）的抓取动作。  
   - 图像展示了真实世界环境中物体的空间关系，支持空间推理与目标接地（grounding）任务的研究。

【图像 12】(238x238)
1. **图像类型**：  
   这是一张实物照片（摄影图像），展示了一个餐桌布置场景，包含餐具和盘子。

2. **图像中的所有文字内容**：  
   图像中**没有可见的文字内容**。所有文本信息来自页面上下文，而非图像本身。

3. **图像中的关键视觉元素和数据信息**：  
   - 一张深色木质桌面。
   - 中央放置一个白色圆形餐盘。
   - 餐盘左侧有两把银色叉子（一把完整，一把部分可见）。
   - 餐盘右侧有一把刀，其刀柄被彩色网格图案覆盖（可能是用于标注或识别的视觉标记）。
   - 左上角可见另一只白色盘子的一部分。
   - 整体布局符合西式餐位摆放习惯，但刀具上的彩色网格暗示该图像可能用于计算机视觉或机器人抓取任务中的对象识别与定位。

4. **图像传达的主要信息**：  
   该图像用于说明“语言引导抓取”（Language-guided Grasping）任务中的**物体定位与抓取姿态生成**。结合上下文，图像展示了机器人如何通过理解自然语言指令（如“Grasp the knife at its handle”）来识别目标物体（刀），并确定合适的抓取位置（刀柄）。刀柄上的彩色网格可能是为了辅助视觉系统进行精确的物体识别和姿态估计。此图强调了在真实场景中实现**语义理解、空间推理与物理交互**的结合。

【图像 13】(200x200)
根据您提供的信息，该图像为PDF第9页中的第13张图像，其上下文来自《IEEE/ASME TRANSACTIONS ON MECHATRONICS》期刊论文中关于“Grounding & Grasp Pose Generation”（接地与抓取姿态生成）的章节。然而，当前图像内容显示为**纯黑色背景**，无可见视觉元素或文字。

以下是基于现有信息的分析：

---

### 1. 图像类型  
**无法确定**。由于图像完全为黑色，无法判断其具体类型（如图表、照片、截图、示意图等）。可能的原因包括：  
- 图像未正确加载；  
- PDF中图像缺失或损坏；  
- 图像被遮挡或渲染失败。

---

### 2. 图像中的所有文字内容  
**无可见文字**。图像为纯黑色，未显示任何文本内容。

---

### 3. 图像中的关键视觉元素和数据信息  
**无有效视觉元素**。图像中未呈现物体、图形、线条、颜色、标注或其他可识别的数据信息。

---

### 4. 图像传达的主要信息  
**无法提取**。由于图像内容为空白（全黑），无法从中获取任何信息。结合上下文标题“Grounding & Grasp Pose Generation”，推测原图应展示以下内容之一：  
- 语言引导抓取（Language-guided Grasping）流程示意图；  
- 人-智能体-物体交互场景（Human-Agent-Object Interaction）；  
- 抓取姿态生成过程中的视觉推理或动作输出示例。

但目前图像未能提供这些信息。

---

### 建议  
请检查原始PDF文件是否完整，确认该图像是否存在或是否因格式问题导致显示异常。若图像在其他设备或软件中正常显示，请重新上传以供进一步分析。

【图像 14】(200x200)
根据您提供的信息，当前图像内容为纯黑色背景，无可见视觉元素。以下是基于上下文和图像状态的详细分析：

---

### 1. 图像类型  
**无法确定**（可能为缺失或损坏的图像）  
图像显示为完全黑色，未呈现任何可识别的图表、照片、截图或图形内容。

---

### 2. 图像中的所有文字内容  
**无文字内容**  
图像中未包含任何可见文本。

---

### 3. 图像中的关键视觉元素和数据信息  
**无有效视觉元素**  
- 背景：纯黑色  
- 缺乏图形、图标、线条、颜色区分或其他视觉结构  
- 无数据点、标签、箭头、流程图等典型信息载体  

---

### 4. 图像传达的主要信息  
**无法传达有效信息**  
由于图像为空白或损坏，无法提取其意图表达的内容。结合上下文“Grounding & Grasp Pose Generation”及“Language-guided Grasping”等术语，推测该图应展示机器人在语言指令引导下的目标定位与抓取姿态生成过程（如场景理解、物体识别、抓取点预测等），但当前图像未能呈现这些内容。

---

### 补充说明  
该图像可能因PDF渲染问题、文件损坏或排版错误导致显示异常。建议检查原始PDF文件或联系文档提供方确认图像是否完整。

【图像 15】(200x200)
该图像为一个**图标（icon）**，并非图表、照片或截图，而是用于表示用户的抽象图形符号。

1. **图像类型**：  
   图标（用户头像类图形符号）

2. **图像中的所有文字内容**：  
   无文字内容。

3. **图像中的关键视觉元素和数据信息**：  
   - 由两个蓝色圆形组成：上方为头部，下方为身体。  
   - 右侧有一个较小的浅蓝色圆形，可能表示另一人或交互关系。  
   - 整体风格简洁，采用扁平化设计，颜色为不同深浅的蓝色。  
   - 通常代表“用户”、“人物”或“人类代理”（agent），在上下文中可能象征“人类-代理-物体交互”中的“人类”角色。

4. **图像传达的主要信息**：  
   该图标用于表示“人类”或“用户”实体，结合页面上下文（如“Human-Agent-Object Interaction”），其主要作用是**视觉化人类在人机交互系统中的角色**，特别是在语言引导抓取任务中作为指令发出者或交互参与者。

【图像 16】(344x249)
1. **图像类型**：  
   这是一张实物照片，展示了多个日常物品放置在木质桌面上的场景。

2. **图像中的所有文字内容**：  
   - 红色盒子上印有：“CHEEZ-IT” 和 “ARE THESE CHEESE CRACKERS READY?”  
   - 黄色瓶子标签上有部分可见文字：“MUSTARD”（芥末）  
   - 白色小盒上可见品牌名：“SALT”（盐）  
   - 橙色内壁的杯子上无明显文字  

3. **图像中的关键视觉元素和数据信息**：  
   - 一个红色的“CHEEZ-IT”饼干盒，位于画面中央偏左。  
   - 一个黄色的芥末瓶，位于右侧。  
   - 一个白色带橙色内壁的马克杯，位于中央前方。  
   - 一个白色的小盐盒，位于左侧。  
   - 所有物品均置于浅色木质桌面上，背景简洁，无其他干扰物。  
   - 物品之间存在空间关系：杯子在前，盐盒在左后方，饼干盒在后方，芥末瓶在右后方。

4. **图像传达的主要信息**：  
   该图展示了一个典型的厨房或餐桌场景，用于说明“语言引导抓取”（Language-guided Grasping）任务中对象识别与空间推理的上下文。结合页面文本，此图可能作为示例场景，用于演示智能代理如何根据自然语言指令（如“给我一杯饮料”）进行物体定位、语义理解与抓取动作生成。物品的摆放和种类为研究多模态感知与人机交互提供了具体环境。

【图像 17】(282x277)
1. **图像类型**：  
   这是一张**实物照片**，展示了机器人机械臂在真实环境中进行抓取任务的场景。

2. **图像中的所有文字内容**：  
   图像中可见的文字包括：  
   - “Colgate”（牙膏品牌标签）  
   - 部分模糊的包装文字，如“TOOTHPASTE”（牙膏）、“GUM”（口香糖）等  
   - 无其他明显可读文本

3. **图像中的关键视觉元素和数据信息**：  
   - 一个黑色的**机器人机械臂末端执行器**（夹爪）悬停于一堆日常物品上方，正准备或正在进行抓取动作。  
   - 物品堆叠在白色桌面上，包括：  
     - 一支红色“Colgate”牙膏  
     - 一管绿色牙膏  
     - 一个蓝色塑料瓶（可能是洗发水或沐浴露）  
     - 一个白色卷状物（可能是纸巾或胶带）  
     - 一把红色手柄的螺丝刀  
     - 一个黑色物体（可能是工具或收纳袋）  
     - 其他颜色鲜艳的小型日用品  
   - 背景为室内环境，有纸箱和文件，表明是实验或测试场景。  
   - 机械臂处于动态姿态，暗示正在进行**目标识别与抓取定位**操作。

4. **图像传达的主要信息**：  
   该图像展示了**语言引导下的机器人抓取系统**在复杂、杂乱的真实环境中对特定物体（如“Colgate牙膏”）进行**语义理解、目标定位与抓取姿态生成**的过程。结合上下文“Grounding & Grasp Pose Generation”，此图强调了机器人如何通过自然语言指令（如“Grasp the toothpaste”）实现从语言到物理动作的映射，体现**多模态感知与推理能力**。

【图像 18】(365x274)
1. **图像类型**：  
   这是一张实物照片，展示了桌面上多个日常物品的布局，属于实验场景的实景拍摄图。

2. **图像中的所有文字内容**：  
   图像中**没有可见的文字内容**。文字信息来自页面上下文，如“Grounding & Grasp Pose Generation”、“Language-guided Grasping”等，但这些文字不在图像本身内。

3. **图像中的关键视觉元素和数据信息**：  
   - **背景**：浅绿色格子桌布，桌面呈斜角视角，左侧有白色墙面。
   - **物体列表**（从左到右、从上到下）：
     - 一个橙色圆形盖子（可能是瓶盖）。
     - 一把折叠的黑色雨伞，带有花纹。
     - 一个倒置的白色纸杯，杯底朝上，下方有一个红色小物体（可能是杯垫或底座）。
     - 一个黑色椭圆形物体（可能是鼠标或遥控器）。
     - 一个小的银灰色电子设备（可能是蓝牙耳机盒或小型充电器）。
     - 一个白色圆柱形瓶子（可能为药瓶或喷雾瓶）。
     - 一个白色长方形扁平物体（可能是香皂、电池或卡片）。
   - **布局特征**：物体分散放置，无明显排列规律，模拟真实生活场景。
   - **光照与视角**：自然光照明，俯视角度略偏，能清晰看到物体形状与相对位置。

4. **图像传达的主要信息**：  
   该图像用于展示一个**机器人抓取任务的物理场景**，作为语言引导抓取（language-guided grasping）研究中的视觉输入。结合上下文，此图是“接地与抓取姿态生成”（Grounding & Grasp Pose Generation）环节的一部分，旨在让机器人根据自然语言指令（如“给我喝的东西”或“抓住键盘右侧的纸巾盒”）识别目标物体并规划抓取动作。图像提供了物体的空间分布信息，支持空间推理与对象识别任务。

【图像 19】(804x862)
1. **图像类型**：  
   该图是一张**组合图表**，包含**柱状图**和**折线图**，用于展示数据趋势与比较。

2. **图像中的所有文字内容**：  
   图像中**无文字内容**。仅包含图形元素（柱状图、折线图）和坐标轴网格线。

3. **图像中的关键视觉元素和数据信息**：  
   - **柱状图**：由7个蓝色垂直柱体组成，从左到右高度逐渐增加，表明数值呈上升趋势。最后一个柱子显著高于其他，显示出最大值。  
   - **折线图**：一条绿色折线连接多个点，整体趋势在前半段较为平缓，后半段急剧上升，尤其在最后两个点之间有明显增长。  
   - **坐标轴**：横轴未标注刻度或标签，纵轴有水平网格线，但未标注数值或单位。  
   - **背景**：黑色背景，白色网格线，增强对比度。

4. **图像传达的主要信息**：  
   该图通过柱状图和折线图的结合，展示了某种指标随类别或时间递增的趋势。柱状图强调各项目的绝对数值差异，而折线图突出变化趋势，尤其是在后期出现快速增长。整体呈现**持续增长**的态势，可能用于说明性能提升、效率改进或系统演化过程。由于缺乏标签，具体含义需结合上下文推断。


================================================================================
第 10 页
================================================================================

【文本内容】
IEEE/ASME TRANSACTIONS ON MECHATRONICS
10
TABLE VII
EMBODIED GRASPING DATASETS.
Dataset
Year
Type
Modality
Grasp Label
Gripper Finger
Objects
Grasps
Scenes
Language
Cornell [159]
2011
Real
RGB-D
Rect.
2
240
8K
Single
×
Jacquard [160]
2018
Sim
RGB-D
Rect.
2
11K
1.1M
Single
×
6-DOF GraspNet [161]
2019
Sim
3D
6D
2
206
7.07M
Single
×
ACRONYM [162]
2021
Sim
3D
6D
2
8872
17.7M
Multi
×
MultiGripperGrasp [163]
2024
Sim
3D
-
2-5
345
30.4M
Single
×
OCID-Grasp [164]
2021
Real
RGB-D
Rect.
2
89
75K
Multi
×
OCID-VLG [165]
2023
Real
RGB-D,3D
Rect.
2
89
75K
Multi
√
ReasoingGrasp [166]
2024
Real
RGB-D
6D
2
64
99.3M
Multi
√
CapGrasp [167]
2024
Sim
3D
-
5
1.8K
50K
Single
√
development of more sophisticated and semantically aware
grasping models, facilitating more intuitive and effective inter-
action with the environment. Table VII presents the datasets
described above, including traditional grasping datasets and
language-based grasping datasets.
2) Language-guided grasping: The concept of language-
guided grasping [165], [166], [168], which has evolved from
this integration, combines MLMs to provide agents with
the capability of semantic scene reasoning. This allows the
agent to execute grasping operations based on implicit or
explicit human instructions. Figure 8 (c) illustrates the pub-
lication trends in recent years on the topic of language-guided
grasping. With the advancement of LLMs, researchers have
shown increasing interest in this topic. Currently, grasping
research is increasingly focused on open-world scenarios,
emphasizing the open-set generalization [169] methods. By
leveraging the generalization capabilities of MLMs, robots
can perform grasping tasks in open-world environments with
greater intelligence and efficiency.
In language-guided grasping, semantics can originate from
explicit instructions [169], [170] and implicit instructions
[166], [167]. Explicit instructions clearly specify the category
of the object to be grasped, such as a banana or an apple.
Implicit instructions, however, require reasoning to identify
the object or a part of the object to be grasped, involving
spatial reasoning and logical reasoning.
Spatial reasoning [165] refers to instructions that may
include the spatial relationship of the object or part to be
grasped, necessitating the inference of grasping posture based
on the spatial relationships of objects within the scene. For
example, “Grasp the keyboard that is to the right of the
brown kleenex box” involves understanding and inferring the
spatial arrangement of objects. Logical reasoning [166], on
the other hand, involves instructions that may contain logical
relationships requiring inference to discern human intent and
subsequently grasp the target. For instance, “I am thirsty, can
you give me something to drink?” would prompt the agent to
potentially hand over a glass of water or a bottle of a beverage.
The agent must ensure that the liquid does not spill during the
handover, thus generating a reasonable grasping posture.
In both cases, the integration of semantic understanding with
spatial and logical reasoning enables the agent to perform
complex grasping tasks effectively and accurately. Figure 8
(a) depicts various types of language-guided grasping tasks.
3) End-to-End Approaches: CLIPORT [168] is a language-
conditioned imitation learning agent that combines the vision-
language pre-trained model CLIP with the Transporter Net
to create an end-to-end dual-stream architecture for semantic
understanding and grasp generation. It is trained using a large
number of expert demonstration data collected from virtual en-
vironments, enabling the agent to perform semantically guided
grasping. Based on the OCID dataset, CROG [165] proposes a
vision-language-grasping dataset and introduces a competitive
end-to-end baseline. It leverages CLIP’s visual foundation
capabilities to learn grasp synthesis directly from image-text
pairs. Reasoning Grasping [166] introduces the first reasoning
grasping benchmark dataset based on the GraspNet-1 Billion
dataset and proposes an end-to-end reasoning grasping model.
The model integrates multimodal LLMs with vision-based
robotic grasping frameworks to generate grasps based on se-
mantics and vision. SemGrasp [167] is a method for semantic-
based grasp generation that incorporates semantic information
into grasp representations to generate dexterous hand grasp
postures. It introduces a discrete representation aligning grasp
space with semantic space, enabling the generation of grasp
postures according to language instructions.
4) Modular Approaches: F3RM [169] seeks to elevate
CLIP’s text-image priors into 3D space, using extracted fea-
tures for language localization followed by grasp generation. It
combines precise 3D geometry with rich semantics from 2D
foundational models, utilizing features extracted from CLIP
to specify objects for manipulation through free-text natural
language. It demonstrates the ability to generalize to unseen
expressions and new object categories. GaussianGrasper [170]
utilizes a 3D Gaussian field to achieve language-guided grasp-
ing tasks. The proposed methodology begins with the construc-
tion of a 3D Gaussian field, followed by feature distillation.
Subsequently, language-based localization is performed using
the extracted features. Finally, grasp pose generation is carried
out based on a SOTA pre-trained grasping network [171]. It
integrates open-vocabulary semantics with precise geometry,
enabling grasping based on language instructions.
These approaches advance language-guided grasping by
using end-to-end and modular frameworks, enhancing robotic
agents’ ability to perform complex grasping tasks from natural
language instructions. Embodied grasping improves robots’
intelligence and utility in-home services and industrial manu-
facturing. However, current methods face limitations, includ-
ing reliance on extensive data and poor generalization. Future
research aims to enhance agent generality, enabling robots
to understand complex semantics, grasp a variety of unseen
objects, and tackle intricate tasks.
VI. EMBODIED AGENT
An agent is defined as an autonomous entity capable of per-
ceiving its environment and acting to achieve specific objec-
tives. Recent advancements in MLMs have further expanded



================================================================================
第 11 页
================================================================================

【文本内容】
IEEE/ASME TRANSACTIONS ON MECHATRONICS
11
Visual Description
High-Level Task Planning
Low-Level Action Planning
(a)
(b)
(c)
(d)
sofa, desk, drawer, 
pillow, …
chair, desk, drawer, sink, 
fridge, …
desk, drawer, potted 
plant, trash can, 
computer, bed, …
There are two sofas in 
front and a door in the left 
corner, …
There is a laptop and two 
monitors on the desktop. 
There are no items placed 
on the desktop to the …
LLM
VLM
Object List
Image Caption
Scene Graph
…
bed
desk
agent
door
…
Visual Token
LLM/VLM
VLA Model
Prompt & Related Case
Text Token & 
Visual Token
Sub-Task
& Visual Description
Visual Representation
Better Performance
Replan
Replan
Step 1. go to the kitchen
Step 2. find the apple
Step 3. pick up the apple
…
Step 1. go to the kitchen
Step 2. find the fridge
Step 3. open the fridge 
…
Replan
Embodiment
Grounding, VQA, 
Grasping, Navigation, …
Embodiment
Action: Navigation
Location: [1.20, 3.10]
Sub-Task
Action: Pick up
Location: [2.45, 2.65]
Target: Apple
Action: Grounding
Target: Apple
Location: [2.45, 2.65]
Action
Take an apple to 
my room
Action Planning
Yes, madam
Task Planning
Sub-task
Visual Context
Perception
Exploration
Grasping
Fig. 9. The architecture of the embodied agent based on embodied multimodal foundation model, which consists of visual perception module, high-level task
planning module, and low-level action planning module.
the application of agents to practical scenarios. When these
MLM-based agents are embodied in physical entities, they
can effectively transfer their capabilities from virtual space
to physical world, thereby becoming embodied agents [172].
To enable embodied agents to operate in the information-
rich and complex real world, the embodied agents have been
developed to show strong multimodal perception, interaction
and planning capabilities, as shown in Fig. 9. To complete
a task, embodied agents typically involves the following
process: 1) decomposing the abstract and complex task into
specific subtasks, which is referred to as high-level Embod-
ied Task Planning. 2) gradually implementing these subtasks
by effectively utilizing Embodied Perception and Embodied
Interaction models or leveraging the Foundation Model’s pol-
icy function, named low-level Embodied Action Planning. It
is worth noting that task planning involves thinking before
acting, and is therefore typically considered in cyber space. In
contrast, action planning must account for effective interaction
with the environment and feedback on this information to the
task planner to adjust task planning. Thus, it is crucial for
embodied agents to align and generalize their abilities from
the cyber space to the physical world.
A. Embodied Task Planning
Traditional embodied task planning methods are usually
based on explicit rules and logical reasoning. For example,
symbolic planning algorithms such as PDDL [173], and search
algorithms like MCTS [174] and A* [175], are used to
generate plans. However, these methods often rely on prede-
fined rules, constraints, and heuristics that are rigid and may
not adapt well to dynamic or unforeseen changes. With the
popularity of LLMs, many works have attempted to use LLMs
for planning or to combine traditional methods with LLMs,
leveraging the rich embedded world knowledge for reasoning
and planning without the need for handcrafted definitions,
greatly enhancing the model’s generalization capabilities.
1) Planning utilizing the Emergent Capabilities of LLMs:
Before the scale-up of natural language models, task planners
were similarly implemented by training models like BERT
on embodied instruction datasets such as Alfred [176] and
Alfworld [177], as demonstrated by FILM [178]. However,
this approach was limited by the examples in the training
set and could not effectively align with the physical world.
Nowadays, thanks to the emergent capabilities of LLMs,
they can decompose abstract tasks using their internal world
knowledge and chain-of-thought reasoning, similar to how
humans reason through task completion steps before acting.
For example, Translated LM [179] and Inner Monologue [180]
can break down complex tasks into manageable steps and
devise solutions using their internal logic and knowledge
systems without additional training. Similarly, the multi-agent
collaboration framework ReAd [181] efficiently self-refined
plans via different prompts. Additionally, some approaches
abstract past successful examples into a series of skills stored
in a memory bank to consider during inference and improve
planning success rates [182]–[184]. Some works utilized code
as the reasoning medium instead of natural language, where
task planning is generated as code based on the available API
library [185], [186]. Furthermore, multi-turn reasoning can
effectively correct potential hallucinations in task planning.
For instance, Socratic Models [187] and Socratic Planner [188]
used Socratic questioning to derive reliable planning.
However, during task planning, potential failures may occur
during execution, often resulting from the planner not fully
accounting for the complexity of the real environment and
the difficulty of task execution [180], [189]. Due to a lack
of visual information, planned subtasks may deviate from the
actual scenario, leading to task failure. Therefore, integrating
visual information into planning or replanning during execu-
tion is necessary. This approach can significantly enhance the
accuracy and feasibility of task planning, better addressing the
challenges of real-world environments.
2) Planning utilizing the visual information from embod-
ied perception model: Based on the above discussion, it is
important to integrate visual information into task planning
(or replanning). In this process, object labels, locations, or de-
scriptions provided by visual input can offer critical references
for task decomposition and execution by LLMs. Through
visual information, LLMs can more accurately identify target
objects and obstacles in the current environment, thereby
optimizing task steps or modifying subtask objectives. Some
works use an object detector to query the objects present in the


【图像 1】(1287x623)
### 1. 图像类型  
**图表（流程图/系统架构图）**  
该图像为一个结构化的技术流程图，展示了一个基于视觉和语言的机器人任务规划与控制系统的整体架构。

---

### 2. 图像中的所有文字内容  

#### 主要模块标题：
- **High-level Planning**（高层规划）
- **Low-level Controller**（低层控制器）
- **Scene Representation**（场景表示）
- **Observed Scene**（观测场景）

#### 模块内文字：
- **User Instruction**:  
  “I need to find the box and open it.”  
  “Step 1: Try to find the box”  
  “Step 3: Pick up the sequence”  
  “Current action: Open the box”

- **LLM**（Large Language Model）：  
  - 输入：Text feature, Semantic Embedding  
  - 输出：Planning actions

- **VLM**（Vision-Language Model）：  
  - 输入：Text token, Visual token  
  - 输出：Executable actions（如：`[3.52, 5.52]`, `Action: Navigation`；`[4.52, 2.52]`, `Action: Open`）

- **Frontier Description**：  
  - C: [3.73, 3.33]  
  - B: [3.25, 4.45]  
  - A: [3.81, 3.02]  
  - Textual Feature → Text Token  
  - Visual Feature → Visual Token  
  - Semantic Embedding

- **Semantic Feature Map**：  
  - 包含多个颜色编码的特征柱状图（绿色、蓝色、橙色等），代表不同语义区域。

- **Frontiers Mask**：  
  - 标注了A、B、C三个位置点，对应于房间中的探索前沿。

- **Observed Scene**：  
  - 包含三张室内环境图片（俯视图、正面图、走廊图）  
  - 路径标注：从起点到目标点 `[2.5, 4]` 的黄色路径  
  - 状态更新：“update”箭头指向地图

- **Exploration**：  
  - Agent图标 + 文本：“I need to go back to the kitchen.”

- **Agent**：  
  - 机器人形象，带有“exploration”标签

- **Other Labels**：
  - “Texture feature”
  - “Semantic Embedding”
  - “Visual Token”
  - “Text Token”
  - “Feature Fusion”
  - “Pixel Features”
  - “Fusion”（合并）
  - “Scene Representation”

---

### 3. 图像中的关键视觉元素和数据信息  

| 元素 | 描述 |
|------|------|
| **用户指令输入** | 用户提出自然语言任务：“I need to find the box and open it.” |
| **高层规划模块（LLM）** | 接收文本特征和语义嵌入，生成分步动作计划（如Step 1, Step 3）。 |
| **低层控制器（VLM）** | 结合文本和视觉token，输出可执行动作（导航、打开等），并结合空间坐标。 |
| **观测场景** | 展示真实或模拟环境的多视角图像（俯视、正面、走廊），包含机器人当前位置和路径。 |
| **像素特征提取** | 从三张图像中提取像素级特征，形成三维立方体结构。 |
| **语义特征融合** | 将像素特征融合为语义特征图（Semantic Feature Map），用不同颜色区分语义区域。 |
| **前沿描述（Frontier Description）** | 提供潜在探索点（A、B、C）的坐标和语义信息。 |
| **前沿掩码（Frontiers Mask）** | 在俯视图中标出探索前沿点A、B、C的位置。 |
| **动作执行** | 显示两个可执行动作：导航至 `[3.52, 5.52]` 和打开 `[4.52, 2.52]` 处的盒子。 |

---

### 4. 图像传达的主要信息  

该图展示了**一个端到端的视觉-语言机器人任务执行系统架构**，其核心是通过**大语言模型（LLM）进行高层任务规划**，并通过**视觉-语言模型（VLM）实现低层控制与动作执行**。

具体流程如下：

1. **用户输入自然语言指令**，系统解析为高层任务步骤。
2. **LLM生成规划序列**（如“寻找盒子”、“打开盒子”）。
3. **感知模块从观测场景中提取视觉和语义信息**，包括像素特征和语义嵌入。
4. **VLM结合文本和视觉信息**，生成精确的空间坐标和可执行动作（如导航、交互）。
5. **系统在环境中执行动作**，并根据反馈更新状态（如“需要返回厨房”）。

该架构实现了从**自然语言指令到具体机器人行为的闭环控制**，强调了**语义理解、空间推理和多模态融合**的重要性，适用于复杂室内环境下的自主机器人任务。

【图像 2】(300x137)
1. **图像类型**：  
   这是一张**三维渲染的室内场景图**，属于计算机生成的虚拟环境图像（可能来自仿真或游戏引擎），用于展示一个房间的布局和物体配置。

2. **图像中的所有文字内容**：  
   图像本身**无直接文字内容**。相关文字信息来自页面上下文，包括：
   - “sofa, desk, drawer, pillow, …”
   - “chair, desk, drawer, sink, fridge, …”
   - “desk, drawer, potted plant, trash can, computer, bed, …”
   - “There are two sofas in front and a door in the left corner, …”
   - “There is a laptop and two monitors on the desktop. There are no items placed on the desktop to the …”

3. **图像中的关键视觉元素和数据信息**：  
   - **房间布局**：呈现一个室内空间，视角为俯视偏斜角，显示房间的多个角落。
   - **家具与物体**：
     - 一张黑色沙发位于画面中央偏上位置。
     - 一张书桌（desk）在左侧墙边，上面有显示器和键盘等设备。
     - 一台电脑主机或服务器架设在右侧墙边。
     - 一扇门位于左下角区域。
     - 墙上挂有一幅画或屏幕。
     - 顶部可见天花板结构及灯光装置。
   - **空间关系**：展示了物体之间的相对位置，如沙发面对书桌、门在角落等。
   - **光照与材质**：整体光照均匀，表面材质简洁，符合仿真环境风格。

4. **图像传达的主要信息**：  
   该图像用于说明一个**可被视觉模型（VLM）或语言模型（LLM）理解的室内场景**，结合上下文中的“Object List”、“Image Caption”、“Scene Graph”等术语，表明此图是用于研究**视觉-语言任务规划系统**（如机器人导航、场景理解）的示例。其主要作用是提供一个**结构清晰、语义丰富的虚拟环境**，以支持高阶任务规划（High-Level Task Planning）与低阶动作规划（Low-Level Action Planning）的研究。

【图像 3】(240x276)
1. **图像类型**：  
   这是一张**俯视图的室内布局示意图**（平面图），属于**图形/示意图**类型，用于表示房间内家具和物体的空间分布。

2. **图像中的所有文字内容**：  
   图像本身**没有直接包含文字**。但根据上下文（PDF第11页第3张图），该图与以下文本相关联：
   - “sofa, desk, drawer, pillow, …”
   - “chair, desk, drawer, sink, fridge, …”
   - “desk, drawer, potted plant, trash can, computer, bed, …”
   - “There are two sofas in front and a door in the left corner, …”
   - “There is a laptop and two monitors on the desktop.”

3. **图像中的关键视觉元素和数据信息**：
   - **房间布局**：呈现一个矩形房间的俯视图，地面为浅棕色。
   - **主要家具**：
     - 中央有一张**圆形桌子**，周围有**6把椅子**。
     - 一张**长方形沙发**位于房间中部偏下位置。
     - 两个**单人沙发**或**扶手椅**分别位于左下角和右下角。
     - 一个**书桌**或**工作台**位于房间右侧墙边，上面有显示器等设备。
     - 一个**床**或**类似平台**位于底部中央，可能是“bed”或“desk”的误判。
     - 一个**小型柜子或抽屉**在右上角。
     - 一个**门**位于左侧墙边（左上角）。
     - 一个**圆形摄像头或传感器**位于右上角天花板处。
   - **其他细节**：
     - 房间内有多个**矩形区域**可能代表电器、储物柜或电子设备。
     - 有一个**灰色方块**（可能是机器人或代理“agent”）位于房间中央下方，靠近床或平台。
     - 一条**竖直线条**在右侧墙边，可能是电源线或装饰条。

4. **图像传达的主要信息**：
   该图展示了一个**典型室内环境的布局**，用于支持**视觉语言模型（VLM）或大语言模型（LLM）在场景理解、任务规划和空间推理方面的研究**。结合上下文，图像与**物体识别、场景图生成、图像描述（Image Caption）以及低层动作规划（Low-Level Action Planning）** 相关。其核心目的是提供一个**结构化、可解析的视觉环境**，以测试模型对空间关系、物体类别和语义理解的能力。例如：
   - 模型需识别“two sofas in front”、“door in the left corner”等空间描述。
   - 支持生成如“there is a laptop and two monitors on the desktop”这样的图像描述。
   - 用于构建**场景图（Scene Graph）**，连接物体及其关系（如“laptop-on-desk”）。

总结：这是一张用于**智能系统环境感知与任务规划研究的虚拟室内场景图**，强调**空间布局、物体识别和语义理解**。

【图像 4】(141x175)
1. **图像类型**：  
   该图是一张**三维渲染的室内场景图像**（3D-rendered indoor scene），属于计算机视觉或机器人环境建模中的典型可视化示例，可能用于任务规划、场景理解或人机交互研究。

2. **图像中的所有文字内容**：  
   图像本身**无直接文字内容**。但根据上下文（PDF第11页第4张图）和页面文本，该图对应于“Visual Description”部分，与以下信息相关：
   - 图像标签：(a), (b), (c), (d) —— 此图为(a)
   - 文本描述：“There are two sofas in front and a door in the left corner, …” → 表明图像中包含沙发和门
   - 其他上下文提到的物体：sofa, desk, drawer, pillow, chair, sink, fridge, laptop, monitors, potted plant, trash can, computer, bed

3. **图像中的关键视觉元素和数据信息**：  
   - **房间布局**：一个室内空间，墙壁为浅棕色方格瓷砖，地面为深色木质地板。
   - **家具与物体**：
     - 中央有一张**桌子**（desk），桌上有**笔记本电脑**（laptop）、**显示器**（monitor）、**文件**、**杯子**等物品。
     - 桌子前方有**两把椅子**（chairs），呈对称排列。
     - 左侧可见一扇**门**（door），位于角落。
     - 墙上有一个**壁挂式设备**（可能是电视或控制面板）。
     - 右侧墙边可能有**储物柜**或**抽屉**（drawer）结构。
   - **视角**：从房间一侧斜向观察，具有一定的深度感，模拟人类或机器人视角。
   - **光照**：均匀照明，无明显阴影，符合虚拟仿真环境特征。

4. **图像传达的主要信息**：  
   该图像展示了一个典型的**家庭或办公室室内环境**，用于支持**视觉语言模型**（VLM）和**大语言模型**（LLM）在**场景理解**和**任务规划**中的应用。结合上下文，其主要作用是：
   - 提供**视觉输入**以生成**物体列表**（Object List）、**图像描述**（Image Caption）和**场景图**（Scene Graph）。
   - 支持**高层任务规划**（High-Level Task Planning）与**低层动作规划**（Low-Level Action Planning）的衔接。
   - 验证系统能否正确识别并理解复杂室内场景中的对象及其空间关系。

> 总结：这是一幅用于**多模态AI系统测试**的3D渲染室内场景图，旨在评估模型对环境的感知、描述和推理能力。

【图像 5】(300x137)
1. **图像类型**：  
   这是一张**三维室内场景的渲染图**（3D rendering），呈现了一个房间的俯视视角，属于计算机生成的虚拟环境图像，可能用于机器人导航、场景理解或视觉语言模型的研究。

2. **图像中的所有文字内容**：  
   图像中**没有直接可见的文字内容**。文字信息来自页面上下文描述，包括：
   - 物体列表（Object List）：sofa, desk, drawer, pillow, chair, sink, fridge, potted plant, trash can, computer, bed, laptop, monitors
   - 场景描述（Image Caption）：如“There are two sofas in front and a door in the left corner…”、“There is a laptop and two monitors on the desktop.”
   - 其他标签：LLM、VLM、Scene Graph、Visual Description 等

3. **图像中的关键视觉元素和数据信息**：  
   - **房间布局**：俯视视角展示了一个矩形房间，墙壁为白色，地面为浅色。
   - **家具与物体**：
     - 两个橙色沙发（位于画面前方）
     - 一张深色书桌（位于中间偏右）
     - 一把黑色椅子（靠近书桌）
     - 一个蓝色门（位于左侧角落）
     - 一扇窗户（位于房间后方中央）
     - 一个小型垃圾桶（在书桌旁）
     - 桌上放置了电脑设备（推测为笔记本和显示器）
     - 墙边有植物盆栽
   - **空间结构**：房间呈L型布局，右侧有一块蓝色区域（可能是门或墙的一部分），整体设计简洁现代。
   - **光照**：自然光从窗户进入，室内光线均匀。

4. **图像传达的主要信息**：  
   该图像用于展示一个**典型的室内家居环境**，作为视觉语言模型（VLM）或大语言模型（LLM）进行**场景理解、对象识别和任务规划**的输入示例。结合上下文，图像配合文本描述，旨在说明如何通过视觉信息提取物体列表、生成图像描述、构建场景图（Scene Graph），并支持高层任务规划（High-Level Task Planning）与低层动作规划（Low-Level Action Planning）。其核心用途是研究多模态系统如何理解复杂室内环境。

【图像 6】(178x251)
1. **图像类型**：  
   该图是一张**俯视视角的室内场景示意图**（或称为“房间布局图”），属于**图形化示意图**，用于表示一个房间内的物体布局和空间关系。风格类似简化的平面设计图或机器人感知任务中的环境地图。

2. **图像中的所有文字内容**：  
   图像本身**无直接可见的文字内容**。但根据上下文（PDF第11页第6张图）及周围文本描述，可推断其与以下标签相关：
   - Object List（物体列表）：bed, desk, drawer, chair, pillow, sink, fridge, potted plant, trash can, computer, laptop, monitors, door
   - Image Caption（图像描述）：如 “There are two sofas in front and a door in the left corner…”、“There is a laptop and two monitors on the desktop.”
   - Scene Graph（场景图）：可能用于表示物体之间的空间和语义关系。
   - LLM / VLM：指语言模型和视觉语言模型在处理此类图像时的应用。

3. **图像中的关键视觉元素和数据信息**：  
   - **床（Bed）**：位于左上角，呈矩形，覆盖蓝色被子，有枕头。
   - **书桌（Desk）**：位于右下角，橙色长条形区域，上面放置多个小物件：
     - 一台笔记本电脑（laptop）
     - 两个显示器（monitors）
     - 鼠标、键盘、鼠标垫等
     - 其他小型设备（可能是打印机、音箱等）
   - **抽屉（Drawer）**：可能集成在书桌下方或侧面，未明确标注。
   - **门（Door）**：位于左侧边缘，呈垂直矩形，颜色较深。
   - **其他物品**：
     - 垃圾桶（trash can）：位于书桌附近。
     - 盆栽（potted plant）：在书桌上方角落。
     - 椅子（chair）：未明显显示，但可能隐含在书桌旁。
     - 桌面整洁，无杂物堆放，符合“no items placed on the desktop”的描述（部分区域）。
   - **整体布局**：房间呈矩形，地面为浅棕色，墙壁为绿色边框，具有典型的室内环境建模特征。

4. **图像传达的主要信息**：  
   该图旨在展示一个**典型室内场景的视觉结构**，用于支持**视觉语言模型（VLM）和大型语言模型（LLM）在多模态任务中的理解与推理**。主要用途包括：
   - **物体识别与定位**：通过图像识别房间中的各类家具和电子设备。
   - **空间关系建模**：例如“床在左上角，书桌在右下角”，“门在左侧”。
   - **场景描述生成**：结合图像与文本，生成自然语言描述（Image Caption）。
   - **高阶任务规划（High-Level Task Planning）与低阶动作规划（Low-Level Action Planning）**：作为机器人导航、家庭自动化或人机交互系统的基础输入。

总结：  
这是一幅用于**多模态AI研究**的简化室内场景图，重点在于**物体布局、空间语义和视觉-语言对齐**，服务于视觉理解、场景建模和智能体决策等任务。

【图像 7】(135x169)
1. **图像类型**：  
   该图像是一个**室内场景的俯视示意图**（或简化二维布局图），属于**图形化表示**，用于展示房间内物体的相对位置和布局。它可能是从机器人或智能系统视角生成的环境地图。

2. **图像中的所有文字内容**：  
   图像本身**无直接文字内容**。但根据上下文（PDF第11页，第7张图）和相邻文本，可推断其关联信息包括：
   - 物体类别：sofa, desk, drawer, pillow, chair, sink, fridge, potted plant, trash can, computer, bed, door, laptop, monitor
   - 视觉模型相关术语：LLM（大语言模型）、VLM（视觉-语言模型）、Object List、Image Caption、Scene Graph
   - 描述性语句：“There are two sofas in front and a door in the left corner…”、“There is a laptop and two monitors on the desktop…”

3. **图像中的关键视觉元素和数据信息**：  
   - **整体布局**：俯视视角下的矩形房间，墙壁呈深色边框。
   - **主要物体**：
     - 左上角有一个橙色方块，可能代表**沙发**（sofa）。
     - 右上角有一个小灰色方块，可能为**门**（door）。
     - 左下角有一块带有彩色图案的区域，类似**地毯**或**装饰画**，可能对应“painted wall”或“rug”。
     - 中央偏左有一个浅色矩形区域，可能是**书桌**（desk）或**床**（bed）。
     - 房间中央有多个小点状或方形物体，可能代表**抽屉**（drawer）、**垃圾桶**（trash can）、**植物**（potted plant）等。
   - **颜色与形状**：使用不同颜色和几何形状区分物体，如橙色表示沙发，灰色表示门，浅色矩形表示家具。
   - **空间关系**：展示了物体之间的相对位置，例如门在角落，沙发在前方，书桌位于中央。

4. **图像传达的主要信息**：  
   该图像用于**可视化室内场景的结构和物体分布**，支持**视觉-语言模型（VLM）** 或 **机器人感知系统** 的任务规划。结合上下文，其目的是展示如何通过视觉输入生成：
   - **对象列表**（Object List）
   - **图像描述**（Image Caption）
   - **场景图**（Scene Graph）
   - 并进一步用于**高层任务规划**（High-Level Task Planning）和**低层动作规划**（Low-Level Action Planning）

   因此，该图是**多模态理解与环境建模**过程中的关键输入，帮助系统理解空间布局并进行智能决策。

【图像 8】(298x137)
1. **图像类型**：  
   该图像为一张**3D渲染的室内场景照片**，属于计算机生成的虚拟环境视图，可能用于机器人导航、视觉理解或场景建模研究。

2. **图像中的所有文字内容**：  
   图像中**无直接可见的文字内容**。文字信息来自页面上下文描述，如“sofa, desk, drawer, pillow, …”、“There are two sofas in front and a door in the left corner, …”等，但这些文字并未出现在图像本身中。

3. **图像中的关键视觉元素和数据信息**：  
   - **空间布局**：展示了一个室内厨房与客厅相连的场景，视角位于房间内部，朝向厨房区域。
   - **主要物体**：
     - 左侧有白色橱柜和台面，上方有悬挂式储物架。
     - 中央区域可见冰箱（右侧深色大型电器）、水槽、炉灶及微波炉等厨房设施。
     - 远处可见一个带有灯光的门廊或通道，通向另一个房间。
     - 天花板上安装了灯具和通风设备。
   - **颜色与材质**：以浅色为主（米白、浅灰），冰箱为深灰色，整体呈现现代简约风格。
   - **光照**：人工光源照明，灯光集中在厨房操作区，营造出清晰的视觉层次。

4. **图像传达的主要信息**：  
   该图像用于展示一个**典型家庭室内环境的三维视觉表示**，结合上下文（如“Visual Description”、“Object List”、“Scene Graph”）可知，其目的是为**视觉语言模型（VLM）或大语言模型（LLM）提供视觉输入**，以支持任务规划、场景理解、物体识别和语义推理。图像作为训练或测试数据的一部分，帮助系统理解空间结构、物体位置及其语义关系。

【图像 9】(86x128)
1. **图像类型**：  
   该图像是一个**俯视视角的室内场景布局图**，属于**示意图或平面图**，用于表示房间内的物体分布和空间结构。它并非真实照片，而是简化后的图形化表示。

2. **图像中的所有文字内容**：  
   图像本身**没有直接包含文字内容**。其上下文信息来自页面文本，包括：
   - “sofa, desk, drawer, pillow, …”
   - “chair, desk, drawer, sink, fridge, …”
   - “desk, drawer, potted plant, trash can, computer, bed, …”
   - “There are two sofas in front and a door in the left corner, …”
   - “There is a laptop and two monitors on the desktop.”
   - “LLM”、“VLM”、“Object List”、“Image Caption”、“Scene Graph”

3. **图像中的关键视觉元素和数据信息**：  
   - **整体布局**：俯视图显示了一个矩形房间，内部包含多个家具和物品。
   - **主要物体**：
     - **床（bed）**：位于左下角，呈绿色矩形。
     - **浴缸（bathtub）**：位于顶部中央，白色长方形，内有紫色花朵图案。
     - **地毯（rug）**：位于右下角，红色带花纹的矩形区域。
     - **桌子/书桌（desk）**：在床右侧，浅色矩形。
     - **椅子（chair）**：在书桌旁，较小的深色形状。
     - **抽屉（drawer）**：可能为书桌的一部分。
     - **门（door）**：位于左侧边缘，垂直窄条状。
     - **其他物品**：如盆栽、垃圾桶等未明确标注但可推断存在。
   - **空间关系**：展示了各物体之间的相对位置，例如床靠近墙，书桌在中间偏右，浴缸靠后墙。

4. **图像传达的主要信息**：  
   该图旨在展示一个典型室内环境的**物体布局与空间配置**，用于支持视觉语言模型（VLM）或大语言模型（LLM）进行**场景理解、对象识别和任务规划**。结合上下文，此图可能是用于训练或评估模型对“场景图（Scene Graph）”、“图像描述（Image Caption）”和“对象列表（Object List）”生成能力的示例，强调从视觉输入中提取语义信息的能力。

【图像 10】(60x83)
根据您提供的信息（PDF第11页中的第10张图像，以及上下文内容），以下是对该图像的详细分析：

---

### 1. **图像类型**  
该图像为**结构化示意图/图表**，具体属于**多模态视觉-语言模型（VLM）与大语言模型（LLM）在场景理解任务中处理流程的对比图**。它由四个子图（(a)、(b)、(c)、(d)）组成，展示不同层级的信息提取过程。

---

### 2. **图像中的所有文字内容**  
图像中标注的文字包括：  
- 子图标签：**(a)**, **(b)**, **(c)**, **(d)**  
- 模块名称：  
  - **High-Level Task Planning**  
  - **Low-Level Action Planning**  
  - **LLM**（Large Language Model）  
  - **VLM**（Vision-Language Model）  
- 数据输入/输出描述：  
  - **Object List**: sofa, desk, drawer, pillow, …  
  - **Image Caption**: There are two sofas in front and a door in the left corner, …  
  - **Scene Graph**: bed, desk, agent, door, …  
  - 其他文本片段如：chair, desk, drawer, sink, fridge, …；desk, drawer, potted plant, trash can, computer, bed, …；There is a laptop and two monitors on the desktop. There are no items placed on the desktop to the …

---

### 3. **图像中的关键视觉元素和数据信息**  
- **整体结构**：图像分为上下两层，上层为“高层任务规划”（High-Level Task Planning），下层为“低层动作规划”（Low-Level Action Planning）。  
- **左侧模块**：标注为 **LLM**，表示仅使用语言模型进行推理，其输入来自文本描述（如 Image Caption 或 Object List）。  
- **右侧模块**：标注为 **VLM**，表示结合视觉与语言信息的模型，其输入包含图像及其语义结构（如 Scene Graph）。  
- **中间连接线**：显示从原始视觉信息到高级语义表示的转换路径，例如从图像 → Object List → Image Caption → Scene Graph。  
- **四幅子图**：分别展示了不同的输入形式或输出结果，用于说明不同模型如何处理同一场景的多模态信息。

---

### 4. **图像传达的主要信息**  
该图旨在说明：  
- 在机器人或智能系统中，**VLM** 能够更有效地整合视觉与语言信息，生成更丰富的场景理解（如 Scene Graph 和精确的对象关系）。  
- 相比之下，**LLM** 主要依赖文本输入，可能缺乏对空间布局和对象间关系的直接感知能力。  
- 通过比较 (a) 到 (d)，展示了从简单物体列表到复杂场景描述的演进过程，强调了**多模态融合**在实现高精度环境理解中的重要性。  

> 总结：此图为一种**多模态感知系统的架构对比图**，突出 VLM 在场景理解任务中优于 LLM 的优势，尤其适用于需要精确空间推理的任务（如家庭机器人导航、交互等）。

【图像 11】(66x78)
由于您提供的图像内容仅是一个表情符号（👤），并未包含实际的图像或视觉信息，因此无法对PDF第11页中的第11张图像进行分析。

不过，根据您提供的**页面文本上下文**，可以推断该图像可能属于以下类型：

---

### 1. **图像类型**  
**图表（Diagram）** —— 很可能是用于说明“视觉语言模型”（VLM）与“大型语言模型”（LLM）在场景理解任务中协同工作的流程图或结构图。结合上下文提到的“Object List”、“Image Caption”、“Scene Graph”等术语，这可能是一张**多模态任务处理流程图**，展示从图像输入到高层任务规划的多个阶段。

---

### 2. **图像中的所有文字内容**  
基于上下文推测，图像中可能包含以下文字标签（但未直接显示在当前输入中）：
- (a) High-Level Task Planning  
- (b) Low-Level Action Planning  
- LLM（Large Language Model）  
- VLM（Vision-Language Model）  
- Object List  
- Image Caption  
- Scene Graph  
- 具体对象名称如：sofa, desk, drawer, pillow, chair, sink, fridge, laptop, monitors, bed, door, potted plant, trash can, computer 等

这些文字可能是标注在不同模块或图示区域上的标签。

---

### 3. **图像中的关键视觉元素和数据信息**  
尽管无法看到图像本身，但从上下文可合理推断其结构如下：

- **四个子图（a）、（b）、（c）、（d）**：分别表示不同层次的信息处理阶段。
- **(a)** 可能展示高层任务规划（High-Level Task Planning），例如“整理房间”这样的抽象目标。
- **(b)** 可能是低层动作规划（Low-Level Action Planning），涉及具体动作序列。
- **(c)** 可能是视觉识别结果，包括物体列表（Object List）和场景图（Scene Graph）。
- **(d)** 可能是图像描述（Image Caption）生成，如：“There are two sofas in front and a door in the left corner…”
- 图中可能存在两个核心模块：
  - **VLM（视觉语言模型）**：负责从图像中提取对象、关系和语义信息。
  - **LLM（大语言模型）**：负责基于VLM输出进行推理、规划和自然语言生成。
- 数据流可能表现为：图像 → VLM → 物体列表 + 场景图 → LLM → 高层任务规划 + 动作指令。

---

### 4. **图像传达的主要信息**  
该图像旨在说明一个**多模态智能系统的工作流程**，即如何通过**视觉语言模型（VLM）解析环境图像**，提取物体、空间关系和语义信息，并将这些信息传递给**大语言模型（LLM）**，以实现从感知到决策的端到端任务规划。

**核心思想**：  
> 利用VLM理解视觉世界，再由LLM进行高层推理和任务分解，从而实现机器人或AI代理在复杂环境中的自主行为规划。

---

### 总结  
虽然当前无法查看图像，但结合上下文可以判断：  
这是一张**多模态系统架构图**，展示了VLM与LLM协同完成“从图像感知到任务规划”的全过程，包含对象识别、场景理解、自然语言描述和动作规划等多个环节。


================================================================================
第 12 页
================================================================================

【文本内容】
IEEE/ASME TRANSACTIONS ON MECHATRONICS
12
environment during task execution and feed this information
back to the LLM, allowing it to modify unreasonable steps
in the current plan [187], [189], [190]. RoboGPT considers
the different names of similar objects within the same task,
further improving the feasibility of replanning [8]. However,
the information provided by labels is still too limited. Can
further scene information be provided? SayPlan [191] pro-
poses using hierarchical 3D scene graphs to represent the
environment, effectively mitigating the challenges of task plan-
ning in large, multi-floor, and multi-room settings. Similarly,
ConceptGraphs [192] also adopts 3D scene graphs to provide
environmental information to LLMs. Compared to SayPlan, it
offers more detailed open-world object detection and presents
task planning in a code-based format, which is more efficient
and better suited to the demands of complex tasks.
However, limited visual information can result in an agent’s
inadequate understanding of its environment. While LLMs
are provided with visual cues, they often fail to capture the
environment’s complexity and dynamic changes, leading to
misunderstandings and task failures. For example, if a towel
is locked in a bathroom cabinet, the agent might repeatedly
search the bathroom without considering this possibility [8].
To address this, more robust algorithms must be developed
to integrate multiple sensory data, enhancing the agent’s en-
vironmental understanding. Additionally, leveraging historical
data and contextual reasoning, even when visual information
is limited, can aid the agent in making reasonable judgments
and decisions. This approach of multimodal integration and
context-based reasoning not only increases task execution
success rates but also provides new perspectives for the
advancement of embodied artificial intelligence.
3) Planning utilizing the VLMs: Compared to converting
environmental information into text using external visual mod-
els, VLM models can capture visual details in latent space,
particularly contextual information that is difficult to represent
with object labels. VLMs can discern rules underlying visual
phenomena; for instance, even if a towel is not visible in
the environment, it can be inferred that the towel might be
stored in a cabinet. This process essentially demonstrates
how abstract visual features and structured textual features
can be more effectively aligned in latent space. In Embod-
iedGPT [193], the Embodied-Former module aligns embodied,
visual, and textual information, effectively considering the
agent’s state and environmental information during task plan-
ning. Unlike EmbodiedGPT, which directly uses third-person
perspective images, LEO [194] encodes 2D egocentric images
and 3D scenes into visual tokens. This method effectively per-
ceives 3D world information and executes tasks accordingly.
Similarly, the EIF-Unknow model utilizes Semantic Feature
Maps extracted from Voxel Features as visual tokens, which
are input along with text tokens into a trained LLaVA model
for task planning [195]. Furthermore, embodied multimodal
foundation models, or VLA models, have been extensively
trained with large datasets in studies like the RT series [2],
[9], PaLM-E [196], and Matcha [197] to achieve alignment of
visual and textual features in embodied scenarios.
However, task planning is only the first step for an agent
in completing an instruction task. Subsequent action planning
determines whether the task can be accomplished. In the
experiments from RoboGPT [8], the accuracy of task planning
reached 96%, but the overall task completion rate was only
60%, limited by the performance of the low-level planner.
Therefore, whether an embodied agent can transition from
the cyber space of “imagining how tasks are completed” to
the physical world of “interacting with the environment and
completing tasks” hinges on effective action planning.
B. Embodied Action Planning
The distinction between task planning and action planning
highlights that action planning must address real-world un-
certainties due to the insufficient granularity of task planning
subtasks [198]. Action planning can be achieved by: 1) using
pre-trained embodied models to complete subtasks via APIs,
or 2) leveraging the VLA model’s capabilities. The results
from action planning are fed back to refine task planning.
1) Action utilizing APIs: A common approach involves
providing LLMs with definitions of well-trained policy models
to understand and use them effectively for specific tasks [189],
[199]. By generating code, LLMs can abstract tools into a
function library, allowing better handling of sub-tasks [186].
Reflexion adjusts these tools during execution to improve
generalization [200]. DEPS enables LLMs to learn and com-
bine various skills through zero-shot learning [201]. The
hierarchical planning paradigm simplifies development by sep-
arating high-level task planning from specific action execution
through policy models. This modularity allows independent
development, testing, and optimization, enhancing flexibility
and maintainability. While it enables adaptability to various
tasks and environments, reliance on external policy models can
introduce latency and affect performance, making the quality
of these models crucial for overall agent effectiveness.
2) Action utilizing VLA model: Different form previous ap-
proach that task planning and action execution are performed
within the same system, this paradigm leverages the capabili-
ties of embodied multimodal foundation models for planning
and executing actions, reducing communication latency and
improving system response speed and efficiency. In VLA
models, the tight integration of perception, decision-making,
and execution modules allows the system to handle complex
tasks and adapt to changes in dynamic environments more
efficiently. This integration also facilitates real-time feedback,
enabling the agent to self-adjust strategies, thereby enhancing
the robustness and adaptability of task execution [10], [193],
[202]. However, this paradigm is undoubtedly more complex
and costly, particularly when dealing with intricate or long-
term tasks. Additionally, a key issue is that an action planner,
without an embodied world model, cannot simulate physical
laws using only the internal knowledge of an LLM. This
limitation hinders the agent to accurately and effectively
complete various tasks in the physical world, preventing the
seamless transfer from cyber space to physical world.
3) Scalability in Diverse Environments: Scalability in em-
bodied agents involves adapting to increased complexity in
larger and more diverse environments through robust per-
ception, efficient decision-making, and resource optimization.



================================================================================
第 13 页
================================================================================

【文本内容】
IEEE/ASME TRANSACTIONS ON MECHATRONICS
13
Strategies include hierarchical SLAM for mapping, multi-
modal perception, and energy-efficient edge computing. Col-
laborative scalability is enhanced via multi-agent systems and
decentralized communication, while generalization relies on
domain adaptation to operate in new environments.
VII. SIM-TO-REAL ADAPTATION
Sim-to-Real adaptation in embodied AI refers to the process
of transferring capabilities or behaviors learned in simulated
environments (cyber space) to real-world scenarios (physical
world). It involves validating and improving the effectiveness
of algorithms, models, and control strategies developed in sim-
ulation to ensure they perform robustly and reliably in physical
environments. To achieve sim-to-real adaptation, embodied
world models, data collection and training methods, and
embodied control algorithms are three essential components.
A. Embodied World Model
Sim-to-Real involves creating simulation-based world mod-
els that closely resemble real-world environments. These mod-
els predict the next state to make decisions and are trained
from scratch on physical world data, unlike VLA models
which are pre-trained on large-scale datasets and fine-tuned
with real-world data. World models are effective for structured
tasks like autonomous driving and object sorting but are less
suited for unstructured, complex tasks.
Learning world models is promising of the physical simu-
lation field. Compared to traditional simulation methods, it
offers significant advantages, such as the ability to reason
about interactions with incomplete information, meet real-time
computation requirements, and improve prediction accuracy
over time. The predictive capability of such world models
is crucial, enabling robots to develop the physical intuition
necessary to operate in the human world. As shown in Fig.
10, according to the learning pipeline of the world environ-
ment, they can be divided into Generation-based methods,
Prediction-based methods and Knowledge-driven methods.
1) Generation-based Methods: As the scale of models
and data increases, generative models have demonstrated the
ability to understand and generate images (e.g., World Models
[203]), videos (e.g., Sora [16], Pandora [204]), point clouds
(e.g., 3D-VLA [205]) or other formats of data (e.g., DWM
[206]) that conform to physical laws. This capability sug-
gests that generative models can internalize world knowledge.
Specifically, after exposure to large datasets, these models not
only capture statistical properties but also simulate physical
and causal relationships through their intrinsic structures.
Thus, generative models function as more than just pattern
recognition tools; they exhibit characteristics of world models.
The embedded world knowledge in these models can be har-
nessed to enhance the performance of other models. By lever-
aging this knowledge, we can improve model generalization,
robustness, adaptability to new environments, and predictive
accuracy on unseen data [204], [205]. However, generative
models also have certain limitations and drawbacks. For in-
stance, they may produce inaccurate or distorted outputs when
faced with biased data distributions or insufficient training
(a) Generation-based Methods
x
Decoder
α 
y
Transformation
L
(b) Prediction-based Methods
Encoder
x
Predictor
α 
y
Transformation
L
Encoder
(c) Knowledge-driven Methods
Encoder
x
Predictor 
/ Decoder
α 
y
Transformation
L
Encoder
Optional
k
Encoder
Fig. 10.
Embodied world models can be roughly divided into three type.
(a) Generation-based Methods learn the transformation relation between
the input space and the output space using an autoencoder framework. (b)
Prediction-based Methods are more general frameworks where a world
model is trained in latent space. (c) Knowledge-driven Methods inject
artificially constructed knowledge into the model, giving the model world
knowledge to obtain output that meets the given knowledge constraints. Note
that the components within the dashed line are optional.
data. Moreover, these models require substantial computational
resources and time for training and often lack interpretability,
which hinders practical application. While generative mod-
els have shown promise in generating content that adheres
to physical laws, challenges such as improving efficiency,
enhancing interpretability, and mitigating data bias must be
addressed for broader application. Continued research is likely
to unlock further value and potential in these models.
2) Prediction-based Methods: The prediction-based world
model predicts and understands the environment by construct-
ing and utilizing internal representations. By reconstructing
corresponding features in the latent space based on provided
conditions, it captures deeper semantics and associated world
knowledge. This model maps input information to a latent
space and operates within that space to extract and utilize
high-level semantic information, thereby enabling the robots
to perceive the essential representation of the world envi-
ronment (e.g., I-JEPA [15], MC-JEPA [207], A-JEPA [208],
Point-JEPA [209], IWM [210]) and more accurately perform
embodied downstream tasks (e.g., iVideoGPT [211], IRASim
[212]), STP [213], MuDreamer [214]). Latent features, unlike
pixel-level information, can abstract and decouple various
forms of knowledge, enabling models to handle complex
tasks and scenes more effectively while enhancing gener-
alization [215]. In spatiotemporal modeling, for instance, a
world model predicts an object’s post-interaction state by
integrating its current state, the nature of the interaction,
and internal knowledge. Specifically, embodied world models
generate dynamic environmental predictions by combining
perceptual information with prior knowledge, relying on both
sensory data and inherent world knowledge to accurately infer
and predict environmental changes [211], [213], [214]. This
process considers the current state of objects alongside their
historical and contextual information.
Similarly, leveraging the world knowledge embedded in its
representations can further enhance the model’s perception
and robustness [15], [207], [210], [216]. By operating in
latent space, it is expected that robots can maintain high
performance in different environments at a lower cost [214].
The key to this approach lies in abstract processing and knowl-
edge decoupling, enabling efficient adaptation to complex
situations. However, such models may exhibit limitations and
instability when dealing with previously unseen environments
and conditions. Additionally, the world knowledge decoupled



================================================================================
第 14 页
================================================================================

【文本内容】
IEEE/ASME TRANSACTIONS ON MECHATRONICS
14
Real2Sim2Real
TRANSIC
1. Real2Sim
Scan
Reconstruct
2. Simulation fine-tuning
3. Sim2Real transfer
1. Simulation Policy
2. Direct Deployment
3. Human-in-the-Loop 
Correction
4. Successful Transfer 
with Learned Residual 
Policy
Domain Randomization
Random Scenario Configuration
1. Train in the simulator
Without Additional Training
2. Test in the real 
world
System Identification
More Realistic 
Simulation Environment
1. Train in the simulator
Lang4Sim2Real
gripper holding 
bread over square
gripper holding 
carrot over 
yellow mat
sim
real
1. Image Language Pretraining
Image 
Encoder
Sim2real Transfer Easily
2. Test in the real world
🔥
2. Imitation Learning Training
LLM
Task Instruction
Fig. 11.
Five pipelines to achieve sim-to-real gap. “Real2Sim2Real” reduces the gap by reconstructing real scenes. “TRANSIC” compensates for the
sim-to-real transfer gap through human-corrected interventions. “Domain Randomization” enhances model transfer adaptability by simulating environmental
diversity. “System Identification” improves sim-to-real environment similarity, thereby mitigating the sim-real gap. “Lang4Sim2Real” uses natural language
to bridge two domains, learning invariant image representations and reducing visual gaps.
in the latent space may have interpretability issues.
3) Knowledge-driven Methods: Knowledge-driven world
models inject artificially constructed knowledge into the mod-
els, endowing them with world knowledge. This method has
shown broad application potential in the field of embodied AI.
For example, in the real2sim2real approach [217], real-world
knowledge is used to build physics-compliant simulators,
which are then used to train robots, enhancing model robust-
ness and generalization capabilities. Additionally, artificially
constructing common sense or physics-compliant knowledge
and applying them to generative models or simulators is a
common strategy (e.g., ElastoGen [218], One-2-3-45 [219],
PLoT [220]). This approach imposes more physically accurate
constraints on the model, enhancing its reliability and inter-
pretability in generation tasks. These constraints ensure the
model’s knowledge is both accurate and consistent, reducing
uncertainty during training and application. Some approaches
combine artificially created physical rules with LLMs or
MLMs. By leveraging the commonsense capabilities of LLMs
and MLMs, these approaches (e.g., Holodeck [56], LEGENT
[221], GRUtopia [222]) generate diverse and semantically rich
scenes through automatic spatial layout optimization. This
greatly advances the development of general-purpose embod-
ied agents by training them in novel and diverse environments.
4) Limitations: Current limitations of world models include
handling the complexity and variability of real-world environ-
ments, such as high-dimensional sensory inputs, dynamic and
stochastic elements, and long-term dependencies. Many mod-
els struggle with generalization across unseen scenarios, often
requiring extensive training data and computational resources.
Additionally, sim-to-real transfer remains problematic, as sim-
ulated environments fail to fully capture real-world physics
and noise. These limitations can be addressed by integrating
more realistic simulators, incorporating multimodal sensory
inputs, and using hierarchical or modular architectures. Im-
proving data efficiency, enhancing transfer learning techniques,
and incorporating real-world priors can also enable more
accurate predictions and adaptive decision-making. Recently,
the Cosmos Platform [223] was proposed by NVIDIA, which
contains autoregressive and diffusion models for Text-to-World
and Video-to-World generation, to accelerate the development
of physical AI systems. It may give us some inspirations about
how to build an embodied world model.
B. Data Collection and Training
For sim-to-real adaptation, the high-quality data is impor-
tant. Traditional data collection methods involve expensive
equipment and precise operations, which are time-consuming
and labor-intensive. Recently, some efficient and cost-effective
methods have been proposed for high-quality demonstration
data collection and training. We discuss data collection meth-
ods in both real-world and simulated environments.
1) Real-World Data: Training large, high-capacity models
with rich datasets has shown great success. This approach is
also promising for robotics, where large, diverse datasets can
enhance generalization and adaptability. Open X-Embodiment
[202] provides data from 22 robots with 527 skills and 160,266
tasks in domestic settings. UMI [224] offers a framework for
collecting dynamic, bimanual data using a handheld gripper.
Mobile ALOHA [225] enables data collection for full-body
mobile manipulation tasks. Human-agent collaboration [226]
improves data quality and efficiency by combining human
input with agent refinement processes.
2) Simulated Data: Data collection in real-world settings
is resource-intensive and time-consuming, making simulation-
based data collection an attractive alternative. Simulated envi-
ronments allow for automated, efficient data collection. For
example, CLIPORT [168] and Transporter Networks [227]
utilized Pybullet simulator data for training and success-
fully transferred models to real-world applications. GAPartNet
[228] developed a large-scale dataset with detailed part-level
annotations for better object interaction in both simulations
and reality. SemGrasp [167] created the CapGrasp dataset for
semantically rich hand grasping in virtual environments.
3) Sim-to-Real Paradigms: Recently, several sim-to-real
paradigms have been introduced, to mitigate the need for ex-
tensive and costly real-world demonstration data by conduct-
ing extensive learning in simulation environments, followed
by migration to real-world settings. This section outlines five
paradigms for sim-to-real transfer, as shown in Fig. 11.
Real2Sim2real [229] improves imitation learning by us-
ing reinforcement learning in a ”digital twin” simulation to
develop strategies, which are then transferred to the real
world. TRANSIC [230] reduces the sim-to-real gap through
real-time human intervention and residual policy training
based on corrected behaviors. Domain Randomization [231]–
[233] increases model generalization by varying simulation
parameters to cover real-world conditions. System Identifica-


【图像 1】(216x186)
1. **图像类型**：  
   这是一张**实物照片**，展示了一个实验室或工作台环境中的机器人系统操作场景。

2. **图像中的所有文字内容**：  
   图像中可见的文字包括：
   - 瓶子标签上的“100%”（可能是某种试剂或液体的标识）。
   - 无其他明显可读文本。页面上下文中的文字属于PDF文档正文，并非图像本身内容。

3. **图像中的关键视觉元素和数据信息**：  
   - 一台**机械臂**（机器人手臂）正位于一个红色托盘上方，似乎在执行抓取或放置动作。
   - 托盘内有多个小物体（可能为样本、工具或组件），部分呈橙色/棕色。
   - 工作台上还放置了：
     - 一个透明塑料瓶（带白色盖子，标有“100%”）；
     - 一个黑色设备（可能是打印机或扫描仪）；
     - 背景中有置物架，摆放着杯子、瓶子等物品。
   - 整体环境为**室内实验室或办公环境**，光线充足，布局整洁。

4. **图像传达的主要信息**：  
   该图像展示了**机器人在真实世界环境中进行操作的场景**，可能用于说明“Sim2Real”（仿真到现实）技术的实际应用。结合页面上下文（如“Real2Sim2Real”、“Simulation Policy”、“Direct Deployment”等），此图很可能代表**真实世界任务的部署阶段**，即经过仿真训练后，机器人在真实环境中执行任务的过程，强调从仿真到现实的迁移能力。

【图像 2】(134x134)
根据您提供的信息，以下是对该图像的详细分析：

---

### 1. 图像类型  
**图形（图标）**  
该图像为一个简单的几何图形图标，呈现为一个带有圆角的白色方框，内部有一条水平绿色线条。

---

### 2. 图像中的所有文字内容  
**无文字内容**  
图像中未包含任何文本或标签。

---

### 3. 图像中的关键视觉元素和数据信息  
- **外框**：白色背景，四角为圆角矩形，边框清晰。
- **中心元素**：一条水平的绿色实线，位于方框中央，长度约为方框宽度的三分之二。
- **颜色**：仅使用绿色和白色，对比鲜明。
- **风格**：极简设计，可能用于表示“进度条”、“状态指示”或“加载状态”。

---

### 4. 图像传达的主要信息  
该图像很可能是一个**占位符图标**或**简化状态指示器**，在上下文（如IEEE/ASME论文）中可能用于：
- 表示某个流程阶段的“进行中”状态；
- 作为流程图中的符号，代表“扫描”或“重建”等步骤（结合页面文本“Scan”、“Reconstruct”）；
- 或是某种系统状态的可视化提示（如“模拟训练中”）。

由于其简洁性，它更可能是**流程图中的一个抽象符号**，而非具体数据图表。结合页面主题“Real2Sim2Real”与“Simulation Policy”，该图标可能象征“仿真阶段的初始化”或“任务执行中的中间状态”。

---

### 总结  
这是一张**无文字的极简图形图标**，可能用于表示**流程中的某个阶段或状态**，如“扫描”或“处理中”。其设计风格符合技术文档中常见的流程图符号，但缺乏具体语义需结合上下文进一步解读。

【图像 3】(132x132)
根据您提供的信息，该图像为 **PDF第14页中的第3张图像**，其上下文内容来自一篇关于“Real2Sim2Real”方法的论文，主题涉及机器人仿真到现实（Sim2Real）的迁移学习。

然而，您上传的图像仅显示一个 **蓝色像素化图形**，形似一个简化的山形或抽象图标，由多个蓝色方块组成，背景为黑色。此图像与论文中描述的“Real2Sim2Real”流程图、系统架构或技术框架无直接关联，可能为 **占位符、错误加载或无关图像**。

---

### 1. 图像类型  
- **图形 / 像素艺术**（Pixel art）  
- 非图表、非照片、非截图，也非标准流程图或示意图

---

### 2. 图像中的所有文字内容  
- **无文字内容**  
- 图像中未包含任何可识别的文字或标签

---

### 3. 图像中的关键视觉元素和数据信息  
- **视觉元素**：  
  - 蓝色方块组成的不规则形状  
  - 整体呈上升趋势的阶梯状结构，类似山峰或抽象地形  
  - 黑色背景，对比强烈  
- **数据信息**：  
  - 无量化数据、坐标轴、标签或数值  
  - 无箭头、流程线或连接关系  
  - 无法从中提取任何技术参数或实验结果  

---

### 4. 图像传达的主要信息  
- **无法传达明确的技术或学术信息**  
- 可能是 **占位符图像** 或 **误插入的图标**（如Logo、装饰性图案）  
- 与上下文“Real2Sim2Real”、“Simulation fine-tuning”、“Sim2Real transfer”等技术内容 **无直接联系**

---

### 总结  
该图像为一个 **无文字的蓝色像素图形**，不具备技术图表的功能，可能是文档排版错误或插图缺失所致。建议结合原文中提及的“Real2Sim2Real”流程图（可能在其他页面）进行分析，以获取完整信息。

【图像 4】(268x224)
1. **图像类型**：  
   该图像是一个**3D渲染图形**，展示了一个重复排列的机器人或机械装置的模拟场景，属于计算机生成的仿真环境图像。

2. **图像中的所有文字内容**：  
   图像中**无可见文字内容**。

3. **图像中的关键视觉元素和数据信息**：  
   - 多个相同的机器人（或机械臂）结构以网格状均匀分布在浅灰色背景上。  
   - 每个机器人由多个部件组成，包括基座、机械臂和末端执行器，整体呈红黑配色。  
   - 机器人姿态一致，表明是同一模型的重复实例化，可能用于批量仿真测试。  
   - 布局规则且对称，暗示这是在仿真环境中进行的系统性实验配置。  

4. **图像传达的主要信息**：  
   该图像展示了**高保真度仿真环境中的多机器人部署场景**，结合上下文“Real2Sim2Real”、“Simulation fine-tuning”、“Domain Randomization”等术语，说明其用于**仿真训练与现实世界迁移（Sim2Real）研究**，特别是通过大规模、多样化仿真场景提升策略鲁棒性，支持后续在真实世界中的部署与验证。

【图像 5】(200x200)
1. **图像类型**：  
   该图像为一个**图形图标**，属于简化风格的机器人形象，常用于表示自动化、人工智能或机器人相关主题。

2. **图像中的所有文字内容**：  
   图像中**无文字内容**。

3. **图像中的关键视觉元素和数据信息**：  
   - 图像主体是一个蓝色的机器人轮廓，由两个部分组成：  
     - 上部为矩形头部，顶部有一个小凸起（类似天线），内有两个黑色圆形作为眼睛。  
     - 下部为椭圆形身体，与头部相连。  
   - 整体设计简洁，采用扁平化风格，颜色为纯蓝色，背景为白色。  
   - 无其他装饰或细节，强调符号性而非写实性。

4. **图像传达的主要信息**：  
   该图标象征“机器人”或“智能系统”，结合上下文（如“Real2Sim2Real”、“Sim2Real transfer”、“Simulation Policy”等术语），可推断其代表**机器人系统在仿真到现实（Sim2Real）迁移过程中的核心实体**，可能用于标识机器人平台、智能体或自动化系统。它在文中可能作为流程图或框架图中的视觉元素，用于直观表示机器人角色。

【图像 6】(278x233)
1. **图像类型**：  
   这是一张**真实场景的照片**（照片），展示了一个实验室环境中的机器人系统。

2. **图像中的所有文字内容**：  
   图像中可见的文字包括：
   - 一个白色容器上的标签：“100%”（部分可见）
   - 其他物品上无清晰可读文字
   - 图像右下角有蓝色箭头标注，但无文字说明

3. **图像中的关键视觉元素和数据信息**：  
   - 一台**机械臂**（可能是协作机器人）位于画面中央，安装在桌面上，正伸向一个黑色设备（可能为微波炉或加热装置）。
   - 机械臂末端装有**夹爪或传感器装置**，正在执行操作。
   - 桌面为红色，背景是木质储物架，上面摆放着各种实验用品（如瓶子、盒子、书籍等）。
   - 机械臂下方有一个**透明的防护罩**，可能用于安全隔离或观察。
   - 图像中添加了**蓝色箭头**，指向机械臂的末端执行器和其动作方向，表明该图强调的是**机器人运动轨迹或操作过程**。
   - 左侧可见一个黑色箱体设备，顶部有显示屏或控制面板，推测为被操作对象（如微波炉）。

4. **图像传达的主要信息**：  
   该图像展示了**机器人在现实环境中执行任务的场景**，结合上下文“Real2Sim2Real”、“Sim2Real transfer”等内容，此图旨在说明**仿真到现实（Sim2Real）技术的实际应用案例**。具体而言，它可能表示：
   - 机器人在真实世界中完成任务（如打开微波炉、取物等），作为仿真训练后的实际部署验证；
   - 强调从仿真环境迁移到真实环境的成功转移；
   - 与论文中提到的“Direct Deployment”或“Human-in-the-Loop Correction”步骤相关，体现机器人在真实场景中的表现。

综上，这是一张用于支持“仿真到现实迁移”研究的**真实实验场景照片**，重点展示机器人在真实环境中的操作能力。

【图像 7】(200x200)
1. **图像类型**：  
   该图像是一个**图形符号**（图标），由几何形状构成，属于**抽象图形标志**。

2. **图像中的所有文字内容**：  
   图像中**无文字内容**。

3. **图像中的关键视觉元素和数据信息**：  
   - 图像主体为一个**绿色的八角星形轮廓**（类似徽章或印章形状）。  
   - 内部包含一个**绿色的对勾（✓）符号**，居中放置。  
   - 整体设计简洁，具有“确认”、“通过”或“成功”的视觉含义。  
   - 背景为黑色，与绿色形成高对比度。

4. **图像传达的主要信息**：  
   该图标象征**验证成功、任务完成或系统通过测试**，常用于表示流程中的某个阶段已顺利完成或被确认。结合上下文（如“Real2Sim2Real”流程、“Sim2Real transfer”、“Successful Transfer”等术语），此图标可能代表**仿真到现实（Sim2Real）迁移的成功实现**，即在仿真环境中训练的策略成功部署到真实世界并取得预期效果。

【图像 8】(222x169)
1. **图像类型**：  
   这是一张**3D渲染的图形图像**，展示了一个机械臂（或机器人）在操作一个物体的场景。图像具有高清晰度和逼真的光影效果，属于典型的仿真环境中的可视化示意图。

2. **图像中的所有文字内容**：  
   图像中**没有直接的文字内容**。文字信息来自页面上下文，而非图像本身。

3. **图像中的关键视觉元素和数据信息**：  
   - **机械臂结构**：图像显示一个白色、多关节的机械臂，末端装有夹持器（gripper），正在抓取一个黑色圆柱形物体。
   - **夹持动作**：夹持器正夹住物体顶部，表明正在进行抓取或放置操作。
   - **阴影效果**：机械臂和物体在浅色背景上投下清晰的阴影，增强了三维感和真实感。
   - **材质与细节**：机械臂表面光滑，呈现金属或塑料质感；夹持器部分有精密结构，体现高精度控制能力。
   - **背景简洁**：背景为单一浅灰色，无干扰元素，突出主体。

4. **图像传达的主要信息**：  
   该图像用于说明“**Sim2Real**”（仿真到现实）技术中的机器人操作场景，可能代表在仿真环境中训练的机器人策略被迁移到真实世界的应用。结合上下文，它体现了**高保真仿真环境**的重要性，以及通过仿真训练实现真实世界部署的可行性。图像强调了**机器人操控的精确性**和**仿真环境的逼真度**，支持“Real2Sim2Real”框架中关于仿真与现实之间迁移的研究主题。

【图像 9】(226x174)
1. **图像类型**：  
   这是一张**实物照片**，展示了一个机械臂在真实环境中的操作场景。

2. **图像中的所有文字内容**：  
   图像中**没有可见的文字内容**。背景和设备上均未显示可读文本。

3. **图像中的关键视觉元素和数据信息**：  
   - 一台**白色机械臂**（可能是协作机器人，如UR系列或类似型号），配备双指夹爪。
   - 机械臂正抓取一个**黑色圆形物体**，该物体放置在浅色桌面上。
   - 背景为实验室环境，包括深色幕布、部分电子设备和线缆。
   - 机械臂的结构清晰可见，包含关节、电机外壳和连接线缆。
   - 物体表面光滑，可能为实验用测试件或传感器部件。

4. **图像传达的主要信息**：  
   该图像展示了**真实世界中机器人执行抓取任务的场景**，与上下文中的“Real2Sim2Real”流程相呼应，强调从真实世界采集数据（Real2Sim）、在仿真中训练并最终部署回真实环境（Sim2Real）的闭环过程。此图可能用于说明**物理系统在真实环境下的行为表现**，作为仿真与现实之间迁移验证的一部分。

【图像 10】(226x189)
1. **图像类型**：  
   这是一张**照片**，展示了机器人机械臂在真实环境中的操作场景，并叠加了图形元素（如绿色箭头和人形轮廓）以增强信息表达。

2. **图像中的所有文字内容**：  
   图像中**没有直接的文字内容**。文字信息来自页面上下文，与图像本身无关。

3. **图像中的关键视觉元素和数据信息**：  
   - **机械臂**：前景是一个白色和黑色相间的机器人机械臂，末端装有夹持器或工具，正对一个圆形物体进行操作。  
   - **绿色箭头**：围绕机械臂末端的绿色弧形箭头，表示旋转或运动轨迹，暗示机械臂正在进行某种动态操作（如抓取、旋转）。  
   - **绿色人形轮廓**：位于图像右侧，代表人类用户或操作者，可能象征“人机交互”或“人类在环”（Human-in-the-Loop）的概念。  
   - **背景**：另一台类似的机械臂部分可见，表明这是一个多机器人或实验环境设置。  
   - **连接线**：从机械臂延伸出的黑色线缆，表明其为电动驱动并连接到控制系统。

4. **图像传达的主要信息**：  
   该图像展示了**机器人在真实世界中执行任务的场景**，结合绿色箭头和人形轮廓，强调了**人机协作**或**人类在环的控制机制**，可能用于说明“Sim2Real”（仿真到现实）技术中的人类干预环节，尤其是在仿真训练后部署到真实环境时的修正与调整过程。这与页面文本中提到的“Human-in-the-Loop Correction”（人类在环校正）和“Sim2Real transfer”（仿真到现实迁移）高度相关。

【图像 11】(226x187)
1. **图像类型**：  
   这是一张**实物照片**，展示了一个机器人系统的实际硬件装置，属于实验或研究场景中的设备截图。

2. **图像中的所有文字内容**：  
   图像中**没有可见的文字内容**。所有文字信息均来自PDF页面的上下文描述，而非图像本身。

3. **图像中的关键视觉元素和数据信息**：  
   - 一个机械臂末端执行器（夹爪）正在抓取一个发光的白色球体。
   - 夹爪结构为金属材质，具有两个对称的夹持部分，设计精密，可能用于精细操作。
   - 球体表面光滑、反光，发出白光，可能是某种传感器或测试物体。
   - 背景中可见另一台类似设备的部分结构，暗示这是一个实验室环境。
   - 设备通过线缆连接，表明其为电动或电子控制的系统。
   - 整体布局整洁，光线均匀，典型科研或工程测试场景。

4. **图像传达的主要信息**：  
   该图像展示了**机器人在真实环境中进行物理交互的场景**，特别是“Sim2Real”（仿真到现实）技术中的实际部署阶段。结合上下文，此图可能用于说明在完成仿真训练后，机器人在真实世界中成功执行任务（如抓取物体），验证了从仿真到现实的迁移能力。图像强调了高精度操作与真实世界物理交互的重要性，是“Real2Sim2Real”流程中“Real”阶段的直观体现。

【图像 12】(468x348)
1. **图像类型**：  
   该图是一张由六个子图组成的**图形化示意图**（或称为“模拟场景拼贴图”），展示的是计算机生成的三维仿真环境中的不同场景配置，属于**虚拟仿真图像**。

2. **图像中的所有文字内容**：  
   图像中**无直接文字内容**。但根据上下文（PDF第14页）可知，该图与“Sim2Real”（仿真到现实）相关，可能用于说明“Domain Randomization”或“Simulation Environment”中的多样化场景设置。

3. **图像中的关键视觉元素和数据信息**：  
   - 图像由**6个独立的子图**组成，呈2行3列排列。
   - 每个子图展示一个**三维虚拟桌面场景**，包含：
     - 一个带有不同颜色和纹理的桌面（如棕色、白色、绿色、橙色等）。
     - 多种几何形状的物体（如立方体、圆柱体、球体、锥体、三角形等），颜色各异（绿色、蓝色、紫色、红色、黄色等）。
     - 物体在桌面上的布局各不相同，表现出**高度随机性**。
     - 背景和桌面材质也存在差异（如木纹、光滑表面、彩色背景墙等）。
   - 所有场景均具有统一的视角（俯视略偏角），表明是同一类仿真系统输出。
   - 底部有三个小圆点，表示这可能是**多页幻灯片或图像序列的一部分**。

4. **图像传达的主要信息**：  
   该图像展示了**仿真环境中通过领域随机化（Domain Randomization）生成的多样化场景**，目的是为了训练机器人或AI系统在各种不确定条件下具备鲁棒性。这些场景模拟了真实世界中可能出现的多种物体配置、光照、材质和颜色变化，从而支持“Sim2Real”迁移任务，即在仿真中训练的策略能成功部署到真实物理环境中。

【图像 13】(379x311)
1. **图像类型**：  
   这是一张**实物照片**，拍摄的是一个桌面上摆放的多种几何形状积木。

2. **图像中的所有文字内容**：  
   图像中**没有文字内容**。

3. **图像中的关键视觉元素和数据信息**：  
   - 一张浅色木桌，桌面为浅棕色，四条腿为浅木色。  
   - 桌面上摆放着多个颜色鲜艳、形状各异的塑料积木，包括：
     - **立方体**：蓝色、红色、黄色。
     - **圆柱体**：蓝色、黄色。
     - **球体**：黄色。
     - **三角锥（金字塔形）**：绿色、红色。
     - **三棱柱**：绿色、红色。
     - **六边形柱体**：黄色。
   - 积木颜色包括：蓝色、红色、黄色、绿色。
   - 积木分布较为分散，无明显排列规律。
   - 背景为灰色地毯，左上角可见部分黑色设备（可能是机器人或传感器）及连接线缆。

4. **图像传达的主要信息**：  
   该图像展示了一个用于机器人学习或仿真训练的**物理环境场景**，可能用于**Sim-to-Real（仿真到现实）迁移**任务，例如物体识别、抓取、堆叠等。结合上下文“Real2Sim2Real”和“Simulation fine-tuning”，这些积木可能是机器人在真实世界中进行感知与操作任务的目标对象，用于验证仿真训练模型在现实环境中的泛化能力。

【图像 14】(231x233)
1. **图像类型**：  
   这是一张三维渲染的计算机图形（3D仿真图像），展示了一个机器人在模拟环境中的操作场景，属于“仿真环境”或“虚拟现实”类图像。

2. **图像中的所有文字内容**：  
   图像中**没有可见的文字内容**。所有文本信息均来自页面上下文，而非图像本身。

3. **图像中的关键视觉元素和数据信息**：  
   - 一个白色的多关节机械臂（机器人手臂），安装在黑色底座上，位于木制工作台上方。  
   - 机械臂末端夹持着一个橙色物体（可能是小盒子或工具）。  
   - 工作台上有一个红色矩形物体（可能是目标物或传感器）。  
   - 背景为浅灰色墙壁，地面为木质地板，整体环境模拟真实实验室或工业场景。  
   - 场景具有逼真的光照和阴影效果，表明这是一个高保真度的仿真环境。

4. **图像传达的主要信息**：  
   该图像展示了“Sim2Real”（仿真到现实）研究中的典型场景——机器人在高度仿真的虚拟环境中执行任务（如抓取、放置），以支持后续在真实世界中的部署。结合上下文，此图可能用于说明通过仿真训练机器人策略，并将其迁移到现实系统的过程，强调仿真环境的真实感对成功迁移的重要性。

【图像 15】(228x227)
1. **图像类型**：  
   这是一张**实物照片**，展示了一台机械臂在真实环境中的操作场景。

2. **图像中的所有文字内容**：  
   图像中**无可见文字内容**。所有文字信息均来自页面上下文（PDF文本），而非图像本身。

3. **图像中的关键视觉元素和数据信息**：  
   - 一台白色机械臂，结构精密，带有黑色关节和传感器部件，末端装有橙色夹持器或工具。
   - 机械臂正悬停于一块彩色拼接垫上方，垫子由粉色、黄色等颜色组成，具有模块化设计特征。
   - 背景为木质桌面和竹编屏风，表明实验环境为室内实验室或工作台。
   - 机械臂下方的橙色物体可能是被夹取的目标物，暗示正在进行抓取或操作任务。
   - 整体布局显示该装置用于机器人感知与操作任务，可能涉及仿真到现实（Sim2Real）的迁移测试。

4. **图像传达的主要信息**：  
   该图像展示了**真实世界中机器人执行任务的场景**，结合上下文“Real2Sim2Real”和“Sim2Real transfer”，说明这是在验证通过仿真训练后，将策略部署到真实机器人系统的过程。图像强调了从仿真到现实转移的实际应用环节，突出机器人在真实环境中完成精细操作的能力，是“仿真-现实迁移”研究流程中的关键一步。

【图像 16】(305x277)
1. **图像类型**：  
   这是一张**照片**，展示了一台实际的工业机器人在实验室或实验环境中的工作场景。

2. **图像中的所有文字内容**：  
   - 机器人臂上印有品牌标识：“**KUKA**”（出现两次）。
   - 机器人底座上有警告标签，包含符号和文字，但具体内容不清晰，仅可见“**WARNING**”字样及黄色三角形警示标志。
   - 背景中有一块展板，上面有模糊的文字和图像，无法辨认具体信息。

3. **图像中的关键视觉元素和数据信息**：  
   - 一台**KUKA品牌的机械臂**，呈灰白色，关节处为橙色，具有多自由度结构。
   - 机械臂末端装有一个**夹持装置或传感器组件**，正对准桌面上的一个小型白色物体（可能是测试件或工件）。
   - 桌面为浅灰色，表面平整，可能为实验台。
   - 背景为实验室环境，包括玻璃隔断、展板和部分其他设备。
   - 整体构图聚焦于机器人与目标物体之间的交互过程。

4. **图像传达的主要信息**：  
   该图像展示了**真实世界中机器人系统的物理部署情况**，结合上下文“Real2Sim2Real”、“Sim2Real transfer”等术语，可推断此图用于说明**从仿真到现实的机器人控制策略迁移过程中的真实世界验证环节**。它强调了机器人在实际环境中执行任务的能力，是“仿真训练 → 真实部署”流程中的关键一步，尤其与“直接部署”或“人机协同修正”等步骤相关。

【图像 17】(182x278)
1. **图像类型**：  
   该图像是一个三维计算机生成的图形（3D渲染图），展示了一个机械臂在仿真环境中的模型，属于典型的机器人仿真场景截图。

2. **图像中的所有文字内容**：  
   图像中无直接可见的文字内容。文字信息来自页面上下文，与图像本身无关。

3. **图像中的关键视觉元素和数据信息**：  
   - **机械臂结构**：图像中心是一个橙色与灰色相间的多关节机械臂，具有多个旋转关节，呈现典型的工业或协作机器人形态。
   - **末端执行器**：机械臂末端连接一个黑色方形装置，可能是夹爪或传感器模块。
   - **坐标轴指示**：在机械臂附近有绿色和蓝色的线段，分别代表Y轴和Z轴方向，表明这是一个带有坐标系的仿真环境。
   - **地面网格**：背景为浅灰色平面，带有网格线，用于表示三维空间中的位置参考。
   - **光照与阴影**：机械臂投射出清晰的阴影，说明存在定向光源，增强了三维感。

4. **图像传达的主要信息**：  
   该图像展示了机器人在仿真环境中（如Gazebo、Unity或PyBullet等）的建模状态，用于“Sim2Real”（仿真到现实）研究流程中的第一步——在仿真中训练机器人策略。结合上下文，此图可能用于说明“Real2Sim2Real”框架中的“Simulation Policy”阶段，即在高保真度仿真中训练机器人行为，为后续迁移到真实世界做准备。

【图像 18】(200x200)
1. **图像类型**：  
   该图像是一个**图形图标**，具体为一个简化的集成电路（IC）芯片的示意图。

2. **图像中的所有文字内容**：  
   图像中**没有文字内容**。

3. **图像中的关键视觉元素和数据信息**：  
   - 中央是一个蓝色边框的正方形，代表芯片主体。  
   - 正方形四个角各有一个小圆点，可能表示芯片的固定或标记点。  
   - 芯片四周均匀分布着多组短线条（引脚），分别位于上下左右四侧，象征芯片的电连接引脚。  
   - 整体采用扁平化设计风格，颜色为单一蓝色，背景为白色。  

4. **图像传达的主要信息**：  
   该图标用于**象征电子芯片、处理器或计算硬件**，在上下文“Real2Sim2Real”、“Simulation Policy”、“Domain Randomization”等技术术语背景下，可能代表**仿真系统中的核心计算单元**，如用于机器人控制、模拟训练或政策学习的嵌入式处理器。它强调了硬件在仿真到现实（Sim2Real）迁移过程中的基础作用。

【图像 19】(112x103)
由于当前无法查看或解析图像内容（提供的图标仅为占位符，表示图像缺失），无法进行实际的图像分析。以下是基于您提供的**页面文本上下文**和常规学术论文中类似图表的推断性分析：

---

### 1. 图像类型  
**推测为流程图或框架图**（可能包含多个步骤、模块与箭头连接），常见于IEEE/ASME Transactions on Mechatronics类期刊中用于描述“Real2Sim2Real”方法的系统流程。

---

### 2. 图像中的所有文字内容  
**无法提取**（因图像不可见）。但根据上下文可推测图像中可能包含以下关键词：
- Real2Sim
- Scan
- Reconstruct
- Simulation fine-tuning
- Sim2Real transfer
- Simulation Policy
- Direct Deployment
- Human-in-the-Loop Correction
- Successful Transfer with Learned Residual Policy
- Domain Randomization
- Random Scenario Configuration
- Train in the simulator
- Test in the real world
- System Identification
- More Realistic Simulation Environment
- Lang4Sim2Real...

这些术语应分布在流程图的不同阶段或模块中。

---

### 3. 图像中的关键视觉元素和数据信息  
**推测内容如下**：
- **多阶段流程结构**：从“Real2Sim”到“Sim2Real”再到“Real”世界的闭环过程。
- **模块化设计**：包括扫描、重建、仿真训练、策略优化、现实部署等模块。
- **双向箭头或反馈环**：体现“Human-in-the-Loop”或“Learned Residual Policy”的迭代修正机制。
- **分支路径**：如“Train in the simulator → Test in the real world”是否需要额外训练。
- **颜色编码或图标**：可能用不同颜色区分真实世界、仿真环境、策略学习等环节。

---

### 4. 图像传达的主要信息  
该图旨在展示一种**从真实世界到仿真再到真实世界的闭环机器人学习框架（Real2Sim2Real）**，其核心思想是：
1. 利用真实环境数据构建高保真仿真；
2. 在仿真中训练策略并进行域随机化增强泛化能力；
3. 将训练好的策略直接迁移至真实世界，并通过残差策略或人工干预实现高效适应；
4. 实现无需额外训练即可完成跨域迁移的目标。

此方法强调**仿真与现实之间的桥梁建设**，结合系统辨识、领域随机化与人机协同，提升机器人在复杂动态环境下的适应能力。

---

> ⚠️ 注意：以上分析基于上下文文本推测，若需精确解读，请提供原始图像文件或详细描述。

【图像 20】(86x86)
根据您提供的信息，目前无法看到图像本身，仅能依据上下文文本进行分析。以下是基于PDF第14页中第20张图像的上下文内容所作的详细分析：

---

### 1. 图像类型  
**图表（流程图/示意图）**  
从上下文来看，该图像很可能是用于说明“Real2Sim2Real”方法流程的**结构化流程图或概念框架图**，包含多个阶段和模块，具有典型的学术论文中用于展示技术流程的风格。

---

### 2. 图像中的所有文字内容  
根据上下文提取的文字内容如下：  

- **Real2Sim2Real**（主标题）
- **TRANSIC**（可能为项目或方法名称）
- **1. Real2Sim**  
  - Scan  
  - Reconstruct  
- **2. Simulation fine-tuning**
- **3. Sim2Real transfer**
- **1. Simulation Policy**
- **2. Direct Deployment**
- **3. Human-in-the-Loop Correction**
- **4. Successful Transfer with Learned Residual Policy**
- **Domain Randomization**
- **Random Scenario Configuration**
- **1. Train in the simulator**
- **Without Additional Training**
- **2. Test in the real world**
- **System Identification**
- **More Realistic Simulation Environment**
- **1. Train in the simulator**
- **Lang4Sim2Rea...**（被截断，推测为“Lang4Sim2Real”）

> 注：部分文字可能为图中不同模块或步骤的标签。

---

### 3. 图像中的关键视觉元素和数据信息  
尽管无图，但从文本可推断图像包含以下结构元素：

- **分步流程结构**：分为“Real2Sim”、“Simulation fine-tuning”、“Sim2Real transfer”三个主要阶段。
- **模块化设计**：
  - “Scan”与“Reconstruct”属于Real2Sim阶段，表示从真实世界获取数据并重建仿真环境。
  - “Simulation Policy”、“Direct Deployment”、“Human-in-the-Loop Correction”等为Sim2Real转移策略。
  - “Learned Residual Policy”表明使用了学习补偿策略以提升迁移效果。
- **对比路径**：
  - 一条路径是“Train in simulator → Test in real world”，强调无需额外训练即可部署。
  - 另一条路径涉及“System Identification”和“More Realistic Simulation Environment”，旨在提高仿真逼真度。
- **技术关键词**：
  - Domain Randomization（领域随机化）
  - Random Scenario Configuration（随机场景配置）
  - Lang4Sim2Real（可能指语言驱动的仿真到现实迁移）

---

### 4. 图像传达的主要信息  
该图像系统性地展示了**从真实世界到仿真再到真实世界的闭环迁移框架（Real2Sim2Real）**，其核心思想包括：

1. **构建高保真仿真环境**：通过扫描与重建真实场景，实现仿真与现实的对齐。
2. **在仿真中训练策略**：利用域随机化、随机场景配置等手段增强模型鲁棒性。
3. **实现Sim2Real迁移**：提出多种迁移方式，包括直接部署、人工干预修正、以及基于学习残差策略的优化方法。
4. **减少真实世界训练需求**：强调“无需额外训练”即可在真实环境中测试，体现高效性和实用性。

整体上，该图旨在说明一种**结合系统辨识、仿真优化与智能迁移策略的机器人或控制系统开发范式**，适用于复杂任务的跨域部署。

---

⚠️ 注意：以上分析基于文本上下文推断，若需更精确描述，请提供图像本身或更高清的文本内容。

【图像 21】(221x70)
根据您提供的图像内容和上下文信息，以下是对该图像的详细分析：

---

### 1. 图像类型  
**图形/标志（Logo）**  
该图像为一个设计简洁的图形标志，包含文字与图标元素，属于品牌或项目标识类图像。

---

### 2. 图像中的所有文字内容  
- **FiLM**  
  文字以黑色粗体显示，字母“F”和“I”为大写，“L”和“M”也为大写，整体风格现代、醒目。

---

### 3. 图像中的关键视觉元素和数据信息  
- **文字部分**：“FiLM”是核心文本，可能代表某个技术术语、项目名称或方法名。
- **图标部分**：右侧有一个橙黄色火焰图案，象征“火”（Fire），与“FiLM”形成谐音或视觉联想。
- **颜色搭配**：背景为浅黄色，文字为黑色，火焰为橙黄渐变色，整体对比鲜明，突出重点。
- **布局结构**：文字在左，火焰图标在右，水平排列，构成一个统一的视觉单元。

---

### 4. 图像传达的主要信息  
该图像很可能是论文中提出的某种方法或框架的命名标志——“FiLM”（可能为“Fire in the Loop for Mapping”或类似缩写，但具体需结合上下文）。  
结合页面上下文（如“Real2Sim2Real”、“Sim2Real transfer”、“Domain Randomization”等术语），推测“FiLM”可能是用于仿真到现实迁移（Sim2Real）过程中的一种策略、模块或系统，其名称通过“火焰”图标强调了“动态性”、“能量”或“激活”等概念，暗示该方法在训练或部署阶段具有增强适应性或鲁棒性的能力。

> 注：由于图像本身仅展示“FiLM”及其火焰图标，缺乏进一步的技术细节，其完整含义需结合PDF中第14页的正文内容进行确认。但在当前上下文中，它极有可能是一个新提出的方法或系统的视觉标识。

--- 

**总结**：这是一个名为“FiLM”的技术或方法的标志图，通过文字与火焰图标结合，传达一种动态、高效或强化学习相关的理念，可能用于机器人、仿真迁移等领域的研究。

【图像 22】(86x410)
1. **图像类型**：  
   该图像是一个图形化标签或图标，属于**图形元素**，可能用于标注神经网络结构中的某一层（如“最后一层”）。

2. **图像中的所有文字内容**：  
   - “Last Layer”

3. **图像中的关键视觉元素和数据信息**：  
   - 背景为黄色矩形区域。  
   - 文字“Last Layer”以黑色字体垂直排列，位于图像中央偏下位置。  
   - 图像顶部有一个火焰形状的图标，呈橙红色，带有发光效果，象征“激活”、“输出”或“最终处理阶段”。

4. **图像传达的主要信息**：  
   该图像表示神经网络或机器学习模型中的“最后一层”（Last Layer），结合火焰图标，可能强调这一层是关键的输出或决策层，具有重要性或高活跃度。此图可能用于论文中说明模型架构，特别是在Sim2Real（仿真到现实）迁移学习的上下文中，暗示该层在策略输出或残差学习中的作用。

【图像 23】(200x200)
1. **图像类型**：图形（几何标志/图标）

2. **图像中的所有文字内容**：无文字内容

3. **图像中的关键视觉元素和数据信息**：
   - 图像为一个绿色的几何图形，由多个圆形节点和连接线构成，形成一个立方体框架结构。
   - 立方体中心有一个六边形内嵌的三维立体符号（类似三棱柱或立方体投影）。
   - 整体设计具有对称性，线条简洁，风格现代，可能象征技术、系统结构或仿真环境。

4. **图像传达的主要信息**：
   - 该图像是一个抽象的标志或图标，可能代表“Real2Sim2Real”框架中的核心概念，如仿真与现实之间的映射关系、系统结构或域随机化（Domain Randomization）的拓扑结构。
   - 结合上下文（IEEE/ASME TRANSACTIONS ON MECHATRONICS 中关于 Sim-to-Real 转移的研究），此图标可能象征模拟环境与真实世界之间的连接与转换机制，强调结构化、模块化的系统设计。


================================================================================
第 15 页
================================================================================

【文本内容】
IEEE/ASME TRANSACTIONS ON MECHATRONICS
15
Fig. 12.
Exemplar tasks from ARIO, where the top row indicates the task
category while the text at the bottom row provides task instructions.
tion [234], [235] creates accurate simulations of real-world
scenes to ensure smooth transitions from simulation to reality.
Lang4sim2real [236] leverages natural language descriptions
to bridge the sim-to-real gap, improving model generalization
with cross-domain image representations.
4) ARIO (All Robots in One): Despite the seemingly unified
structure of pre-training datasets like Open X-embodiment,
several critical issues remain unresolved. These issues include
the absence of comprehensive sensory modalities—no dataset
currently integrates images, 3D vision, text, tactile, and audi-
tory inputs simultaneously. Additionally, the lack of a unified
format in multi-robot datasets complicates data processing and
loading. Furthermore, there is an incompatibility in represent-
ing diverse control objects across different robotic platforms,
insufficient data volume that hinders large-scale pretraining,
and a scarcity of datasets that combine both simulated and real
data, which is essential for addressing the sim-to-real gap.
To overcome these challenges, we propose ARIO (All
Robots In One) , which is a new dataset standard that optimizes
existing datasets and facilitates the development of more versa-
tile and general-purpose embodied agents. The ARIO standard
[237] records control and motion data from robots with
different morphologies in a unified format. ARIO’s unified
format accommodates variable data from diverse robot types,
ensuring precise timestamps. This standard enables users to
efficiently train high-performing, generalizable embodied AI
models, positioning ARIO as the ideal format for embodied
AI datasets. Building on the ARIO standard, a unified large-
scale ARIO dataset is further developed, which comprises
approximately 3 million episodes collected from 258 series
and 321,064 tasks. Fig. 12 shows exemplar tasks from ARIO.
The ARIO dataset addresses the limitations of current
datasets and facilitates the development of robust, general-
purpose embodied agents. By providing a cohesive framework
for data collection and representation, ARIO paves the way
for the development of more powerful and versatile embodied
agents, capable of navigating and interacting with the physical
world in complex and diverse ways.
5) Real-world Deployments of Embodied AI Systems:
Embodied AI systems have made significant strides across
various scenes. In healthcare, robots like the Da Vinci Surgical
System and Moxi automate tasks such as surgery precision and
supply deliveries. In logistics, Amazon Robotics and Boston
Dynamics’ Stretch improve efficiency in warehousing and
transportation. Manufacturing benefits from AI-driven robots
like Fanuc and ABB, enhancing precision and collaboration.
Nevertheless, sim-to-real adaptation faces significant chal-
lenges, including the domain gap between simulation and real-
world data distributions, the complexity of dynamic real-world
interactions, and the limited diversity of training data. Models
often overfit to simulations, struggle with real-world sensor
noise, and fail to handle unexpected events.
VIII. CHALLENGES AND FUTURE DIRECTIONS
Despite of the rapid progress of embodied AI, it faces
several challenges and presents exciting future directions.
High-quality Robotic Datasets: Obtaining sufficient real
world robotic data remains a significant challenge. Collecting
this data is both time-consuming and resource-intensive. Rely-
ing solely on simulation data worsens the sim-to-real gap prob-
lem. Creating diverse real world robotic datasets necessitates
close and extensive collaboration among various institutions.
Additionally, the development of more realistic and efficient
simulators is essential for improving the quality of simulated
data. For building generalizable embodied models capable of
cross-scenario and cross-task applications in robotics, it is
essential to construct large-scale datasets, leveraging high-
quality simulated environment data to assist real world data.
Long-Horizon Task Execution: Executing single instruc-
tions can often entail long-horizon tasks for robots, exempli-
fied by commands like “clean the kitchen”, which involve
activities such as rearranging objects, sweeping floors, wip-
ing tables, and more. Accomplishing such tasks successfully
necessitates the robot’s ability to plan and execute a sequence
of low-level actions over extended time spans. While current
high-level task planners have shown initial success, they often
prove inadequate in diverse scenarios due to their lack of
tuning for embodied tasks. Addressing this challenge requires
the development of efficient planners equipped with robust
perception capabilities and much commonsense knowledge.
To balance the trade-off between planning complexity and
real-time adaptability, we can combine lightweight monitor
modules for high-frequency monitoring, and two adapters for
subtask and path adaptation reasoning at a lower frequency.
Causal Reasoning: Existing data-driven embodied agents
make decisions based on intrinsic data correlations. However,
this approach does not allow the models to truly understand
the causal relations between knowledge, behavior, and envi-
ronment, resulting in biased strategies. This makes it difficult
to ensure that they can operate in real-world environments in
a robust and reliable manner. Therefore, it is important for
embodied agents to be driven by world knowledge, capable
of autonomous causal reasoning. By understanding the world
through interaction and learning its workings via abductive
reasoning, we can further enhance the adaptability, decision
reliability, and generalization capabilities of embodied agents
in complex real-world environments. For embodied tasks, it is
necessary to establish spatial-temporal causal relations across
modalities through interactive instructions and state predic-
tions. Moreover, agents need to understand the affordances of
objects to achieve adaptive task planning in dynamic scenes.
Unified Evaluation Benchmark: While numerous bench-
marks exist for evaluating low-level control policies, they vary


【图像 1】(1947x924)
1. **图像类型**：  
   该图像是由三张并列的**照片**组成的复合图像，每张照片展示了一个机器人执行任务的场景。整体属于**示例性任务展示图（Exemplar tasks）**，用于说明不同类型的机器人操作任务。

2. **图像中的所有文字内容**：  
   - 左侧上方标题：**Long-horizon task**  
     下方指令：  
     *"Pick up the rice paper, water dish, brush pen, and calligraphy copy, and place them in sequence at the top right, bottom right, bottom left, and top left corner of the table."*  

   - 中间上方标题：**Bimanual manipulation**  
     下方指令：  
     *“Pick up the open book on the table with both hands for reading.”*  

   - 右侧上方标题：**Contact-rich task**  
     下方指令：  
     *"Pick up the pencil on the table and write down the number '3' on the paper."*

3. **图像中的关键视觉元素和数据信息**：  
   - **左侧图像（Long-horizon task）**：  
     - 一台机械臂正在操作桌面上的物品，包括毛笔、砚台、宣纸等书法工具。  
     - 任务涉及多个步骤和物体的顺序摆放，体现长期规划能力。  
   - **中间图像（Bimanual manipulation）**：  
     - 两个机械臂协同操作一本打开的书，模拟双臂抓取动作。  
     - 强调双手协调与精细操作能力。  
   - **右侧图像（Contact-rich task）**：  
     - 一个机械臂手持铅笔，在纸上书写数字“3”。  
     - 任务需要持续接触和精细控制，体现高精度交互能力。  

4. **图像传达的主要信息**：  
   该图展示了ARIO（All Robots in One）框架中三种典型机器人任务类型：  
   - **长时序任务（Long-horizon task）**：涉及多步骤、多物体的序列化操作。  
   - **双臂操作（Bimanual manipulation）**：强调双臂协同完成复杂动作。  
   - **高接触密度任务（Contact-rich task）**：要求精确的触觉控制与连续交互。  
   图像旨在说明ARIO系统能够支持多样化、复杂的人机交互任务，涵盖从简单抓取到精细操作的广泛场景。


================================================================================
第 16 页
================================================================================

【文本内容】
IEEE/ASME TRANSACTIONS ON MECHATRONICS
16
significantly in the assessed skills. Furthermore, the objects
and scenes in these benchmarks are typically limited by
simulator constraints. To comprehensively evaluate embodied
models, the benchmark should encompass a diverse range of
skills using realistic simulators. Many benchmarks for high-
level task planners focus on assessing planning capability
through question-answering tasks. However, a more desirable
approach involves evaluating both the high-level task planner
and the low-level control policy together for executing long-
horizon tasks and measuring success rates, rather than relying
solely on isolated assessments of the planner.
Security and Privacy: Embodied agents face significant
security challenges when deployed in sensitive or private
spaces. These agents often rely on LLMs for decision-making,
which introduces new vulnerabilities. For instance, LLMs are
susceptible to backdoor attacks like word injection, scenario
manipulation, and knowledge injection, which can lead to
dangerous outcomes such as autonomous vehicles accelerating
towards obstacles or robots performing hazardous actions. To
mitigate these risks, we can evaluate potential attack vectors
and develop more robust defenses. Additionally, the secure
prompting, state management, and safety validation mecha-
nisms can be used to enhance the security and robustness.
IX. CONCLUSION
Embodied AI allows agents to sense, perceive, and interact
with various objects from both cyber space and physical world,
which exhibits its vital significance toward achieving AGI.
This survey extensively reviews embodied robots, simulators,
four representative embodied tasks: visual active perception,
embodied interaction, embodied agents and sim-to-real adapta-
tion, and future research directions. The comparative summary
of the embodied robots, simulators, datasets, and approaches
provides a clear picture of the recent development in embodied
AI, which greatly benefits the future research along this
emerging and promising research direction.
REFERENCES
[1] C. Machinery, “Computing machinery and intelligence-am turing,”
Mind, vol. 59, no. 236, p. 433, 1950.
[2] B. Zitkovich, T. Yu, S. Xu, P. Xu, T. Xiao, F. Xia, J. Wu, P. Wohlhart,
S. Welker, A. Wahid et al., “Rt-2: Vision-language-action models
transfer web knowledge to robotic control,” in CoRL, 2023, pp. 2165–
2183.
[3] S. Belkhale, T. Ding, T. Xiao, P. Sermanet, Q. Vuong, J. Tompson,
Y. Chebotar, D. Dwibedi, and D. Sadigh, “Rt-h: Action hierarchies
using language,” arXiv preprint arXiv:2403.01823, 2024.
[4] T. Wu, S. He, J. Liu, S. Sun, K. Liu, Q.-L. Han, and Y. Tang, “A
brief overview of chatgpt: The history, status quo and potential future
development,” IEEE/CAA Journal of Automatica Sinica, vol. 10, no. 5,
pp. 1122–1136, 2023.
[5] J. Duan, S. Yu, H. L. Tan, H. Zhu, and C. Tan, “A survey of embodied
ai: From simulators to research tasks,” IEEE Transactions on Emerging
Topics in Computational Intelligence, vol. 6, no. 2, pp. 230–244, 2022.
[6] Y. Ma, Z. Song, Y. Zhuang, J. Hao, and I. King, “A survey
on vision-language-action models for embodied ai,” arXiv preprint
arXiv:2405.14093, 2024.
[7] H. Gao, Y. Liu, W. Sun, and X. Yu, “Adaptive wavelet tracking control
of dual-linear-motor-driven gantry stage with suppression of crossbeam
rotation,” IEEE/ASME TMECH, 2023.
[8] Y. Chen, W. Cui, Y. Chen, M. Tan, X. Zhang, D. Zhao, and H. Wang,
“Robogpt: an intelligent agent of making embodied long-term decisions
for daily instruction tasks,” arXiv preprint arXiv:2311.15649, 2023.
[9] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn,
K. Gopalakrishnan, K. Hausman, A. Herzog, J. Hsu et al., “Rt-1:
Robotics transformer for real-world control at scale,” arXiv preprint
arXiv:2212.06817, 2022.
[10] B. Zitkovich, T. Yu, S. Xu, P. Xu, T. Xiao, F. Xia, J. Wu, P. Wohlhart,
S. Welker, A. Wahid et al., “Rt-2: Vision-language-action models
transfer web knowledge to robotic control,” in CoRL, 2023, pp. 2165–
2183.
[11] Y. Hu, Q. Xie, V. Jain, J. Francis, J. Patrikar, N. Keetha, S. Kim,
Y. Xie, T. Zhang, Z. Zhao et al., “Toward general-purpose robots
via foundation models: A survey and meta-analysis,” arXiv preprint
arXiv:2312.08782, 2023.
[12] R. McCarthy, D. C. Tan, D. Schmidt, F. Acero, N. Herr, Y. Du, T. G.
Thuruthel, and Z. Li, “Towards generalist robot learning from internet
video: A survey,” arXiv preprint arXiv:2404.19664, 2024.
[13] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal,
G. Sastry, A. Askell, P. Mishkin, J. Clark et al., “Learning transferable
visual models from natural language supervision,” in ICML.
PMLR,
2021, pp. 8748–8763.
[14] J. Li, D. Li, S. Savarese, and S. Hoi, “Blip-2: Bootstrapping language-
image pre-training with frozen image encoders and large language
models,” in ICML.
PMLR, 2023, pp. 19 730–19 742.
[15] M. Assran, Q. Duval, I. Misra, P. Bojanowski, P. Vincent, M. Rabbat,
Y. LeCun, and N. Ballas, “Self-supervised learning from images with a
joint-embedding predictive architecture,” in CVPR, 2023, pp. 15 619–
15 629.
[16] Z. Zhu, X. Wang, W. Zhao, C. Min, N. Deng, M. Dou, Y. Wang,
B. Shi, K. Wang, C. Zhang et al., “Is sora a world simulator? a
comprehensive survey on general world models and beyond,” arXiv
preprint arXiv:2405.03520, 2024.
[17] R. Pfeifer and F. Iida, “Embodied artificial intelligence: Trends and
challenges,” Lecture notes in computer science, pp. 1–26, 2004.
[18] L. Ren, J. Dong, S. Liu, L. Zhang, and L. Wang, “Embodied intelli-
gence toward future smart manufacturing in the era of ai foundation
model,” IEEE/ASME TMECH, pp. 1–11, 2024.
[19] Z. Liao, G. Jiang, F. Zhao, Y. Wu, Y. Yue, and X. Mei, “Dynamic skill
learning from human demonstration based on the human arm stiffness
estimation model and riemannian dmp,” IEEE/ASME TMECH, vol. 28,
no. 2, pp. 1149–1160, 2023.
[20] X. Zhao, Y. Zhang, W. Ding, B. Tao, and H. Ding, “A dual-arm
robot cooperation framework based on a nonlinear model predictive
cooperative control,” IEEE/ASME TMECH, vol. 29, no. 5, pp. 3993–
4005, 2024.
[21] C. Li, F. Liu, Y. Wang, and M. Buss, “Data-informed residual re-
inforcement learning for high-dimensional robotic tracking control,”
IEEE/ASME TMECH, pp. 1–11, 2024.
[22] V. Ortenzi, N. Marturi, M. Mistry, J. Kuo, and R. Stolkin, “Vision-based
framework to estimate robot configuration and kinematic constraints,”
IEEE/ASME TMECH, vol. 23, no. 5, pp. 2402–2412, 2018.
[23] S. K. Kommuri, S. Han, and S. Lee, “External torque estimation
using higher order sliding-mode observer for robot manipulators,”
IEEE/ASME TMECH, vol. 27, no. 1, pp. 513–523, 2022.
[24] E. Spyrakos-Papastavridis, P. R. N. Childs, and J. S. Dai, “Passivity
preservation for variable impedance control of compliant robots,”
IEEE/ASME TMECH, vol. 25, no. 5, pp. 2342–2353, 2020.
[25] P. R. Wurman, R. D’Andrea, and M. Mountz, “Coordinating hundreds
of cooperative, autonomous vehicles in warehouses,” AI magazine,
vol. 29, no. 1, pp. 9–9, 2008.
[26] Z. Shao and J. Zhang, “Vision-based adaptive trajectory tracking
control of wheeled mobile robot with unknown translational external
parameters,” IEEE/ASME TMECH, vol. 29, no. 1, pp. 358–365, 2024.
[27] D. A. Bennett, S. A. Dalley, D. Truex, and M. Goldfarb, “A multi-
grasp hand prosthesis for providing precision and conformal grasps,”
IEEE/ASME TMECH, vol. 20, no. 4, pp. 1697–1704, 2015.
[28] W. Chen, C. Xiong, and S. Yue, “Mechanical implementation of kine-
matic synergy for continual grasping generation of anthropomorphic
hand,” IEEE/ASME TMECH, vol. 20, no. 3, pp. 1249–1263, 2015.
[29] J. Xiang, T. Tao, Y. Gu, T. Shu, Z. Wang, Z. Yang, and Z. Hu,
“Language models meet world models: Embodied experiences enhance
language models,” NeurIPS, vol. 36, 2024.
[30] B. Siciliano, O. Khatib, and T. Kr¨oger, Springer handbook of robotics.
Springer, 2008, vol. 200.
[31] Y. Yang, Z. He, P. Jiao, and H. Ren, “Bioinspired soft robotics: How do
we learn from creatures?” IEEE Reviews in Biomedical Engineering,
2022.



================================================================================
第 17 页
================================================================================

【文本内容】
IEEE/ASME TRANSACTIONS ON MECHATRONICS
17
[32] R. K. Katzschmann, J. DelPreto, R. MacCurdy, and D. Rus, “Explo-
ration of underwater life with an acoustically controlled soft robotic
fish,” Science Robotics, vol. 3, no. 16, p. eaar3449, 2018.
[33] G. C. de Croon, J. Dupeyroux, S. B. Fuller, and J. A. Marshall, “Insect-
inspired ai for autonomous robots,” Science robotics, vol. 7, no. 67, p.
eabl6334, 2022.
[34] N. R. Sinatra, C. B. Teeple, D. M. Vogt, K. K. Parker, D. F. Gruber,
and R. J. Wood, “Ultragentle manipulation of delicate structures using
a soft robotic gripper,” Science Robotics, vol. 4, no. 33, p. eaax5425,
2019.
[35] G. Authors, “Genesis: A universal and generative physics engine
for robotics and beyond,” December 2024. [Online]. Available:
https://github.com/Genesis-Embodied-AI/Genesis
[36] NVIDIA, “Nvidia isaac sim: Robotics simulation and synthetic data,”
2023. [Online]. Available: https://developer.nvidia.com/isaac-sim
[37] V. Makoviychuk, L. Wawrzyniak, Y. Guo, M. Lu, K. Storey, M. Mack-
lin, D. Hoeller, N. Rudin, A. Allshire, A. Handa et al., “Isaac gym:
High performance gpu-based physics simulation for robot learning,”
arXiv preprint arXiv:2108.10470, 2021.
[38] N. Koenig and A. Howard, “Design and use paradigms for gazebo, an
open-source multi-robot simulator,” in IROS, vol. 3, 2004, pp. 2149–
2154.
[39] E. Coumans and Y. Bai, “Pybullet, a python module for physics
simulation for games, robotics and machine learning,” 2016.
[40] Cyberbotics,
“Webots:
open-source
robot
simulator.”
[Online].
Available: https://github.com/cyberbotics/webots
[41] E. Todorov, T. Erez, and Y. Tassa, “Mujoco: A physics engine for
model-based control,” in IROS, 2012, pp. 5026–5033.
[42] A. Juliani, V.-P. Berges, E. Teng, A. Cohen, J. Harper, C. Elion, C. Goy,
Y. Gao, H. Henry, M. Mattar, and D. Lange, “Unity: A general platform
for intelligent agents,” arXiv preprint arXiv:1809.02627, 2020.
[43] S. Shah, D. Dey, C. Lovett, and A. Kapoor, “Airsim: High-fidelity
visual and physical simulation for autonomous vehicles,” in Field and
Service Robotics, 2017.
[44] ISAE-SUPAERO, “Morse: the modular open robots simulator engine.”
[45] E. Rohmer, S. P. Singh, and M. Freese, “V-rep: A versatile and scalable
robot simulation framework,” in IROS, 2013, pp. 1321–1326.
[46] C. Wang, Q. Zhang, Q. Tian, S. Li, X. Wang, D. Lane, Y. Petillot, and
S. Wang, “Learning mobile manipulation through deep reinforcement
learning,” Sensors, vol. 20, no. 3, p. 939, 2020.
[47] N. Koenig and A. Howard, “Design and use paradigms for gazebo, an
open-source multi-robot simulator,” in IROS, vol. 3, 2004, pp. 2149–
2154.
[48] F. Xiang, Y. Qin, K. Mo, Y. Xia, H. Zhu, F. Liu, M. Liu, H. Jiang,
Y. Yuan, H. Wang, L. Yi, A. X. Chang, L. J. Guibas, and H. Su,
“Sapien: A simulated part-based interactive environment,” in CVPR,
Jun 2020.
[49] X. Puig, K. Ra, M. Boben, J. Li, T. Wang, S. Fidler, and A. Torralba,
“Virtualhome: Simulating household activities via programs,” in CVPR,
Jun 2018.
[50] E. Kolve, R. Mottaghi, D. Gordon, Y. Zhu, A. Gupta, and A. Farhadi,
“Ai2-thor: An interactive 3d environment for visual ai,” arXiv: Com-
puter Vision and Pattern Recognition,arXiv: Computer Vision and
Pattern Recognition, Dec 2017.
[51] B. Shen, F. Xia, C. Li, R. Mart´ın-Mart´ın, L. Fan, G. Wang, C. P´erez-
D’Arpino, S. Buch, S. Srivastava, L. Tchapmi, M. Tchapmi, K. Vainio,
J. Wong, L. Fei-Fei, and S. Savarese, “igibson 1.0: A simulation
environment for interactive tasks in large realistic scenes,” in IROS,
2021, pp. 7520–7527.
[52] C. Gan, J. Schwartz, S. Alter, M. Schrimpf, J. Traer, J. Freitas, J. Kubil-
ius, A. Bhandwaldar, N. Haber, M. Sano, K. Kim, E. Wang, D. Mrowca,
M. Lingelbach, A. Curtis, K. Feigelis, D. Bear, D. Gutfreund, D. Cox,
J. DiCarlo, J. McDermott, J. Tenenbaum, and D. Yamins, “Threed-
world: A platform for interactive multi-modal physical simulation,”
NeurIPS, Dec 2021.
[53] A. Chang, A. Dai, T. Funkhouser, M. Halber, M. Niebner, M. Savva,
S. Song, A. Zeng, and Y. Zhang, “Matterport3d: Learning from rgb-d
data in indoor environments,” in 3DV, Oct 2017.
[54] P. Ren, M. Li, Z. Luo, X. Song, Z. Chen, W. Liufu, Y. Yang,
H. Zheng, R. Xu, Z. Huang et al., “Infiniteworld: A unified scalable
simulation framework for general visual-language robot interaction,”
arXiv preprint arXiv:2412.05789, 2024.
[55] Y. Wang, Z. Xian, F. Chen, T.-H. Wang, Y. Wang, K. Fragkiadaki,
Z. Erickson, D. Held, and C. Gan, “Robogen: Towards unleashing
infinite data for automated robot learning via generative simulation,”
arXiv:2311.01455, Nov 2023.
[56] Y. Yang, F.-Y. Sun, L. Weihs, E. VanderBilt, A. Herrasti, W. Han,
J. Wu, N. Haber, R. Krishna, L. Liu et al., “Holodeck: Language
guided generation of 3d embodied ai environments,” in CVPR, 2024,
pp. 16 227–16 237.
[57] Y. Yang, B. Jia, P. Zhi, and S. Huang, “Physcene: Physically inter-
actable 3d scene synthesis for embodied ai,” in CVPR, 2024.
[58] M. Deitke, E. VanderBilt, A. Herrasti, L. Weihs, J. Salvador, K. Ehsani,
W. Han, E. Kolve, A. Farhadi, A. Kembhavi, and R. Mottaghi,
“ProcTHOR: Large-Scale Embodied AI Using Procedural Generation,”
in NeurIPS, 2022, outstanding Paper Award.
[59] L. Fei-Fei and R. Krishna, “Searching for computer vision north stars,”
Daedalus, vol. 151, no. 2, pp. 85–99, 2022.
[60] A. J. Davison, I. D. Reid, N. D. Molton, and O. Stasse, “Monoslam:
Real-time single camera slam,” IEEE TPAMI, vol. 29, no. 6, pp. 1052–
1067, 2007.
[61] R. Mur-Artal, J. M. M. Montiel, and J. D. Tardos, “Orb-slam: a versatile
and accurate monocular slam system,” IEEE TRO, vol. 31, no. 5, pp.
1147–1163, 2015.
[62] J. Engel, T. Sch¨ops, and D. Cremers, “Lsd-slam: Large-scale direct
monocular slam,” in ECCV.
Springer, 2014, pp. 834–849.
[63] R. F. Salas-Moreno, R. A. Newcombe, H. Strasdat, P. H. Kelly, and
A. J. Davison, “Slam++: Simultaneous localisation and mapping at the
level of objects,” in CVPR, 2013, pp. 1352–1359.
[64] L. Nicholson, M. Milford, and N. S¨underhauf, “Quadricslam: Dual
quadrics from object detections as landmarks in object-oriented slam,”
IEEE RAL, vol. 4, no. 1, pp. 1–8, 2018.
[65] Z. Liao, Y. Hu, J. Zhang, X. Qi, X. Zhang, and W. Wang, “So-slam:
Semantic object slam with scale proportional and symmetrical texture
constraints,” IEEE RAL, vol. 7, no. 2, pp. 4008–4015, 2022.
[66] S. Cheng, C. Sun, S. Zhang, and D. Zhang, “Sg-slam: A real-time
rgb-d visual slam toward dynamic scenes with semantic and geometric
information,” IEE TIM, vol. 72, pp. 1–12, 2022.
[67] J. He, M. Li, Y. Wang, and H. Wang, “Ovd-slam: An online visual
slam for dynamic environments,” IEEE Sensors Journal, 2023.
[68] C. Yan, D. Qu, D. Xu, B. Zhao, Z. Wang, D. Wang, and X. Li, “Gs-
slam: Dense visual slam with 3d gaussian splatting,” in CVPR, 2024,
pp. 19 595–19 604.
[69] X. Chen, H. Ma, J. Wan, B. Li, and T. Xia, “Multi-view 3d object
detection network for autonomous driving,” in CVPR, 2017, pp. 1907–
1915.
[70] A. H. Lang, S. Vora, H. Caesar, L. Zhou, J. Yang, and O. Beijbom,
“Pointpillars: Fast encoders for object detection from point clouds,” in
CVPR, 2019, pp. 12 697–12 705.
[71] H. Su, S. Maji, E. Kalogerakis, and E. Learned-Miller, “Multi-view
convolutional neural networks for 3d shape recognition,” in ICCV,
2015, pp. 945–953.
[72] D. Maturana and S. Scherer, “Voxnet: A 3d convolutional neural
network for real-time object recognition,” in IROS, 2015, pp. 922–928.
[73] S. Song, F. Yu, A. Zeng, A. X. Chang, M. Savva, and T. Funkhouser,
“Semantic scene completion from a single depth image,” in CVPR,
2017, pp. 1746–1754.
[74] C. Choy, J. Gwak, and S. Savarese, “4d spatio-temporal convnets:
Minkowski convolutional neural networks,” in CVPR, 2019, pp. 3075–
3084.
[75] B. Graham, M. Engelcke, and L. Van Der Maaten, “3d semantic
segmentation with submanifold sparse convolutional networks,” in
CVPR, 2018, pp. 9224–9232.
[76] T. Wang, X. Mao, C. Zhu, R. Xu, R. Lyu, P. Li, X. Chen, W. Zhang,
K. Chen, T. Xue et al., “Embodiedscan: A holistic multi-modal 3d
perception suite towards embodied ai,” in CVPR, 2024, pp. 19 757–
19 767.
[77] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, “Pointnet: Deep learning on
point sets for 3d classification and segmentation,” in CVPR, 2017, pp.
652–660.
[78] C. R. Qi, L. Yi, H. Su, and L. J. Guibas, “Pointnet++: Deep hierarchical
feature learning on point sets in a metric space,” NeurIPS, vol. 30, 2017.
[79] X. Ma, C. Qin, H. You, H. Ran, and Y. Fu, “Rethinking network design
and local geometry in point cloud: A simple residual mlp framework,”
arXiv preprint arXiv:2202.07123, 2022.
[80] H. Zhao, L. Jiang, J. Jia, P. H. Torr, and V. Koltun, “Point transformer,”
in ICCV, 2021, pp. 16 259–16 268.
[81] Y.-Q. Yang, Y.-X. Guo, J.-Y. Xiong, Y. Liu, H. Pan, P.-S. Wang,
X. Tong, and B. Guo, “Swin3d: A pretrained transformer backbone
for 3d indoor scene understanding,” arXiv preprint arXiv:2304.06906,
2023.



================================================================================
第 18 页
================================================================================

【文本内容】
IEEE/ASME TRANSACTIONS ON MECHATRONICS
18
[82] X. Wu, Y. Lao, L. Jiang, X. Liu, and H. Zhao, “Point transformer
v2: Grouped vector attention and partition-based pooling,” NeurIPS,
vol. 35, pp. 33 330–33 342, 2022.
[83] Z. Zhu, X. Ma, Y. Chen, Z. Deng, S. Huang, and Q. Li, “3d-vista: Pre-
trained transformer for 3d vision and text alignment,” in ICCV, 2023,
pp. 2911–2921.
[84] J. Huang, S. Yong, X. Ma, X. Linghu, P. Li, Y. Wang, Q. Li, S.-C. Zhu,
B. Jia, and S. Huang, “An embodied generalist agent in 3d world,” in
ICML, 2024.
[85] Z. Zhu, Z. Zhang, X. Ma, X. Niu, Y. Chen, B. Jia, Z. Deng, S. Huang,
and Q. Li, “Unifying 3d vision-language understanding via promptable
queries,” arXiv preprint arXiv:2405.11442, 2024.
[86] D. Liang, X. Zhou, X. Wang, X. Zhu, W. Xu, Z. Zou, X. Ye, and X. Bai,
“Pointmamba: A simple state space model for point cloud analysis,”
arXiv preprint arXiv:2402.10739, 2024.
[87] X. Han, Y. Tang, Z. Wang, and X. Li, “Mamba3d: Enhancing local
features for 3d point cloud analysis via state space model,” arXiv
preprint arXiv:2404.14966, 2024.
[88] L. Pinto, D. Gandhi, Y. Han, Y.-L. Park, and A. Gupta, “The curious
robot: Learning visual representations via physical interactions,” in
ECCV, 2016, pp. 3–18.
[89] G. Tatiya, J. Francis, and J. Sinapov, “Transferring implicit knowledge
of non-visual object properties across heterogeneous robot morpholo-
gies,” in ICRA, 2023, pp. 11 315–11 321.
[90] D. Jayaraman and K. Grauman, “Learning to look around: Intelligently
exploring unseen environments for unknown tasks,” in CVPR, 2018, pp.
1238–1247.
[91] L. Jin, X. Chen, J. R¨uckin, and M. Popovi´c, “Neu-nbv: Next best
view planning using uncertainty estimation in image-based neural
rendering,” in IROS, 2023, pp. 11 305–11 312.
[92] Y. Hu, J. Geng, C. Wang, J. Keller, and S. Scherer, “Off-policy
evaluation with online adaptation for robot exploration in challenging
environments,” IEEE RAL, 2023.
[93] L. Fan, M. Liang, Y. Li, G. Hua, and Y. Wu, “Evidential active
recognition: Intelligent and prudent open-world embodied perception,”
in CVPR, 2024, pp. 16 351–16 361.
[94] S. Mokssit, D. B. Licea, B. Guermah, and M. Ghogho, “Deep learning
techniques for visual slam: A survey,” IEEE Access, vol. 11, pp.
20 026–20 050, 2023.
[95] K. Chen, J. Zhang, J. Liu, Q. Tong, R. Liu, and S. Chen, “Semantic vi-
sual simultaneous localization and mapping: A survey,” arXiv preprint
arXiv:2209.06428, 2022.
[96] P. K. Vinodkumar, D. Karabulut, E. Avots, C. Ozcinar, and G. Anbar-
jafari, “A survey on deep learning based segmentation, detection and
classification for 3d point clouds,” Entropy, vol. 25, no. 4, p. 635, 2023.
[97] H. Durrant-Whyte and T. Bailey, “Simultaneous localization and map-
ping: part i,” IEEE robotics & automation magazine, vol. 13, no. 2,
pp. 99–110, 2006.
[98] X. Zhang, J. Lai, D. Xu, H. Li, and M. Fu, “2d lidar-based slam
and path planning for indoor rescue using mobile robots,” Journal of
Advanced Transportation, vol. 2020, no. 1, p. 8867937, 2020.
[99] J. Ruan, B. Li, Y. Wang, and Z. Fang, “Gp-slam+: real-time 3d
lidar slam based on improved regionalized gaussian process map
reconstruction,” in IROS, 2020, pp. 5171–5178.
[100] J. Luo, Y. Liu, W. Chen, Z. Li, Y. Wang, G. Li, and L. Lin, “Dspnet:
Dual-vision scene perception for robust 3d question answering,” in
Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, 2025.
[101] Z. Wei, J. Lin, Y. Liu, W. Chen, J. Luo, G. Li, and L. Lin, “3dafford-
splat: Efficient affordance reasoning with 3d gaussians,” arXiv preprint
arXiv:2504.11218, 2025.
[102] V. Mittal, “Attngrounder: Talking to cars with attention,” in ECCV.
Springer, 2020, pp. 62–73.
[103] X. Wang, Q. Huang, A. Celikyilmaz, J. Gao, D. Shen, Y.-F. Wang,
W. Y. Wang, and L. Zhang, “Reinforced cross-modal matching and
self-supervised imitation learning for vision-language navigation,” in
CVPR, 2019, pp. 6629–6638.
[104] C. Bermejo, L. H. Lee, P. Chojecki, D. Przewozny, and P. Hui,
“Exploring button designs for mid-air interaction in virtual reality: A
hexa-metric evaluation of key representations and multi-modal cues,”
Proceedings of the ACM on Human-Computer Interaction, vol. 5, no.
EICS, pp. 1–26, 2021.
[105] P. Anderson, Q. Wu, D. Teney, J. Bruce, M. Johnson, N. Sunderhauf,
I. Reid, S. Gould, and A. van den Hengel, “Vision-and-language
navigation: Interpreting visually-grounded navigation instructions in
real environments,” in CVPR, Jun 2018.
[106] V. Jain, G. Magalhaes, A. Ku, A. Vaswani, E. Ie, and J. Baldridge, “Stay
on the path: Instruction fidelity in vision-and-language navigation,” in
ACL, Jan 2019.
[107] J. Krantz, E. Wijmans, A. Majumdar, D. Batra, and S. Lee, Beyond the
Nav-Graph: Vision-and-Language Navigation in Continuous Environ-
ments, Jan 2020, p. 104–120.
[108] H. Chen, A. Suhr, D. Misra, N. Snavely, and Y. Artzi, “Touchdown:
Natural language navigation and spatial reasoning in visual street
environments,” in CVPR, Jun 2019.
[109] Y. Qi, Q. Wu, P. Anderson, X. Wang, W. Y. Wang, C. Shen, and
A. van den Hengel, “Reverie: Remote embodied visual referring
expression in real indoor environments,” in CVPR, Jun 2020.
[110] F. Zhu, X. Liang, Y. Zhu, Q. Yu, X. Chang, and X. Liang, “Soon:
Scenario oriented object navigation with graph-based exploration,” in
CVPR, Jun 2021.
[111] H. Wang, A. G. H. Chen, X. Li, M. Wu, and H. Dong, “Find what you
want: Learning demand-conditioned object attribute space for demand-
driven navigation,” NeurIPS, 2023.
[112] M. Shridhar, J. Thomason, D. Gordon, Y. Bisk, W. Han, R. Mottaghi,
L. Zettlemoyer, and D. Fox, “Alfred: A benchmark for interpreting
grounded instructions for everyday tasks,” in CVPR, Jun 2020.
[113] S. Yenamandra, A. Ramachandran, K. Yadav, A. Wang, M. Khanna,
T. Gervet, T.-Y. Yang, V. Jain, A. Clegg, J. Turner, Z. Kira, M. Savva,
A. Chang, D. Chaplot, D. Batra, R. Mottaghi, Y. Bisk, and C. Paxton,
“Homerobot: Open-vocabulary mobile manipulation,” in NeurIPS, Jun
2023.
[114] C. Li, R. Zhang, J. Wong, C. Gokmen, S. Srivastava, R. Mart´ın-Mart´ın,
C. Wang, G. Levine, M. Lingelbach, J. Sun et al., “Behavior-1k: A
benchmark for embodied ai with 1,000 everyday activities and realistic
simulation,” in CoRL.
PMLR, 2023, pp. 80–93.
[115] J. Thomason, M. Murray, M. Cakmak, and L. Zettlemoyer, “Vision-
and-dialog navigation,” in CoRL.
PMLR, 2020, pp. 394–406.
[116] X. Gao, Q. Gao, R. Gong, K. Lin, G. Thattai, and G. Sukhatme, “Di-
alfred: Dialogue-enabled agents for embodied instruction following,”
arXiv:2202.13330, 2022.
[117] Y. Hong, C. Rodriguez, Y. Qi, Q. Wu, and S. Gould, “Language and
visual entity relationship graph for agent navigation,” NeurIPS, vol. 33,
pp. 7685–7696, 2020.
[118] W. Zhang, C. Ma, Q. Wu, and X. Yang, “Language-guided navigation
via cross-modal grounding and alternate adversarial learning,” IEEE
TCSVT, vol. 31, pp. 3469–3481, 2020.
[119] X. Wang, Q. Huang, A. Celikyilmaz, J. Gao, D. Shen, Y.-F. Wang, W. Y.
Wang, and L. Zhang, “Vision-language navigation policy learning and
adaptation,” IEEE TPAMI, vol. 43, no. 12, pp. 4205–4216, 2021.
[120] S. Y. Min, D. S. Chaplot, P. K. Ravikumar, Y. Bisk, and R. Salakhut-
dinov, “FILM: Following instructions in language with modular meth-
ods,” in ICLR, 2022.
[121] D. Shah, B. Osinski, B. Ichter, and S. Levine, “Lm-nav: Robotic
navigation with large pre-trained models of language, vision, and
action,” in CoRL, 2022.
[122] Y. Qiao, Y. Qi, Y. Hong, Z. Yu, P. Wang, and Q. Wu, “Hop: History-
and-order aware pretraining for vision-and-language navigation,” in
CVPR, 2022, pp. 15 397–15 406.
[123] D. Zheng, S. Huang, L. Zhao, Y. Zhong, and L. Wang, “Towards
learning a generalist model for embodied navigation,” in CVPR, Jun
2024.
[124] J. Gao, X. Yao, and C. Xu, “Fast-slow test-time adaptation for online
vision-and-language navigation,” 2024.
[125] Y. Long, X. Li, W. Cai, and H. Dong, “Discuss before moving: Visual
language navigation via multi-expert discussions,” in ICRA, 2024.
[126] R. D. M. S. C. L. Q. C. Liuyi Wang, Zongtao He, “Vision-and-language
navigation via causal learning,” in CVPR, Jun 2024.
[127] Y. Y. Rui Liu, Wenguan Wang, “Volumetric environment representation
for vision-language navigation,” in CVPR, Jun 2024.
[128] J. Zhang, K. Wang, R. Xu, G. Zhou, Y. Hong, X. Fang, Q. Wu,
Z. Zhang, and W. He, “Navid: Video-based vlm plans the next step for
vision-and-language navigation,” ArXiv, vol. abs/2402.15852, 2024.
[129] X. E. Wang, W. Xiong, H. Wang, and W. Y. Wang, “Look before you
leap: Bridging model-free and model-based reinforcement learning for
planned-ahead vision-and-language navigation,” in ECCV, 2018.
[130] D. An, Y. Qi, Y. Huang, Q. Wu, L. Wang, and T. Tan, “Neighbor-view
enhanced model for vision and language navigation,” in ACM MM,
2021, pp. 5101–5109.
[131] Y. Hong, Z. Wang, Q. Wu, and S. Gould, “Bridging the gap be-
tween learning in discrete and continuous environments for vision-
and-language navigation,” in CVPR, 2022, pp. 15 418–15 428.



================================================================================
第 19 页
================================================================================

【文本内容】
IEEE/ASME TRANSACTIONS ON MECHATRONICS
19
[132] Y. Qiao, Y. Qi, Z. Yu, J. Liu, and Q. Wu, “March in chat: Interac-
tive prompting for remote embodied referring expression,” in ICCV,
October 2023, pp. 15 758–15 767.
[133] Z. Wang, X. Li, J. Yang, Y. Liu, J. Hu, M. Jiang, and S. Jiang, “Looka-
head exploration with neural radiance representation for continuous
vision-language navigation,” in CVPR, 2024.
[134] D. An, H. Wang, W. Wang, Z. Wang, Y. Huang, K. He, and L. Wang,
“Etpnav: Evolving topological planning for vision-language navigation
in continuous environments,” IEEE TPAMI, 2024.
[135] S. Bhambri, B. Kim, and J. Choi, “Multi-level compositional reasoning
for interactive instruction following,” in AAAI, vol. 37, no. 1, 2023, pp.
223–231.
[136] C. Xu, H. T. Nguyen, C. Amato, and L. L. Wong, “Vision and language
navigation in the real world via online visual language mapping,”
ArXiv, vol. abs/2310.10822, 2023.
[137] X. Song, W. Chen, Y. Liu, W. Chen, G. Li, and L. Lin, “Towards long-
horizon vision-language navigation: Platform, benchmark and method,”
in Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, 2025.
[138] A. Das, S. Datta, G. Gkioxari, S. Lee, D. Parikh, and D. Batra,
“Embodied question answering,” in CVPR, 2018, pp. 1–10.
[139] L. Yu, X. Chen, G. Gkioxari, M. Bansal, T. L. Berg, and D. Batra,
“Multi-target embodied question answering,” in CVPR, 2019, pp. 6309–
6318.
[140] E. Wijmans, S. Datta, O. Maksymets, A. Das, G. Gkioxari, S. Lee,
I. Essa, D. Parikh, and D. Batra, “Embodied question answering in
photorealistic environments with point cloud perception,” in CVPR,
2019, pp. 6659–6668.
[141] D. Gordon, A. Kembhavi, M. Rastegari, J. Redmon, D. Fox, and
A. Farhadi, “Iqa: Visual question answering in interactive environ-
ments,” in CVPR, 2018, pp. 4089–4098.
[142] C. Cangea, E. Belilovsky, P. Li`o, and A. Courville, “Videonavqa:
Bridging the gap between visual and embodied question answering,”
arXiv preprint arXiv:1908.04950, 2019.
[143] X. Ma, S. Yong, Z. Zheng, Q. Li, Y. Liang, S.-C. Zhu, and S. Huang,
“Sqa3d: Situated question answering in 3d scenes,” in ICLR, 2023.
[144] S. Tan, M. Ge, D. Guo, H. Liu, and F. Sun, “Knowledge-based
embodied question answering,” IEEE TPAMI, 2023.
[145] A. Majumdar, A. Ajay, X. Zhang, P. Putta, S. Yenamandra, M. Henaff,
S. Silwal, P. Mcvay, O. Maksymets, S. Arnaud et al., “Openeqa:
Embodied question answering in the era of foundation models,” in
CVPR, 2024, pp. 16 488–16 498.
[146] A. Z. Ren, J. Clark, A. Dixit, M. Itkina, A. Majumdar, and D. Sadigh,
“Explore until confident: Efficient exploration for embodied question
answering,” arXiv preprint arXiv:2403.15941, 2024.
[147] V. S. Dorbala, P. Goyal, R. Piramuthu, M. Johnston, D. Manocha,
and R. Ghanadhan, “S-eqa: Tackling situational queries in embodied
question answering,” arXiv preprint arXiv:2405.04732, 2024.
[148] K. Jiang, Y. Liu, W. Chen, J. Luo, Z. Chen, L. Pan, G. Li, and L. Lin,
“Beyond the destination: A novel benchmark for exploration-aware
embodied question answering,” 2025.
[149] Y. Wu, Y. Wu, G. Gkioxari, and Y. Tian, “Building generalizable
agents with a realistic and rich 3d environment,” arXiv preprint
arXiv:1801.02209, 2018.
[150] M. Savva, A. X. Chang, A. Dosovitskiy, T. Funkhouser, and V. Koltun,
“Minos: Multimodal indoor simulator for navigation in complex envi-
ronments,” arXiv preprint arXiv:1712.03931, 2017.
[151] A. Chang, A. Dai, T. Funkhouser, M. Halber, M. Niessner, M. Savva,
S. Song, A. Zeng, and Y. Zhang, “Matterport3d: Learning from rgb-d
data in indoor environments,” arXiv preprint arXiv:1709.06158, 2017.
[152] Y. Wu, L. Jiang, and Y. Yang, “Revisiting embodiedqa: A simple
baseline and beyond,” IEEE TIP, vol. 29, pp. 3984–3992, 2020.
[153] S. Tan, W. Xiang, H. Liu, D. Guo, and F. Sun, “Multi-agent embodied
question answering in interactive environments,” in ECCV, 2020, pp.
663–678.
[154] B. Yamauchi, “A frontier-based approach for autonomous exploration,”
in Proceedings IEEE International Symposium on Computational In-
telligence in Robotics and Automation CIRA’97.’Towards New Compu-
tational Principles for Robotics and Automation’, 1997, pp. 146–151.
[155] K. Sakamoto, D. Azuma, T. Miyanishi, S. Kurita, and M. Kawanabe,
“Map-based modular approach for zero-shot embodied question an-
swering,” arXiv preprint arXiv:2405.16559, 2024.
[156] B. Patel, V. S. Dorbala, and A. S. Bedi, “Embodied question answering
via multi-llm systems,” arXiv preprint arXiv:2406.10918, 2024.
[157] H. Liu, S. K. Sampath, N. Wang, and C. Yang, “Multifingered grasp
planning based on gaussian process implicit surface and its partial
differentials,” IEEE/ASME TMECH, vol. 29, no. 5, pp. 3522–3533,
2024.
[158] L. Chen, P. Huang, Y. Li, and Z. Meng, “Edge-dependent efficient grasp
rectangle search in robotic grasp detection,” IEEE/ASME TMECH,
vol. 26, no. 6, pp. 2922–2931, 2021.
[159] Y. Jiang, S. Moseson, and A. Saxena, “Efficient grasping from rgbd
images: Learning using a new rectangle representation,” in ICRA, 2011,
pp. 3304–3311.
[160] A. Depierre, E. Dellandr´ea, and L. Chen, “Jacquard: A large scale
dataset for robotic grasp detection,” in IROS, 2018, pp. 3511–3516.
[161] A. Mousavian, C. Eppner, and D. Fox, “6-dof graspnet: Variational
grasp generation for object manipulation,” in ICCV, 2019, pp. 2901–
2910.
[162] C. Eppner, A. Mousavian, and D. Fox, “Acronym: A large-scale grasp
dataset based on simulation,” in ICRA, 2021, pp. 6222–6227.
[163] L. F. C. Murrilo, N. Khargonkar, B. Prabhakaran, and Y. Xiang,
“Multigrippergrasp: A dataset for robotic grasping from parallel jaw
grippers to dexterous hands,” arXiv preprint arXiv:2403.09841, 2024.
[164] S. Ainetter and F. Fraundorfer, “End-to-end trainable deep neural
network for robotic grasp detection and semantic segmentation from
rgb,” in ICRA, 2021, pp. 13 452–13 458.
[165] G. Tziafas, Y. Xu, A. Goel, M. Kasaei, Z. Li, and H. Kasaei,
“Language-guided robot grasping: Clip-based referring grasp synthesis
in clutter,” arXiv preprint arXiv:2311.05779, 2023.
[166] S. Jin, J. Xu, Y. Lei, and L. Zhang, “Reasoning grasping via multimodal
large language model,” arXiv preprint arXiv:2402.06798, 2024.
[167] K. Li, J. Wang, L. Yang, C. Lu, and B. Dai, “Semgrasp: Semantic
grasp generation via language aligned discretization,” arXiv preprint
arXiv:2404.03590, 2024.
[168] M. Shridhar, L. Manuelli, and D. Fox, “Cliport: What and where
pathways for robotic manipulation,” in CoRL.
PMLR, 2022, pp. 894–
906.
[169] W. Shen, G. Yang, A. Yu, J. Wong, L. P. Kaelbling, and P. Isola, “Dis-
tilled feature fields enable few-shot language-guided manipulation,” in
7th Annual CoRL, 2023.
[170] Y. Zheng, X. Chen, Y. Zheng, S. Gu, R. Yang, B. Jin, P. Li,
C. Zhong, Z. Wang, L. Liu et al., “Gaussiangrasper: 3d language
gaussian splatting for open-vocabulary robotic grasping,” arXiv preprint
arXiv:2403.09637, 2024.
[171] H.-S. Fang, C. Wang, H. Fang, M. Gou, J. Liu, H. Yan, W. Liu, Y. Xie,
and C. Lu, “Anygrasp: Robust and efficient grasp perception in spatial
and temporal domains,” IEEE TRO, 2023.
[172] Z. Xi, W. Chen, X. Guo, W. He, Y. Ding, B. Hong, M. Zhang, J. Wang,
S. Jin, E. Zhou et al., “The rise and potential of large language model
based agents: A survey,” arXiv preprint arXiv:2309.07864, 2023.
[173] D. McDermott, M. Ghallab, A. E. Howe, C. A. Knoblock, A. Ram,
M. M. Veloso, D. S. Weld, and D. E. Wilkins, “Pddl-the planning
domain definition language,” 1998.
[174] N. C. Metropolis and S. M. Ulam, “The monte carlo method.” Journal
of the American Statistical Association, vol. 44 247, pp. 335–41, 1949.
[175] P. E. Hart, N. J. Nilsson, and B. Raphael, “A formal basis for the
heuristic determination of minimum cost paths,” IEEE Trans. Syst. Sci.
Cybern., vol. 4, pp. 100–107, 1968.
[176] M. Shridhar, J. Thomason, D. Gordon, Y. Bisk, W. Han, R. Mottaghi,
L. Zettlemoyer, and D. Fox, “Alfred: A benchmark for interpreting
grounded instructions for everyday tasks,” in CVPR, 2020, pp. 10 740–
10 749.
[177] M. Shridhar, X. Yuan, M.-A. Cˆot´e, Y. Bisk, A. Trischler, and
M. Hausknecht, “Alfworld: Aligning text and embodied environments
for interactive learning,” arXiv preprint arXiv:2010.03768, 2020.
[178] S. Y. Min, D. S. Chaplot, P. Ravikumar, Y. Bisk, and R. Salakhutdinov,
“Film: Following instructions in language with modular methods,”
arXiv preprint arXiv:2110.07342, 2021.
[179] W. Huang, P. Abbeel, D. Pathak, and I. Mordatch, “Language models
as zero-shot planners: Extracting actionable knowledge for embodied
agents,” in ICML.
PMLR, 2022, pp. 9118–9147.
[180] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng,
J. Tompson, I. Mordatch, Y. Chebotar et al., “Inner monologue: Em-
bodied reasoning through planning with language models,” in CoRL.
PMLR, 2023, pp. 1769–1782.
[181] Y. Zhang, S. Yang, C. Bai, F. Wu, X. Li, X. Li, and Z. Wang, “Towards
efficient llm grounding for embodied multi-agent collaboration,” arXiv
preprint arXiv:2405.14314, 2024.
[182] G. Sarch, Y. Wu, M. Tarr, and K. Fragkiadaki, “Open-ended in-
structable embodied agents with memory-augmented large language
models,” in EMNLP, 2023, pp. 3468–3500.



================================================================================
第 20 页
================================================================================

【文本内容】
IEEE/ASME TRANSACTIONS ON MECHATRONICS
20
[183] G. Wang, Y. Xie, Y. Jiang, A. Mandlekar, C. Xiao, Y. Zhu, L. Fan,
and A. Anandkumar, “Voyager: An open-ended embodied agent with
large language models,” arXiv preprint arXiv:2305.16291, 2023.
[184] P. Sharma, A. Torralba, and J. Andreas, “Skill induction and planning
with latent language,” in Annual Meeting of the Association for
Computational Linguistics, 2021.
[185] I. Singh, V. Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay,
D. Fox, J. Thomason, and A. Garg, “Progprompt: Generating situated
robot task plans using large language models,” in ICRA, 2023, pp.
11 523–11 530.
[186] J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. R.
Florence, and A. Zeng, “Code as policies: Language model programs
for embodied control,” ICRA, pp. 9493–9500, 2022.
[187] A. Zeng, M. Attarian, K. M. Choromanski, A. Wong, S. Welker,
F. Tombari, A. Purohit, M. S. Ryoo, V. Sindhwani, J. Lee et al.,
“Socratic models: Composing zero-shot multimodal reasoning with
language,” in ICLR, 2023.
[188] S. Shin, J. Kim, G.-C. Kang, B.-T. Zhang et al., “Socratic planner:
Inquiry-based zero-shot planning for embodied instruction following,”
arXiv preprint arXiv:2404.15190, 2024.
[189] C. H. Song, J. Wu, C. Washington, B. M. Sadler, W.-L. Chao, and Y. Su,
“Llm-planner: Few-shot grounded planning for embodied agents with
large language models,” ICCV, pp. 2986–2997, 2022.
[190] Z. Wu, Z. Wang, X. Xu, J. Lu, and H. Yan, “Embodied task planning
with large language models,” arXiv preprint arXiv:2307.01848, 2023.
[191] K. Rana, J. Haviland, S. Garg, J. Abou-Chakra, I. D. Reid, and
N. S¨underhauf, “Sayplan: Grounding large language models using 3d
scene graphs for scalable task planning,” in CoRL, 2023.
[192] Q. Gu, A. Kuwajerwala, S. Morin, K. M. Jatavallabhula, B. Sen,
A. Agarwal, C. Rivera, W. Paul, K. Ellis, R. Chellappa, C. Gan, C. M.
de Melo, J. B. Tenenbaum, A. Torralba, F. Shkurti, and L. Paull,
“Conceptgraphs: Open-vocabulary 3d scene graphs for perception and
planning,” ArXiv, vol. abs/2309.16650, 2023.
[193] Y. Mu, Q. Zhang, M. Hu, W. Wang, M. Ding, J. Jin, B. Wang, J. Dai,
Y. Qiao, and P. Luo, “Embodiedgpt: Vision-language pre-training via
embodied chain of thought,” NeurIPS, vol. 36, 2024.
[194] J. Huang, S. Yong, X. Ma, X. Linghu, P. Li, Y. Wang, Q. Li, S.-C. Zhu,
B. Jia, and S. Huang, “An embodied generalist agent in 3d world,” in
ICML, 2024.
[195] Z. Wu, Z. Wang, X. Xu, J. Lu, and H. Yan, “Embodied instruction
following in unknown environments,” 2024.
[196] D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter,
A. Wahid, J. Tompson, Q. Vuong, T. Yu et al., “Palm-e: an embodied
multimodal language model,” in ICML, 2023, pp. 8469–8488.
[197] X. Zhao, M. Li, C. Weber, M. B. Hafez, and S. Wermter, “Chat with the
environment: Interactive multimodal perception using large language
models,” in IROS, 2023, pp. 3590–3596.
[198] Z. Zhao, S. Cheng, Y. Ding, Z. Zhou, S. Zhang, D. Xu, and Y. Zhao, “A
survey of optimization-based task and motion planning: From classical
to learning approaches,” IEEE/ASME TMECH, pp. 1–27, 2024.
[199] A. Brohan, Y. Chebotar, C. Finn, K. Hausman, A. Herzog, D. Ho,
J. Ibarz, A. Irpan, E. Jang, R. Julian et al., “Do as i can, not as i
say: Grounding language in robotic affordances,” in CoRL, 2023, pp.
287–318.
[200] N.
Shinn,
B.
Labash,
and
A.
Gopinath,
“Reflexion:
an
au-
tonomous agent with dynamic memory and self-reflection,” ArXiv, vol.
abs/2303.11366, 2023.
[201] Z. Wang, S. Cai, A. Liu, X. Ma, and Y. Liang, “Describe, explain, plan
and select: Interactive planning with large language models enables
open-world multi-task agents,” ArXiv, vol. abs/2302.01560, 2023.
[202] Q. Vuong, S. Levine, H. R. Walke, K. Pertsch, A. Singh, R. Doshi,
C. Xu, J. Luo, L. Tan, D. Shah et al., “Open x-embodiment: Robotic
learning datasets and rt-x models,” in Towards Generalist Robots:
Learning Paradigms for Scalable Skill Acquisition@ CoRL2023, 2023.
[203] D.
Ha
and
J.
Schmidhuber,
“World
models,”
arXiv
preprint
arXiv:1803.10122, 2018.
[204] J. Xiang, G. Liu, Y. Gu, Q. Gao, Y. Ning, Y. Zha, Z. Feng,
T. Tao, S. Hao, Y. Shi et al., “Pandora: Towards general world
model with natural language actions and video states,” arXiv preprint
arXiv:2406.09455, 2024.
[205] H. Zhen, X. Qiu, P. Chen, J. Yang, X. Yan, Y. Du, Y. Hong, and C. Gan,
“3d-vla: A 3d vision-language-action generative world model,” arXiv
preprint arXiv:2403.09631, 2024.
[206] Z. Ding, A. Zhang, Y. Tian, and Q. Zheng, “Diffusion world model,”
arXiv preprint arXiv:2402.03570, 2024.
[207] A. Bardes, J. Ponce, and Y. LeCun, “Mc-jepa: A joint-embedding
predictive architecture for self-supervised learning of motion and
content features,” arXiv preprint arXiv:2307.12698, 2023.
[208] Z. Fei, M. Fan, and J. Huang, “A-jepa: Joint-embedding predictive
architecture can listen,” arXiv preprint arXiv:2311.15830, 2023.
[209] A. Saito and J. Poovvancheri, “Point-jepa: A joint embedding predictive
architecture for self-supervised learning on point cloud,” arXiv preprint
arXiv:2404.16432, 2024.
[210] Q. Garrido, M. Assran, N. Ballas, A. Bardes, L. Najman, and Y. Le-
Cun, “Learning and leveraging world models in visual representation
learning,” arXiv preprint arXiv:2403.00504, 2024.
[211] J. Wu, S. Yin, N. Feng, X. He, D. Li, J. Hao, and M. Long,
“ivideogpt: Interactive videogpts are scalable world models,” arXiv
preprint arXiv:2405.15223, 2024.
[212] F. Zhu, H. Wu, S. Guo, Y. Liu, C. Cheang, and T. Kong, “Irasim:
Learning interactive real-robot action simulators,” arXiv preprint
arXiv:2406.14540, 2024.
[213] J. Yang, B. Liu, J. Fu, B. Pan, G. Wu, and L. Wang, “Spatiotempo-
ral predictive pre-training for robotic motor control,” arXiv preprint
arXiv:2403.05304, 2024.
[214] M. Burchi and R. Timofte, “Mudreamer: Learning predictive world
models without reconstruction,” arXiv preprint arXiv:2405.15083,
2024.
[215] Y. LeCun, “A path towards autonomous machine intelligence version
0.9. 2, 2022-06-27,” Open Review, vol. 62, no. 1, pp. 1–62, 2022.
[216] A. Dawid and Y. LeCun, “Introduction to latent variable energy-
based models: A path towards autonomous machine intelligence,” arXiv
preprint arXiv:2306.02572, 2023.
[217] V. Lim, H. Huang, L. Y. Chen, J. Wang, J. Ichnowski, D. Seita,
M. Laskey, and K. Goldberg, “Real2sim2real: Self-supervised learning
of physical single-step dynamic actions for planar robot casting,” in
ICRA, 2022, pp. 8282–8289.
[218] Y. Feng, Y. Shang, X. Feng, L. Lan, S. Zhe, T. Shao, H. Wu, K. Zhou,
H. Su, C. Jiang et al., “Elastogen: 4d generative elastodynamics,” arXiv
preprint arXiv:2405.15056, 2024.
[219] M. Liu, C. Xu, H. Jin, L. Chen, M. Varma T, Z. Xu, and H. Su, “One-
2-3-45: Any single image to 3d mesh in 45 seconds without per-shape
optimization,” NeurIPS, vol. 36, 2024.
[220] L. Wong, G. Grand, A. K. Lew, N. D. Goodman, V. K. Mansinghka,
J. Andreas, and J. B. Tenenbaum, “From word models to world models:
Translating from natural language to the probabilistic language of
thought,” arXiv preprint arXiv:2306.12672, 2023.
[221] Z. Cheng, Z. Wang, J. Hu, S. Hu, A. Liu, Y. Tu, P. Li, L. Shi, Z. Liu, and
M. Sun, “Legent: Open platform for embodied agents,” arXiv preprint
arXiv:2404.18243, 2024.
[222] H. Wang, J. Chen, W. Huang, Q. Ben, T. Wang, B. Mi, T. Huang,
S. Zhao, Y. Chen, S. Yang et al., “Grutopia: Dream general robots in
a city at scale,” arXiv preprint arXiv:2407.10943, 2024.
[223] N. Agarwal, A. Ali, M. Bala, Y. Balaji, E. Barker, T. Cai, P. Chattopad-
hyay, Y. Chen, Y. Cui, Y. Ding et al., “Cosmos world foundation model
platform for physical ai,” arXiv preprint arXiv:2501.03575, 2025.
[224] C. Chi, Z. Xu, C. Pan, E. Cousineau, B. Burchfiel, S. Feng, R. Tedrake,
and S. Song, “Universal manipulation interface: In-the-wild robot
teaching without in-the-wild robots,” arXiv preprint arXiv:2402.10329,
2024.
[225] Z. Fu, T. Z. Zhao, and C. Finn, “Mobile aloha: Learning bimanual
mobile manipulation with low-cost whole-body teleoperation,” arXiv
preprint arXiv:2401.02117, 2024.
[226] S. Luo, Q. Peng, J. Lv, K. Hong, K. R. Driggs-Campbell, C. Lu, and
Y.-L. Li, “Human-agent joint learning for efficient robot manipulation
skill acquisition,” arXiv preprint arXiv:2407.00299, 2024.
[227] A. Zeng, P. Florence, J. Tompson, S. Welker, J. Chien, M. Attarian,
T. Armstrong, I. Krasin, D. Duong, V. Sindhwani et al., “Transporter
networks: Rearranging the visual world for robotic manipulation,” in
CoRL.
PMLR, 2021, pp. 726–747.
[228] H. Geng, H. Xu, C. Zhao, C. Xu, L. Yi, S. Huang, and H. Wang,
“Gapartnet: Cross-category domain-generalizable object perception and
manipulation via generalizable and actionable parts,” in CVPR, 2023,
pp. 7081–7091.
[229] M. Torne, A. Simeonov, Z. Li, A. Chan, T. Chen, A. Gupta,
and P. Agrawal, “Reconciling reality through simulation: A real-
to-sim-to-real approach for robust manipulation,” arXiv preprint
arXiv:2403.03949, 2024.
[230] Y. Jiang, C. Wang, R. Zhang, J. Wu, and L. Fei-Fei, “Transic: Sim-to-
real policy transfer by learning from online correction,” arXiv preprint
arXiv:2405.10315, 2024.



================================================================================
第 21 页
================================================================================

【文本内容】
IEEE/ASME TRANSACTIONS ON MECHATRONICS
21
[231] J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and P. Abbeel,
“Domain randomization for transferring deep neural networks from
simulation to the real world,” in IROS, 2017, pp. 23–30.
[232] O. M. Andrychowicz, B. Baker, M. Chociej, R. Jozefowicz, B. Mc-
Grew, J. Pachocki, A. Petron, M. Plappert, G. Powell, A. Ray et al.,
“Learning dexterous in-hand manipulation,” IJRR, vol. 39, no. 1, pp.
3–20, 2020.
[233] J. Matas, S. James, and A. J. Davison, “Sim-to-real reinforcement
learning for deformable object manipulation,” in CoRL. PMLR, 2018,
pp. 734–743.
[234] M. Kaspar, J. D. M. Osorio, and J. Bock, “Sim2real transfer for
reinforcement learning without dynamics randomization,” in IROS,
2020, pp. 4383–4388.
[235] W. Yu, J. Tan, C. K. Liu, and G. Turk, “Preparing for the unknown:
Learning a universal policy with online system identification,” arXiv
preprint arXiv:1702.02453, 2017.
[236] A. Yu, A. Foote, R. Mooney, and R. Mart´ın-Mart´ın, “Natural language
can help bridge the sim2real gap,” arXiv preprint arXiv:2405.10020,
2024.
[237] Z. Wang, H. Zheng, Y. Nie, W. Xu, Q. Wang, H. Ye, Z. Li, K. Zhang,
X. Cheng, W. Dong et al., “All robots in one: A new standard and
unified dataset for versatile, general-purpose embodied agents,” arXiv
preprint arXiv:2408.10899, 2024.
Yang Liu (M’21) is currently an Associate Professor
working at the School of Computer Science and
Engineering, Sun Yat-sen University. He received
his Ph.D. degree from Xidian University in 2019.
His current research interests include multi-modal
reasoning, causality learning and embodied AI. He
is the recipient of the First Prize of the Third Guang-
dong Province Young Computer Science Academic
Show. He has authorized and co-authorized more
than 40 papers in top-tier academic journals and
conferences such as TPAMI, TIP, CVPR and ICCV.
Weixing Chen has received the B.S. degree from
the college of Medicine and Biological Information
Engineering, Northeastern University, in 2020 and
M.S. degree from Shenzhen Institute of Advanced
Technology, Chinese Academy of Sciences in 2023.
He is currently a Ph.D. student at the School of
Computer Science and Engineering, Sun Yat-sen
University. His main interests include multi-modal
learning, causal relation discovery, and embodied ai.
He has been serving as a reviewer for numerous
academic journals and conferences such as TNNLS,
NeurIPS, ICML, ICLR, MICCAI, and ACM MM.
Yongjie Bai received the B.S. degree from the
School of Computer Science and Technology, Dalian
University of Technology, in 2024. He is currently
a Ph.D. student at the School of Computer Science
and Engineering, Sun Yat-sen University. His main
research interests include embodied AI, robot learn-
ing, and multi-modal learning.
Xiaodan Liang (Senior Member, IEEE) is currently
a Professor at Sun Yat-sen University. She was a
postdoc researcher in the machine learning depart-
ment at Carnegie Mellon University, working with
Prof. Eric Xing, from 2016 to 2018. She received her
PhD degree from Sun Yat-sen University in 2016,
advised by Liang Lin. She has published several
cutting-edge projects on human-related analysis, in-
cluding human parsing, pedestrian detection and in-
stance segmentation, 2D/3D human pose estimation,
and activity recognition.
Guanbin Li (M’15) is currently a Professor in
School of Computer Science and Engineering, Sun
Yat-Sen University. He received his PhD degree
from the University of Hong Kong in 2016. His
current research interests include computer vision,
image processing, and deep learning. He is a recipi-
ent of ICCV 2019 Best Paper Nomination Award. He
has authorized and co-authorized on more than 100
papers in top-tier academic journals and conferences.
He serves as an area chair for the conference of
VISAPP. He has been serving as a reviewer for
numerous academic journals and conferences such as TPAMI, IJCV, TIP,
TMM, TCyb, CVPR, ICCV, ECCV and NeurIPS.
Wen Gao (Fellow, IEEE) received the Ph.D. degree
in electronics engineering from the University of
Tokyo, Tokyo, Japan, in 1991. He is currently a
Professor of computer science with the School of
Electronic Engineering and Computer Science, In-
stitute of Digital Media, Peking University, Beijing,
China. Before joining Peking University, he was
a Professor of computer science with the Harbin
Institute of Technology, Harbin, China, from 1991
to 1995, and a Professor with the Institute of Com-
puting Technology, Chinese Academy of Sciences,
Beijing, China. He has authored or coauthored extensively, including five
books and more than 600 technical articles in refereed journals and conference
proceedings in the areas of image processing, video coding and commu-
nication, pattern recognition, multimedia information retrieval, multimodal
interfaces, and bioinformatics. He is a Member of the China Engineering
Academy.
Liang Lin (Fellow, IEEE) is currently a Full Profes-
sor with Sun Yat-sen University, Guangzhou, China.
From 2008 to 2010, he was a Postdoctoral Fellow
with the University of California, Los Angeles, Los
Angeles, CA, USA. From 2016 to 2018, he led
the SenseTime R&D teams to develop cutting-edge
and deliverable solutions for computer vision, data
analysis and mining, and intelligent robotic systems.
He has authored or coauthored more than 100 pa-
pers in top-tier academic journals and conferences,
such as 15 papers in IEEE TRANSACTIONS ON
PATTERN ANALYSIS AND MACHINE I NTELLIGENCE and International
Journal of Computer Vision, and more than 60 papers in CVPR, ICCV,
NeurIPS, and IJCAI. He was an Associate Editor for IEEE TRANSACTIONS
ON MULTIMEDIA , IEEE TRANSACTIONS ON NEURAL NETWORKS
AND LEARNING SYSTEMS, and was an Area/Session Chair for numerous
conferences, such as CVPR, ICCV, AAAI, ICME, and ICMR. He was the
recipient of the Annual Best Paper Award by Pattern Recognition (Elsevier)
in 2018, Best Paper Diamond Award at IEEE ICME 2017, Best Paper Runner-
Up Award at ACM NPAR 2010, Google Faculty Award in 2012, Best Student
Paper Award at IEEE ICME 2014, and Hong Kong Scholars Award in 2014.
He is a Fellow of IAPR, AAIA, and IET.


【图像 1】(1531x1579)
1. **图像类型**：  
   这是一张人物肖像照片，属于标准的证件照或作者头像类型。

2. **图像中的所有文字内容**：  
   图像中**没有文字内容**。

3. **图像中的关键视觉元素和数据信息**：  
   - 一位东亚男性，短发，戴黑色边框眼镜。  
   - 穿着蓝色纽扣衬衫（可能是牛仔布材质），领口有白色纽扣。  
   - 背景为纯白色，光线均匀，符合正式证件照风格。  
   - 表情平静，直视镜头，嘴角微扬，呈现中性微笑。  
   - 拍摄角度为正面平视，构图居中，聚焦于面部和上半身。

4. **图像传达的主要信息**：  
   该图像主要用于学术出版物中的作者介绍，可能对应论文或期刊文章的作者之一。结合上下文（IEEE/ASME TRANSACTIONS ON MECHATRONICS）及文献引用格式，此图为某位研究人员的个人肖像，用于标识其身份，常见于期刊论文末尾的作者简介部分。

【图像 2】(250x314)
1. **图像类型**：  
   这是一张人物肖像照片，属于标准的个人证件照或学术头像风格。

2. **图像中的所有文字内容**：  
   图像中**没有文字内容**。

3. **图像中的关键视觉元素和数据信息**：  
   - 一位中年男性，戴眼镜，表情严肃。  
   - 穿着黑色西装外套和白色衬衫，衣着正式。  
   - 背景为均匀灰色渐变，无其他干扰元素。  
   - 光线柔和，聚焦于面部，符合专业摄影标准。

4. **图像传达的主要信息**：  
   该图像很可能用于学术出版物中的作者介绍或个人简介部分，展示某位研究人员或学者的形象，通常与论文作者身份相关。结合上下文（IEEE/ASME TRANSACTIONS ON MECHATRONICS），这可能是某篇论文作者的头像。

【图像 3】(354x472)
1. **图像类型**：  
   这是一张人物肖像照片，属于标准证件照风格。

2. **图像中的文字内容**：  
   图像中**没有文字内容**。

3. **图像中的关键视觉元素和数据信息**：  
   - 一位东亚男性，面部正对镜头，表情自然，目光直视前方。  
   - 黑色短发，整齐梳理。  
   - 穿着深色带有细小白色几何图案的衬衫（可能是格纹或点状设计）。  
   - 背景为纯白色，符合证件照的标准背景要求。  
   - 光线均匀，无明显阴影，面部细节清晰可见。

4. **图像传达的主要信息**：  
   该图像为一张标准的人物头像照，通常用于学术论文、作者介绍、身份识别等场景。结合上下文（IEEE/ASME期刊论文的参考文献页），此图极有可能是某位作者的个人照片，用于在论文末尾或作者简介部分展示其身份信息。由于图像位于参考文献之后，推测可能是作者列表中的某位研究人员的照片。

【图像 4】(358x441)
1. **图像类型**：  
   这是一张人物照片，属于标准的证件照或头像类型。

2. **图像中的所有文字内容**：  
   图像中无任何可见文字内容。

3. **图像中的关键视觉元素和数据信息**：  
   - 一位亚洲男性，短发，戴金属细框眼镜。  
   - 穿着浅蓝色翻领衬衫，衣领整齐，扣子系好。  
   - 背景为纯白色，无其他装饰或干扰元素。  
   - 面部表情自然，目光正视镜头，嘴唇微闭。  
   - 光线均匀，面部清晰，无明显阴影。

4. **图像传达的主要信息**：  
   该图像为一张正式的人物肖像照，可能用于学术论文、作者介绍或个人资料展示。结合上下文（IEEE/ASME TRANSACTIONS ON MECHATRONICS）及页面位置（第21页第4张图），推测此图为某位作者的个人照片，用于标识其身份，常见于期刊论文的作者简介部分。

【图像 5】(1918x2680)
1. **图像类型**：  
   这是一张人物证件照风格的正面照片，属于**照片**类型。

2. **图像中的所有文字内容**：  
   图像中**无文字内容**。

3. **图像中的关键视觉元素和数据信息**：  
   - 人物为一位年轻男性，面部正对镜头，表情平静，目光直视前方。  
   - 黑色短发，发型整齐，刘海覆盖额头。  
   - 穿着白色翻领衬衫，衣领平整，纽扣系好。  
   - 背景为纯白色，无任何图案或装饰，符合标准证件照要求。  
   - 光线均匀，面部无明显阴影，曝光适中，清晰度高。

4. **图像传达的主要信息**：  
   该图像是一张标准的个人证件照，用于身份识别或正式用途（如学术、行政、证件申请等）。其主要功能是展示个体的清晰面部特征，便于辨认。结合上下文（IEEE/ASME期刊页面），可能用于作者或研究人员的身份信息展示。

【图像 6】(2755x3844)
1. **图像类型**：  
   这是一张人物肖像照片，属于标准证件照风格。

2. **图像中的所有文字内容**：  
   图像中**没有文字内容**。

3. **图像中的关键视觉元素和数据信息**：  
   - 一位年轻男性，面部正对镜头，表情平静。  
   - 短发，黑色，整齐修剪。  
   - 戴着一副黑色细框圆形眼镜。  
   - 穿着白色衬衫，领口整洁，外搭深色西装外套。  
   - 背景为纯白色，无任何装饰或干扰元素。  
   - 光线均匀，面部无明显阴影，符合标准证件照的拍摄要求。

4. **图像传达的主要信息**：  
   该图像为一张正式的人物肖像照，可能用于学术论文、个人简历或作者介绍等场景。结合上下文（IEEE/ASME期刊论文的参考文献页），此图极有可能是某位作者的个人照片，用于在论文末尾的作者简介部分展示其身份。

【图像 7】(253x343)
根据您提供的信息，以下是对该图像的详细分析：

---

### 1. 图像类型  
**照片**（人物肖像照）

这是一张人物的正面半身照，属于标准的证件照或正式肖像照风格。

---

### 2. 图像中的所有文字内容  
**无文字内容**

图像中未包含任何可见的文字、标签或图注。仅显示一位女性人物的面部和上半身。

---

### 3. 图像中的关键视觉元素和数据信息  
- **人物特征**：  
  - 一位亚洲女性，面带微笑，表情自然友好。  
  - 黑色长发，披肩而下，略带波浪。  
  - 耳戴白色珍珠耳钉。  
  - 穿着深色西装外套，内搭白色衣物，领口有V形设计。  
  - 左手持有一叠白色扇形物品（可能是纸张或装饰物），部分遮挡左肩。

- **背景**：  
  - 单色浅灰色背景，简洁无干扰，符合专业摄影风格。

- **光线与构图**：  
  - 光线均匀柔和，面部清晰，无明显阴影。  
  - 构图居中，聚焦于人物面部，属于典型的肖像拍摄方式。

---

### 4. 图像传达的主要信息  
该图像主要呈现一位女性的专业形象，可能用于学术、职业或机构介绍。结合上下文（IEEE/ASME TRANSACTIONS ON MECHATRONICS 的参考文献页面），此图像**极有可能是某位作者或研究人员的个人照片**，常用于期刊论文末尾的作者简介部分。

尽管当前页面文本未直接提及该图像，但其位置（第21页第7张图）以及风格表明它可能是附在文章末尾的作者头像之一，用于识别贡献者身份。

---

### 总结  
- **图像类型**：人物肖像照  
- **文字内容**：无  
- **关键元素**：女性、微笑、正式服装、珍珠耳钉、手持白色扇形物、灰色背景  
- **主要信息**：展示一位研究人员或作者的正式形象，用于学术出版物中的身份识别  

> 注：由于缺乏明确的图注或上下文说明，无法确认具体身份，但图像功能可推断为“作者肖像”。
