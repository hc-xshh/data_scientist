import os
import base64
from pathlib import Path
from openai import OpenAI
from typing import Dict
import io
import tempfile
from urllib.parse import urlparse
from langchain_core.tools import tool
from pydantic import BaseModel, Field

# å®‰è£…ä¾èµ–: pip install PyMuPDF Pillow
try:
    import fitz  # PyMuPDF
    from PIL import Image
except ImportError:
    print("è¯·å…ˆå®‰è£…ä¾èµ–: pip install PyMuPDF Pillow")


# å·¥å…·å‚æ•°å®šä¹‰
class ParsePDFParams(BaseModel):
    """è§£æPDFæ–‡ä»¶çš„å‚æ•°"""
    pdf_path: str = Field(description="PDFæ–‡ä»¶çš„å®Œæ•´è·¯å¾„æˆ–ä¸Šä¼ æ–‡ä»¶çš„URLï¼ˆå¦‚ http://localhost:5000/files/xxx.pdfï¼‰")
    analyze_images: bool = Field(default=True, description="æ˜¯å¦ä½¿ç”¨å¤§æ¨¡å‹åˆ†æå›¾åƒå†…å®¹ï¼Œé»˜è®¤True")
    save_to_file: bool = Field(default=False, description="æ˜¯å¦å°†ç»“æœä¿å­˜åˆ°æ–‡ä»¶ï¼Œé»˜è®¤False")
    output_path: str = Field(default="", description="è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœsave_to_fileä¸ºTrueåˆ™å¿…å¡«")


# è¾…åŠ©å‡½æ•°
def _image_to_base64(image_bytes: bytes, max_size: int = 1920, quality: int = 85) -> str:
    """å°†å›¾åƒå­—èŠ‚è½¬æ¢ä¸ºbase64ç¼–ç ï¼Œå¹¶è¿›è¡Œå‹ç¼©ä¼˜åŒ–"""
    img = Image.open(io.BytesIO(image_bytes))
    
    # è½¬æ¢RGBAåˆ°RGB
    if img.mode == 'RGBA':
        background = Image.new('RGB', img.size, (255, 255, 255))
        background.paste(img, mask=img.split()[3])
        img = background
    elif img.mode != 'RGB':
        img = img.convert('RGB')
    
    # å‹ç¼©å›¾åƒå°ºå¯¸
    if img.width > max_size or img.height > max_size:
        ratio = min(max_size / img.width, max_size / img.height)
        new_size = (int(img.width * ratio), int(img.height * ratio))
        img = img.resize(new_size, Image.Resampling.LANCZOS)
    
    # è½¬æ¢ä¸ºbase64
    buffered = io.BytesIO()
    img.save(buffered, format="JPEG", quality=quality, optimize=True)
    img_base64 = base64.b64encode(buffered.getvalue()).decode('utf-8')
    
    return img_base64


def _analyze_image_with_llm(img_base64: str, context: str = "") -> str:
    """ä½¿ç”¨å¤§æ¨¡å‹åˆ†æå›¾åƒå†…å®¹"""
    client = OpenAI(
        api_key=os.getenv("DASHSCOPE_API_KEY"),
        base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
    )
    
    prompt = f"""è¯·è¯¦ç»†åˆ†æè¿™å¼ å›¾åƒçš„å†…å®¹ã€‚

{context}

è¯·æå–å¹¶æè¿°ï¼š
1. å›¾åƒç±»å‹ï¼ˆå›¾è¡¨ã€ç…§ç‰‡ã€æˆªå›¾ã€å›¾å½¢ç­‰ï¼‰
2. å›¾åƒä¸­çš„æ‰€æœ‰æ–‡å­—å†…å®¹
3. å›¾åƒä¸­çš„å…³é”®è§†è§‰å…ƒç´ å’Œæ•°æ®ä¿¡æ¯
4. å›¾åƒä¼ è¾¾çš„ä¸»è¦ä¿¡æ¯

è¯·ç”¨ç®€æ´æ¸…æ™°çš„è¯­è¨€æè¿°ï¼Œé‡ç‚¹å…³æ³¨ä¿¡æ¯æå–ã€‚"""

    content = [
        {"type": "text", "text": prompt},
        {
            "type": "image_url",
            "image_url": {"url": f"data:image/jpeg;base64,{img_base64}"}
        }
    ]
    
    try:
        completion = client.chat.completions.create(
            model="qwen-vl-max",
            messages=[{"role": "user", "content": content}],
            stream=False,
        )
        return completion.choices[0].message.content
    except Exception as e:
        return f"[å›¾åƒåˆ†æå¤±è´¥: {str(e)}]"


def _parse_pdf_complete(pdf_path: str, analyze_images: bool = True) -> Dict:
    """
    å®Œæ•´è§£æPDFæ–‡ä»¶ï¼Œæå–æ–‡æœ¬å’Œå›¾åƒå†…å®¹
    
    Args:
        pdf_path: PDFæ–‡ä»¶è·¯å¾„æˆ–URLï¼ˆå¦‚ http://localhost:5000/files/xxx.pdfï¼‰
        analyze_images: æ˜¯å¦ä½¿ç”¨å¤§æ¨¡å‹åˆ†æå›¾åƒï¼Œé»˜è®¤True
    
    Returns:
        åŒ…å«å®Œæ•´PDFå†…å®¹çš„å­—å…¸
    """
    # å¤„ç† URL æ ¼å¼çš„è·¯å¾„
    if pdf_path.startswith('http://') or pdf_path.startswith('https://'):
        # ä» URL ä¸­æå–æ–‡ä»¶å
        parsed_url = urlparse(pdf_path)
        filename = os.path.basename(parsed_url.path)
        # æ„å»ºä¸´æ—¶æ–‡ä»¶å¤¹çš„å®Œæ•´è·¯å¾„
        local_pdf_path = os.path.join(tempfile.gettempdir(), filename)
        print(f"æ£€æµ‹åˆ°URLæ ¼å¼ï¼Œè½¬æ¢ä¸ºæœ¬åœ°è·¯å¾„: {local_pdf_path}")
    else:
        local_pdf_path = pdf_path
    
    if not Path(local_pdf_path).exists():
        raise FileNotFoundError(f"PDFæ–‡ä»¶ä¸å­˜åœ¨: {local_pdf_path}")
    
    print(f"å¼€å§‹è§£æPDF: {local_pdf_path}")
    pdf_document = fitz.open(local_pdf_path)
    total_pages = len(pdf_document)
    print(f"æ€»é¡µæ•°: {total_pages}")
    
    result = {
        'total_pages': total_pages,
        'pages': [],
        'full_content': ''
    }
    
    full_content_parts = []
    
    for page_num in range(total_pages):
        print(f"\nå¤„ç†ç¬¬ {page_num + 1}/{total_pages} é¡µ...")
        page = pdf_document[page_num]
        
        # æå–æ–‡æœ¬å†…å®¹
        text = page.get_text()
        print(f"  æå–æ–‡æœ¬: {len(text)} å­—ç¬¦")
        
        # æå–å›¾åƒ
        image_list = page.get_images(full=True)
        print(f"  å‘ç°å›¾åƒ: {len(image_list)} ä¸ª")
        
        page_data = {
            'page_num': page_num + 1,
            'text': text,
            'images': []
        }
        
        # å¤„ç†é¡µé¢æ–‡æœ¬
        page_content = f"\n{'='*80}\nç¬¬ {page_num + 1} é¡µ\n{'='*80}\n"
        
        if text.strip():
            page_content += f"\nã€æ–‡æœ¬å†…å®¹ã€‘\n{text}\n"
        
        # å¤„ç†å›¾åƒ
        for img_index, img_info in enumerate(image_list):
            xref = img_info[0]
            try:
                # æå–å›¾åƒæ•°æ®
                base_image = pdf_document.extract_image(xref)
                image_bytes = base_image["image"]
                
                # è·å–å›¾åƒå°ºå¯¸
                img = Image.open(io.BytesIO(image_bytes))
                img_size = (img.width, img.height)
                print(f"    å›¾åƒ {img_index + 1}: {img_size[0]}x{img_size[1]}")
                
                image_data = {
                    'image_index': img_index + 1,
                    'size': img_size,
                    'analysis': ''
                }
                
                # ä½¿ç”¨å¤§æ¨¡å‹åˆ†æå›¾åƒ
                if analyze_images:
                    print(f"    æ­£åœ¨åˆ†æå›¾åƒ {img_index + 1}...")
                    img_base64 = _image_to_base64(image_bytes)
                    context = f"è¿™æ˜¯PDFç¬¬{page_num + 1}é¡µä¸­çš„ç¬¬{img_index + 1}å¼ å›¾åƒã€‚"
                    if text.strip():
                        context += f"\n\né¡µé¢æ–‡æœ¬ä¸Šä¸‹æ–‡ï¼š\n{text[:500]}..."
                    
                    analysis = _analyze_image_with_llm(img_base64, context)
                    image_data['analysis'] = analysis
                    
                    page_content += f"\nã€å›¾åƒ {img_index + 1}ã€‘({img_size[0]}x{img_size[1]})\n{analysis}\n"
                else:
                    image_data['analysis'] = f"[å›¾åƒ {img_index + 1}: {img_size[0]}x{img_size[1]}]"
                    page_content += f"\nã€å›¾åƒ {img_index + 1}ã€‘{img_size[0]}x{img_size[1]}\n"
                
                page_data['images'].append(image_data)
                
            except Exception as e:
                print(f"    å›¾åƒ {img_index + 1} å¤„ç†å¤±è´¥: {str(e)}")
                page_data['images'].append({
                    'image_index': img_index + 1,
                    'size': (0, 0),
                    'analysis': f"[å›¾åƒæå–å¤±è´¥: {str(e)}]"
                })
        
        result['pages'].append(page_data)
        full_content_parts.append(page_content)
    
    pdf_document.close()
    
    # æ•´åˆå®Œæ•´å†…å®¹
    result['full_content'] = '\n'.join(full_content_parts)
    
    print(f"\n{'='*80}")
    print("PDFè§£æå®Œæˆï¼")
    print(f"{'='*80}")
    
    return result


def _save_result_to_file(result: Dict, output_path: str):
    """å°†è§£æç»“æœä¿å­˜åˆ°æ–‡ä»¶"""
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(result['full_content'])
    print(f"\nç»“æœå·²ä¿å­˜åˆ°: {output_path}")


# å®šä¹‰langchainå·¥å…·

@tool(args_schema=ParsePDFParams)
def parse_pdf_document(
    pdf_path: str,
    analyze_images: bool = True,
    save_to_file: bool = False,
    output_path: str = ""
) -> str:
    """
    å®Œæ•´è§£æPDFæ–‡æ¡£ï¼Œæå–æ–‡æœ¬å’Œå›¾åƒå†…å®¹ï¼Œå¯é€‰ä½¿ç”¨è§†è§‰å¤§æ¨¡å‹åˆ†æå›¾åƒã€‚
    
    é€‚ç”¨åœºæ™¯:
    - æå–PDFæ–‡æ¡£çš„æ‰€æœ‰æ–‡æœ¬å†…å®¹
    - è¯†åˆ«å’Œåˆ†æPDFä¸­çš„å›¾è¡¨ã€å›¾ç‰‡
    - ä½¿ç”¨è§†è§‰å¤§æ¨¡å‹ç†è§£å›¾åƒå†…å®¹
    - å°†PDFè½¬æ¢ä¸ºç»“æ„åŒ–æ–‡æœ¬
    - åˆ†æå­¦æœ¯è®ºæ–‡ã€æŠ¥å‘Šã€åˆåŒç­‰æ–‡æ¡£
    - æå–PDFä¸­çš„è¡¨æ ¼å’Œæ•°æ®
    
    åŠŸèƒ½ç‰¹ç‚¹:
    - é€é¡µæå–æ–‡æœ¬å†…å®¹
    - è¯†åˆ«å¹¶æå–æ‰€æœ‰å›¾åƒ
    - å¯é€‰ä½¿ç”¨qwen-vl-maxæ¨¡å‹åˆ†æå›¾åƒå†…å®¹
    - è‡ªåŠ¨ä¼˜åŒ–å›¾åƒå¤§å°å’Œè´¨é‡
    - è¾“å‡ºç»“æ„åŒ–çš„å®Œæ•´å†…å®¹
    - å¯é€‰ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
    - æ”¯æŒæœ¬åœ°è·¯å¾„å’Œä¸Šä¼ æ–‡ä»¶URLï¼ˆè‡ªåŠ¨ä»ä¸´æ—¶æ–‡ä»¶å¤¹è·å–ï¼‰
    
    å‚æ•°è¯´æ˜:
    - pdf_path: PDFæ–‡ä»¶çš„å®Œæ•´è·¯å¾„æˆ–ä¸Šä¼ æ–‡ä»¶çš„URLï¼ˆå¦‚ http://localhost:5000/files/xxx.pdfï¼‰
    - analyze_images: æ˜¯å¦ä½¿ç”¨å¤§æ¨¡å‹åˆ†æå›¾åƒï¼ˆéœ€è¦DASHSCOPE_API_KEYç¯å¢ƒå˜é‡ï¼‰
    - save_to_file: æ˜¯å¦å°†ç»“æœä¿å­˜åˆ°æ–‡ä»¶
    - output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå½“save_to_fileä¸ºTrueæ—¶å¿…å¡«ï¼‰
    
    è¿”å›:
    - åŒ…å«PDFå®Œæ•´å†…å®¹çš„æ–‡æœ¬ï¼ŒåŒ…æ‹¬æ‰€æœ‰é¡µé¢çš„æ–‡æœ¬å’Œå›¾åƒåˆ†æç»“æœ
    """
    try:
        # è§£æPDF
        result = _parse_pdf_complete(pdf_path, analyze_images)
        
        # å¯é€‰ä¿å­˜åˆ°æ–‡ä»¶
        if save_to_file:
            if not output_path:
                output_path = pdf_path.replace('.pdf', '_parsed.txt')
            _save_result_to_file(result, output_path)
        
        # æ„å»ºæ‘˜è¦ä¿¡æ¯
        summary = f"""PDFè§£æå®Œæˆï¼

ğŸ“„ æ–‡ä»¶: {pdf_path}
ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:
  - æ€»é¡µæ•°: {result['total_pages']}
  - æ€»æ–‡æœ¬é•¿åº¦: {sum(len(p['text']) for p in result['pages'])} å­—ç¬¦
  - æ€»å›¾åƒæ•°: {sum(len(p['images']) for p in result['pages'])} ä¸ª
  - å›¾åƒåˆ†æ: {'å·²å¯ç”¨' if analyze_images else 'æœªå¯ç”¨'}

{'='*80}
å®Œæ•´å†…å®¹:
{'='*80}

{result['full_content']}
"""
        
        return summary
        
    except FileNotFoundError as e:
        return f"âŒ é”™è¯¯: {str(e)}"
    except Exception as e:
        return f"âŒ PDFè§£æå¤±è´¥: {str(e)}"


# å¯¼å‡ºå·¥å…·åˆ—è¡¨
pdf_parser_tools = [
    parse_pdf_document
]
